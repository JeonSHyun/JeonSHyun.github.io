---
title: Discrete Random variables
author: JSH
date: 2024-07-09 10:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, IMS]
use_math: true
---

# Random Variables

## Random Variables 

<!-- 
random variable은 표본공간에 들어가는 원소(element)들을 실수로 대응시켜주는 함수
vs. probability는 표본공간의 부분집합(set)을 실수로 대응시켜주는 함수
-->

### Definition
For sample space $S$, a function $X$, which assigns to $c \in S$ only one number $X(c) = x$

i.e., $X$: random variable $\Leftrightarrow X:S \rightarrow R$

Random variable is defined on the element of sample space

The probability set function, $P$, is defined on the subset of sample space

### Relationship between probability set function and random variable
In our notion, we define that for $A \subset R$

$X^{-1}\\{A\\} \equiv \\{c \in S: X(c) \in A\\}$: make sure that $X^{-1}\\{A\\}$ indicates the inverse image of $A$ <!-- X^-1{A}는 event, 표본공간의 부분집합 -->

$P(X^{-1}\\{A\\}) \equiv P(X \in A) \equiv P_X(A)$

For example, $\\{c \in S: X(c) \leq a\\} = X^{-1}\\{(-\infty, a\]\\}$   <!-- X^-1{a}: 확률변수 X가 a가 되게 하는 sample space의 집합 -->

![image](https://github.com/user-attachments/assets/9f3f2219-0210-4dce-a396-9c79060dc130)


#### Example
When we toss a coin, we are usually interested in the total number of spots on the sides that are "up" are counted.
If we define $X(H) = 1$ and $X(T) = 0$,
$P_X(1) = P(X = 1) = P(X^{-1} \\{1 \\}) = P(\\{c \in S: X(c) = 1 \\}) = P(\\{H \\})$  <!-- 반드시 이 과정을 거쳐서 답을 도출한다는 것을 기억하기 -->

#### Example
When we cast a die,
* We define $X(1) = 1, X(2) = 2, \ldots, X(6) = 6$. then, $P(X \leq 4) = P(X^{-1} \\{(-\infty, 4]\\}) = P(\\{c \in X: X(c) \leq 4 \\}) = P(\\{1, 2, 3, 4 \\})$
* We define $X(c) = 4$ if $C$ is a even number and otherwise $X(c) = 5$. Then, $P(X \leq 4) = P(X^{-1} \\{(-\infty, 4]\\}) = P(\\{c \in S: X(c) \leq 4 \\}) = P(\\{2, 4, 6 \\})$
<!-- 동일한 확률공간이라 하더라도 확률변수를 어떻게 정의하느냐에 따라서 확률이 달라진다 -->

#### Example
(uniform distribution) Let $X$ be the number of upfaces when two coins are cast.
It is known that $P(H) = P(T) = 0.5$.
Then, find $P(X \leq x)$.

$P(X \leq x) = P( \\{c \in S \mid X(c) \leq x \\}), X(c) \in \\{0, 1 ,2 \\}$

* $x < 0: P(X \leq x) = 0$
* $0 \leq x < 1: P(X \leq x) = P(X = 0) = P(\\{TT \\}) = 1/4$
* $1 \leq x < 2: P(X \leq x) = P(X = 0 or 1) = P(\\{TT, TH, HT \\}) = 3/4$
* $x \geq 2: P(X \leq x) = P(X = 0 or 1 or 2) = P(\\{TT, TH, HT, HH \\}) = 1$

#### Two major difficulties in practical situations
* In many practical situations, the probabilities assigned to the events are unknown
  *  Estimate it by using the relative frequency
  *  Under the certain assumptions, we can estimate it (e.g., possion distribution)
  In general, if we know the distribution of $X$, we can estimate the parameter and then we can calculate $P(X < c)$
* There are many ways of defining a random variable $X$ on $S$

<!-- 여기에 정리하지 않은 예제들도 반드시 파악하고 가야 함! -->


### Space of $X$
The set of real number $S_p(X) = \\{x: x = X(c), c \in S \\}$  <!-- sample space에 들어가는 개별 outcome들을 확률변수에 대입시켰을 때 나올 수 있는 값들의 집합 --> 

### Types of random variable
* $X$: discrete r.v. $\Leftrightarrow S_p(X)$: a countable set <!-- 많아봤자 자연수의 집합일 때 -->
* $X$: continuous r.v. $\Leftrightarrow S_p(X)$: an interval of real numbers

c.f.) countable: one-to-one with positive integers

## Probability distribution function

### Probability mass function (pmf)
For $X$: discrete r.v.,

$p_X(x) = \begin{cases}P(X=x) & x \in S_p(X) \\ 0 & \text{o.w.}\end{cases}$

where $S_p(X) = \\{d_1, d_2, \ldots\\}$

For a given probability set function, its probability mass (density) function can be different, depending on definition of a random variable


### Probability density function (pdf)
For $X$: continuous r.v.,

$P(a < X \leq b) = \int_{a}^{b} f_X(t)dt$ for $f_X(t) \geq 0$ on $[a, b]$, where $a$ and $b$ are any real numbers

$f_X(x)$: probability density function of $X$  <!-- pdf는 무수히 많다. 그 중 아무거나 사용하면 됨 -->

#### Support of $X, S(X)$
* $X$: discrete r.v. $\Rightarrow S(X) = \\{x \in R: P_X(x) > 0 \\}$  <!-- discrete일 때는 S_(X) = S(X) --> 
* $X$: continuous r.v. $\Rightarrow S(X) = \\{x \in R: f_X(x) > 0 \\}$  <!-- continuous일 때는 Space는 확률변수에 의해 다 대응 가능, support는 f_X가 양수일 때이기 때문에 의미가 조금 다르다 -->


### Cumulative distribution function (cdf)
$F_X$: cumulative distribution function of $X$ 
$\Leftrightarrow F_X(x) = P(X \leq x) = P(X^{-1}\\{(-\infty, x]\\})$

* When $X$: discrete r.v. and $S(X) = \\{d_1, d_2, \ldots, d_m, \ldots\\}, F_X(x) = \sum_{d_i \leq x} p_X(d_i)$
* When $X$: continuous r.v., $F_X(x) = \int_{-\infty}^{x} f_X(t)dt$

#### Example
$X$: a real number chosen at random between 0 & 1, and it is distance from 0.
Find the cdf of $X$.

$F_X(x) =$
* $x < 0: 0$
* $0 \leq x \leq 1: x$
* $x > 1: 1$

Then, $f_X(x) = 1$ if $0 \leq x \leq 1$ and 0 o.w

<!-- cdf를 구할 때도 경우를 꼭 나누어야 한다 -->

#### Basic properties of cdf
$F(x)$: cdf of $X$,
* If $a < b, F(a) \leq F(b)$, ($F$: non-decreasing function)
* $0 \leq F(x) \leq 1$
* $\lim_{x \rightarrow -\infty}F(x) = 0, \lim_{x \rightarrow \infty}F(x) = 1$
  * $\lim_{x \rightarrow \infty}F(x) = \lim_{x \rightarrow \infty}P(X\leq x) = \lim_{x \rightarrow \infty}P(X^{-1}(-\infty, x]) = \lim_{n \rightarrow \infty}P(A_n)$
  * $A_1 \subset A_2 \subset \ldots = \cup_{n=1}^{\infty} A_n = S$: increasing property
* $\lim_{h \downarrow 0}F(x+h) = F(x)$, ($F$: right continuous)
  * $\lim_{h \downarrow 0}F(x+h) = \lim_{n \rightarrow \infty}F(x+\frac{1}{n}) = \lim_{n \rightarrow \infty}P(X^{-1} (-\infty, x + \frac{1}{n}]) = \lim_{n \rightarrow \infty}P(A_n) = P(\cap_{n = 1}^{\infty} A_n) = P(X \leq x)$
  * $A_1 \supset A_2 \supset \ldots = \cap_{n=1}^{\infty}A_n$: decreasing property

#### Theorems with respect to cdf
* $X = Y \Rightarrow F_X(x) = F_Y(x)$; however, the converse does not hold
* $P(a < X \leq b) = F_X(b) - F_X(a)$
* $P(X = x) = F_X(x) - F_X(x-)$ where $F_X(x-) = \lim_{t \uparrow x}F_X(t)$  <!-- cdf가 연속인 점에서는 확률이 0이고, cdf가 뛰는 점에서는 확률이 차이만큼이 있다 -->
* $X$: continuous r.v. $\Rightarrow f_X(x) = \frac{dx}{d}F_X(x)$ where $F_X(x)$ is differentiable

#### Example
Find $P(-1 < X \leq 1/2)$ and $P(X = 1)$ where $X$ has the discontinuous cdf 
$F_X(x) = \begin{cases} 0 & x < 0 \\ x/2 & 0 \leq x < 1 \\ 1 & 1 \leq x \end{cases}$

* $P(X = 1) = 1 - 1/2 = 1/2$
* $f_X(x) = 1/2, 0 < x < 1$
* $f_X(x) = 0, x \leq 0$

Therefore, $P(-1 < x \leq 1/2) = F_X(1/2) - F_X(-1) = 1/4$


# Discrete Random Variables

## Properties of pmf
* $0 \leq p_X(x) \leq 1, x \in R$
* $\sum_{x \in R}p_X(x) = 1$
* $P(a \leq X \leq b) = \sum_{a \leq x \leq b}p_X(x)$

### Example
(Geometric distribution)
Consider a sequence of independent flips of a coin.
$X$: the $ of flips needed to obtain the first head.

Find the space, pmf of $X$, and the probability that the first heads appears on an odd number.

* $S_p(X) = \\{1, 2, \ldots \\}$
* $p_X(x) = P(X = x) = (\frac{1}{2})^{x-1} \cdot \frac{1}{2} = (\frac{1}{2})^x$ if $x \in S_p(X)$, $0$ if $x \notin S_p(X)$  <!-- pmf -->
* $\sum_{x = 1, 3, 5, \ldots} P_X(x) = \sum_{x = 1, 3, 5, \ldots} (\frac{1}{2})^x = \frac{1/2}{1 - 1/4} = 2/3$

### Example
(Hypergeometric distribution)
From 100 fuses where there are 20 defective fuses, inspect five of them at random.
Then let $X$ be the number of defective fuses among the 5 that are inspected.
What are the pmf and space of $X$.

$P_X(x) = P(X = x) = \frac{\binom{20}{x} \cdot \binom{80}{5 - x}}{\binom{100}{5}}$

$S_p(X) = \\{0, 1, 2, 3, 4, 5 \\}$


### Definition of Expectatoin
$X$: discrete

If $X$ has a pmf $p(x)$ and $\sum_{x} \mid u(x) \mid p(x) < \infty, E(u(X)) = \sum_{x}u(x)p(x) = \sum_{x \in S_p(X)} u(x)p(x)$

### Properties of Expectation
* If $c$ is a constant, then $E(c) = c$
* If $c$ is a constant and $u$ is a function, then $E[cu(X)] = c E[u(X)]$
* If $c_1$ and $c_2$ are constants and $u_1$ and $u_2$ are functions, then $E[c_1 u_1(X) + c_2 u_2(X)] = c_1 E[u_1(X)] + c_2 E[u_2(X)]$

<!-- x가 클수록 E(x)는 커지는 경향. 일종의 무게중심 같은 것으로 이해하기 -->

#### Example
Let $u(x) = (x - b)^2$, where $b$ is a constant.
Find $b$ that minimizes $E[u(X)]$.

$E[u(X)] = E[(x - b)^2] = E[x^2 -2bx + b^2] = E[x^2] - 2bE[x] + b^2$.
Therefore, $b = E[x]$


### Mean, Variance, standard deviation

#### Definition
* Mean: $\mu = E(X)$
* Variance: $\sigma^2 = Var(X) = E[(X-\mu)^2]$
* Standard deviatoin: $\sigma = sd(X) = \sqrt{\sigma^2}$
* $r$th moment around $b$: $E[(X-b)^r]$

#### Properties
* $Var(X) \geq 0$
  * $Var(X) = E[(X - \mu)^2] = \sum (x - \mu)^2 P_X(x)$  <!-- 둘 다 0보다 크거나 같은 값을 가진다 -->
* $Var(X) = E(X^2) - E(X)^2$
  $(\because) Var(X) = E(X-\mu)^2 = E(X^2 - 2\mu X + \mu^2) = E(X^2) - E(X)^2$
* $Var(aX + b) = a^2 Var(X)$
  * $Var(aX + b) = E((aX + b - a \mu - b)^2) = E(a^2 (X - \mu)^2) = a^2 E((X - \mu)^2) = a^2 Var(X)$

### Transformation
Space of $Y = g(X)$: $S_p(Y) = \\{g(x): x \in S_p(X) \\}$

#### pdf of $Y = g(X)$
$p_X(x)$: $X$'s pmf,

* When $g(x)$ is one-to-one from $S_p(X)$ to $S_p(Y)$ and pmf of $X$ is $p_X(x)$, the pmf of $Y$ is $p_Y(y) = p_X(g^{-1}(y))$
  * $P_Y(y) = P(Y = y) = P(g(X) = y) = P(X = g^{-1}(y)) = P_X(g^{-1}(y))$
* When $g(x)$ is not one-to-one, the pmf of $Y$ is $p_Y(y) = \sum_{g(x) = y}p_X(x)$  <!-- 모든 p_X(x)를 더해서 계산 -->

#### Example
Find pmf $p_X(x), p_Y(y), p_Z(z)$
* $X$: flip number on which the first head appears
  * $S_p(X) = \\{1, 2, \ldots \\}$
  * $P_X(x) = (\frac{1}{2})^x$ if $x \in S_p(Y)$, $0$ if o.w.
* $Y$: flip number before the first head
  * $Y = X - 1$
  * $S_p(Y) = \\{0, 1, \ldots \\}$
  * $p_Y(y) = P(Y = y) = P(X - 1 = y) = P(X = y + 1) = p_X(y + 1) = (\frac{1}{2})^{y + 1}$ if $y \in S_p(Y), 0$ if o.w.
* $Z$: $2X$
  * $S_p(z) = \\{2, 4, 6, \ldots \\}$
  * $p_Z(z) = P(Z = z) = P(2X = z) = P(X = z/2) = p_X(z/2) = (\frac{1}{2})^{z/2}$ if $z \in S_z(Z), 0$ if o.w.

#### Example
Let $p_X(x) = (\frac{1}{2})^x, x = 1, 2, 3, \ldots$.
When r.v. $Z = (X - 2)^2$, find pmf of $Z$.

$S_p(X) = \\{1, 2, 3, \ldots \\}$ and $S_p(Z) = \\{0, 1, 4, 9, \ldots \\}$

$p_Z(z) =$
* $p_X(2), Z = 0$
* $p_X(1) + p_X(3), Z = 1$
* $p_X(2 + \sqrt(z)), Z = 4, 9, \ldots$

#### Example
Let $p_{X_{i}}(x) = p^x (1 - p)^{1 - x}, x = 0, 1$ and $i = 1, 2, \ldots, n$ where $X_i$ are independent.
Then, if $Z = \sum_{i = 1}^n X_i$, find pmf of $Z$.

$p_Z(z) = P(\sum_i X_i = z), X_i \in \\{0, 1 \\} = \sum_{x_1 + \ldots + x_n = z} P(X_1 = x_1) \cdot P(X_2 = x_2) \cdot \ldots P(X_n = x_n) = \sum_{x_1 + \ldots + x_n = z} p^z (1 - p)^{n - z} = \binom{n}{z} p^z (1 - p)^{n - z}$


### Bernoulli distribution 
<!-- 여기서부터 이산형인 경우 -->

#### Bernoulli experiment
Random experiment of which the outcome can be classified in one of two mutually exclusive and exhaustive ways.
That is, $S$ = {success, failure}

#### Bernoulli trial
To perform a Bernoulli experiment several independent times

#### Bernoulli random variable
A r.v. associated with a Bernoulli trial by defining it as follows: $X(\\{success\\}) = 1, X(\\{failure\\}) = 0$

#### Definition (Bernoulli distribution)
$X \sim Bernoulli(p); P(X=1) = p, P(X = 0) = 1 - p$

$\Leftrightarrow$ pmf: $p_X(x) = p^x (1-p)^{1-x}, x = 0, 1$

* Mean: $E(X) = p$
  * $E(X) = \sum x \cdot p_X(x) = 0 \cdot (1 - p) + 1 \cdot p = p$
* Variance: $Var(X) = p(1-p)$
  * $Var(X) = E(X^2) - (E(X))^2 = 0^2 \cdot (1 - p) + 1^2 \cdot p - p^2 = p(1 - p)$


### Binomial distribution
In a sequence of Bernoulli trials, we are often interested in the total number of successes and not in the order of their occurrence

For $X_i \sim Bernoulli(p)$, $Z = X_1 + \ldots + X_n$.
$Z \sim B(n, p)$

#### Binomial theorem
$(a+b)^n = \sum_{x=0}^n \binom{n}{x} a^x b^{n-x}$, where $\binom{n}{x} = \frac{n!}{x!(n-x)!}$

#### Definition
$X \sim B(n, p)$
$\Leftrightarrow p_X(x) = \binom{n}{x} p^x (1-p)^{1-x}, x = 0, 1, \ldots, n$

* Mean: $E(X) = np$
  * $E(X) = E(X_1 + \ldots + X_n) = E(X_1) + \ldots + E(X_n) = p + \ldots + p = np$
  * $E(X) = \sum_{x = 0}^n x \cdot \binom{n}{x} p^x (1 - p)^{n - x}$  <!-- 이걸 푸는 방법은 그냥 풀면되긴하는데 풀이 확인하거나 수업 다시 듣기 -->
* Variance: $Var(X) = np(1-p)$
  * $Var(X) = E(X^2) - (E(X))^2 = \sum_{x = 0}^n x^2 \binom{n}{x} p^x (1 - p)^{n - x} - (E(X))^2$

#### Example
Suppose that an urn contains $N_1$ success balls and $N_2$ failure balls.
Let $X$ equal the number of success balls in a random sample of size $n$ that is taken from this urn.

* With replacement: $P(X = x) = \binom{n}{x} (\frac{N_1}{N_1 + N_2})^x (1 - \frac{N_1}{N_1 + N_2})^{n - x}$
* Without replacement: $P(X = x) = \frac{\binom{N_1}{x} \binom{N_2}{n - x}}{\binom{N_1 + N_2}{n}}$

<!-- N1, N2가 크고, n이 작으면 hypergeometric과 bionomial이 비슷해진다 -->


### Hypergeometric distribution
The hypergeometric distribution is used to determine the probability of a certain number of "successes" in a series of draws made without replacement from a fixed population.

When a population is large and a random sample size is relatively small, it makes little difference if the sampling is done with or withoug replacement. (Bionomial distribution and hypergeometirc distribution are similar)


### Geometric distribution
In a sequence of Bernoulli trials, we are often interested in the total number of Bernoulli trials needed to get the first success

#### Definition
$X \sim Geo(p); P(X=1) = p$
$\Leftrightarrow$ pmf: $p_X(x) = p(1-p)^{x-1}, x = 1, 2, \ldots$

* Mean: $E(X) = 1/p$
  * $E(X) = \sum_{x = 1}^{\infty} x \cdot p(1-p)^{x - 1}$  <!-- E(X) - pE(X) 계산하면 구할 수 있다 + 솔루션 참고 -->
  * $E(X^2) = \sum_{x = 1}^{\infty} x^2 \cdot p(1-p)^{x - 1}$
* Variance: $Var(X) = \frac{1-p}{p^2}$
* Remark: $\frac{1}{1 - p} = 1 + p + p^2 + p^3 + \ldots$


### Negative binomial distribution
In a sequence of Bernoulli trials, we are often interested in the total number of Bernoulli trials needed to observe the $r$th success
<!-- r번 성공 할때까지 bernoulli trial 시행. 따라서 마지막 시행은 반드시 성공으로 끝난다 -->
<!-- support가 r부터 무한대까지?? -->

#### Definition
$X \sim Negbin(r, p)$

$\Leftrightarrow$ pmf: $p_X(x) = \binom{x-1}{r-1}p^r (1-p)^{x-r}, x = r, r+1, r+2, \ldots$

* Mean: $E(X) = r/p$
* Variance: $Var(X) = \frac{r(1-p)}{p^2}$
* By Taylor's theorem, $(1-p)^{-r} = \binom{r-1}{r-1} + \binom{r}{r-1} p + \binom{r+1}{r-1} p^2 + \binom{r+2}{r-1} p^3 + \ldots$


### Moment generating function (mgf)

#### $r$th moment: $E(X^r)$
#### Definition of mgf of $X$
Let $X$ be a discrete r.v. such that $E(e^{tX}) < \infty$ for some $h > 0$ and $-h < t < h$.  <!-- -h < t < h 에서 E(e^tX) 존재하고, 0에서 미분가능함 -->
Then, mgf of $X: M_X(t) = E(e^{tX}) = \sum_{x \in S}e^{tx} p_X(x)$

#### Example
Let $X$ have the pdf $p(x) = \frac{6}{\pi^2 x^2}, x = 1, 2, 3, \ldots$.
Find the mgf of $X$.

$M_X(t) = E(e^{tx}) = \sum_{x = 1}^{\infty}e^{tx}/x^2 \cdot \frac{6}{\pi^2}$.

$\sum_{x = 1}^{\infty} \frac{(e^t)^x}{x^2} = \lim_{n \rightarrow \infty} \sum_{x = 1}^n \frac{(e^t)^x}{x^2} \geq \lim_{n \rightarrow \infty} \frac{(e^t)^n}{n^2} \rightarrow \infty$

Therefore, mgf of $X$ is not exist.

#### Relationship between mgf and moment ($E(X^m)$)
For any type of random variable $X$,
$E(X^r) = M_X^{(r)}(0), M_X(t) = \sum_{r=0}^{\infty} \frac{E(X^r)}{r!} t^r, |t| < h$ for some $h > 0$, where $M_X(t)$ exists

<!-- E(X^r)은 mgf를 r번 미분해서 0을 대입한 것과 같다 -->

* Proof
  * $M_X(t) = E(e^{tx}) = E(1 + \frac{1}{2} tX + \frac{1}{3!} (tX)^2 + \ldots)$  <!-- 증명 일부라는데 정확히 이해 x -->
  * $e^x = 1 + \frac{1}{2} x + \frac{1}{3!} x^2 + \ldots$
* $E(X) = M_X^{(1)}(0)$
* $Var(X) = M_X^{(2)}(0) - (M_X^{(1)}(0))^2$

#### Mgf of binomial distribution

#### Mgf of negative binomial distribution


#### Uniqueness of mgf (it is also true for continuous random variable)
$M_X(t) = M_Y(t)$ for some $h > 0$ and $-h < t < h$

$\Leftrightarrow F_X(z) = F_Y(z) \forall z \in R$

#### mgf of Binomial distribution
$X \sim B(n, p)$

$M_X(t) = (pe^t + 1 - p)^n$


### Poisson distribution

#### Example of Poisson distribution
Some experiments result in counting the number of times particular events occur at given times or with given physical objects

#### We can have an approximate Poisson process with parameter $\lambda$ if the following conditions are satisfied
* The changes occurring in non-overlapping intervals are independent
* The probability of exactly one change occurring in a sufficiently short interval of length $h$ is approximately $\lambda h$
* The probability of two or more changes occurring in a sufficiently short interval is essentially zero

Under these conditions, we assume that the probability of exactly one change in unit interval is $\lambda$.

$P(X=x) = \lim_{n \rightarrow \infty} P(\text{we observe } x \text{changes from } n \text{units with } 1/n \text{length}) = \lim_{n \rightarrow \infty} {}_nC_x (\frac{\lambda}{n})^x (1 - \frac{\lambda}{n})^{n-x} = \frac{\lambda^x e^{-\lambda}}{x!}$

#### Approximation
If $n$ is large and $p$ is small,
$\frac{(np)^x e^{-np}}{x!} \approx {}_nC_x p^x (1-p)^{n-x}$

#### Definition (Poisson distribution)
$X \sim Poisson(m), m > 0 \Leftrightarrow p(x) = \frac{m^x e^{-m}}{x!}, x = 0, 1, 2, \ldots$

* By Maclaurin's Theorem
$e^m = \sum_{x = 0}^{\infty} \frac{m^x}{x!}$

#### mgf
$M(t) = exp(m(e^t - 1)), -\infty < t < \infty$

#### Mean
$E(X) = m$

#### Variance
$Var(X) = m$
