---
title: Discrete Random variables
author: JSH
date: 2024-07-09 10:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, IMS]
use_math: true
---

# Random Variables

## Random Variables

### Definition
For sample space $S$, a function $X$, which assigns to $c \in S$ only one number $X(c) = x$

i.e., $X$: random variable $\Leftrightarrow X:S \rightarrow R$

Random variable is defined on the element of sample space

The probability set function, $P$, is defined on the subset of sample space

### Relationship between probability set function and random variable
In our notion, we define that for $A \subset R$

$X^{-1}\\{A\\} \equiv \\{c \in S: X(c) \in A\\}$: make sure that $X^{-1}\\{A\\}$ indicates the inverse image of $A$

$P(X^{-1}\\{A\\}) \equiv P(X \in A) \equiv P_X(A)$

For example, $\\{c \in S: X(c) \leq a\\} = X^{-1}\\{(-\infty, a\]\\}$

### Space of $X$
The set of real number $S_p(X) = \\{x: x = X(c), c \in S \\}$

### Types of random variable
* $X$: discrete r.v. $\Leftrightarrow S_p(X)$: a countable set
* $X$: continuous r.v. $\Leftrightarrow S_p(X)$: an interval of real numbers


## Probability distribution function

### Probability mass function (pmf)
For $X$: discrete r.v.,

$p_X(x) = \begin{cases}P(X=x) & x \in S_p(X) \\ 0 & \text{o.w.}\end{cases}$

where $S_p(X) = \\{d_1, d_2, \ldots\\}$

### Probability density function (pdf)
For $X$: continuous r.v.,

$P(a < X \leq b) = \int_{a}^{b} f_X(t)dt$ for $f_X(t) \geq 0$ on $[a, b]$, where $a$ and $b$ are any real numbers

$f_X(x)$: probability density function of $X$

#### Support of $X, S(X)$
* $X$: discrete r.v. $\Rightarrow S(X) = \\{x \in R: P_X(x) > 0 \\}$
* $X$: continuous r.v. $\Rightarrow S(X) = \\{x \in R: f_X(x) > 0 \\}$

### Cumulative distribution function (cdf)
$F_X$: cumulative distribution function of $X$ 
$\Leftrightarrow F_X(x) = P(X \leq x) = P(X^{-1}\\{(-\infty, x]\\})$

* When $X$: discrete r.v. and $S(X) = \\{d_1, d_2, \ldots, d_m, \ldots\\}, F_X(x) = \sum_{d_i \leq x} p_X(d_i)$
* When $X$: continuous r.v., $F_X(x) = \int_{-\infty}^{x} f_X(t)dt$

#### Basic properties of cdf
$F(x)$: cdf of $X$,
* If $a < b, F(a) \leq F(b)$, ($F$: non-decreasing function)
* $0 \leq F(x) \leq 1$
* $\lim_{x \rightarrow -\infty}F(x) = 0, \lim_{x \rightarrow \infty}F(x) = 1$
* $\lim_{h \downarrow 0}F(x+h) = F(x)$, ($F$: right continuous)

#### Theorems with respect to cdf
* $X = Y \Rightarrow F_X(x) = F_Y(x)$; however, the converse does not hold
* $P(a < X \leq b) = F_X(b) - F_X(a)$
* $P(X = x) = F_X(x) - F_X(x-)$ where $F_X(x-) = \lim_{t \uparrow x}F_X(t)$
* $X$: continuous r.v. $\Rightarrow f_X(x) = \frac{dx}{d}F_X(x)$ where $F_X(x)$ is differentiable


# Discrete Random Variables

## Properties of pmf
* $0 \leq p_X(x) \leq 1, x \in R$
* $\sum_{x \in R}p_X(x) = 1$
* $P(a \leq X \leq b) = \sum_{a \leq x \leq b}p_X(x)$

### Definition of Expectatoin
$X$: discrete

If $X$ has a pmf $p(x)$ and $\sum_{x}|u(x)|p(x) < \infty, E(u(X)) = \sum_{x}u(x)p(x)$

### Properties of Expectation
* If $c$ is a constant, then $E(c) = c$
* If $c$ is a constant and $u$ is a function, then $E[cu(X)] = c E[u(X)]$
* If $c_1$ and $c_2$ are constants and $u_1$ and $u_2$ are functions, then $E[c_1 u_1(X) + c_2 u_2(X)] = c_1 E[u_1(X)] + c_2 E[u_2(X)]$

### Mean, Variance, standard deviation

#### Definition
* Mean: $\mu = E(X)$
* Variance: $\sigma^2 = Var(X) = E[(X-\mu)^2]$
* Standard deviatoin: $\sigma = sd(X) = \sqrt{\sigma^2}$
* $r$th moment around $b$: $E[(X-b)^r]$

#### Properties
* $Var(X) \geq 0$
* $Var(X) = E(X^2) - E(X)^2$ \\ $(\because) Var(X) = E(X-\mu)^2 = E(X^2 - 2\mu X + \mu^2) = E(X^2) - E(X)^2$
* $Var(aX+b) = a^2 Var(X)$

### Transformation
Space of $Y = g(X)$: $S_p(Y) = \\{g(x): x \in S_p(X) \\}$

#### pdf of $Y = g(X)$
$p_X(x)$: $X$'s pmf,

* When $g(x)$ is one-to-one from $S_p(X)$ to $S_p(Y)$ and pmf of $X$ is $p_X(x)$, the pmf of $Y$ is $p_Y(y) = p_X(g^{-1}(y))$
* When $g(x)$ is not one-to-one, the pmf of $Y$ is $p_Y(y) = \sum_{g(x) = y}p_X(x)$

### Bernoulli distribution

#### Bernoulli experiment
Random experiment of which the outcome can be classified in one of two mutually exclusive and exhaustive ways.

That is, $S$ = {success, failure}

#### Bernoulli trial
To perform a Bernoulli experiment several independent times

#### Bernoulli random variable
A r.v. associated with a Bernoulli trial by defining it as follows: $X(\\{success\\}) = 1, X(\\{failure\\}) = 0$

#### Definition (Bernoulli distribution)
$X \sim Bernoulli(p); P(X=1) = p$

$\Leftrightarrow$ pmf: $p_X(x) = p^x (1-p)^{1-x}, x = 0, 1$

#### Mean
$E(X) = p$
#### Variance
$Var(X) = p(1-p)$


### Binomial distribution
In a sequence of Bernoulli trials, we are often interested in the total number of successes and not in the order of their occurrence

#### Binomial theorem
$(a+b)^n = \sum_{x=0}^n \binom{n}{x} a^x b^{n-x}$, where $\binom{n}{x} = \frac{n!}{x!(n-x)!}$

#### Definition
$X \sim B(n, p)$
$\Leftrightarrow p_X(x) = \binom{n}{x} p^x (1-p)^{1-x}, x = 0, 1, \ldots, n$

#### Mean: $E(X) = np$
#### Variance: $Var(X) = np(1-p)$


### Hypergeometric distribution
The hypergeometric distribution is used to determine the probability of a certain number of "successes" in a series of draws made without replacement from a fixed population.

When a population is large and a random sample size is relatively small, it makes little difference if the sampling is done with or withoug replacement. (Bionomial distribution and hypergeometirc distribution are similar)


### Geometric distribution
In a sequence of Bernoulli trials, we are often interested in the total number of Bernoulli trials needed to get the first success

#### Definition
$X \sim Geo(p); P(X=1) = p$
$\Leftrightarrow$ pmf: $p_X(x) = p(1-p)^{x-1}, x = 1, 2, \ldots$

#### Mean
$E(X) = 1/p$
#### Variance
$Var(X) = \frac{1-p}{p^2}$


### Negative binomial distribution
In a sequence of Bernoulli trials, we are often interested in the total number of Bernoulli trials needed to observe the $r$th success

#### Definition
$X \sim Negbin(r, p)$

$\Leftrightarrow$ pmf: $p_X(x) = \binom{x-1}{r-1}p^r (1-p)^{x-r}, x = r, r+1, r+2, \ldots$

#### Mean
$E(X) = r/p$
#### Variance
$Var(X) = \frac{r(1-p)}{p^2}$

* By Taylor's theorem, $(1-p)^{-r} = \binom{r-1}{r-1} + \binom{r}{r-1} p + \binom{r+1}{r-1} p^2 + \binom{r+2}{r-1} p^3 + \ldots$


### Moment generating function (mgf)

#### $r$th moment: $E(X^r)$
#### Definition of mgf of $X$
Let $X$ be a discrete r.v. such that $E(e^{tX}) < \infty$ for some $h > 0$ and $-h < t < h$.
Then, mgf of $X: M_X(t) = E(e^{tX}) = \sum_{x \in S}e^{tx} p_X(x)$

#### Relationship between mgf and moment ($E(X^m)$)
For any type of random variable $X$,
$E(X^r) = M_X^{(r)}(0), M_X(t) = \sum_{r=0}^{\infty} \frac{E(X^r)}{r!} t^r, |t| < h$ for some $h > 0$, where $M_X(t)$ exists

#### Uniqueness of mgf (it is also true for continuous random variable)
$M_X(t) = M_Y(t)$ for some $h > 0$ and $-h < t < h$

$\Leftrightarrow F_X(z) = F_Y(z) \forall z \in R$

#### mgf of Binomial distribution
$X \sim B(n, p)$

$M_X(t) = (pe^t + 1 - p)^n$


### Poisson distribution

#### Example of Poisson distribution
Some experiments result in counting the number of times particular events occur at given times or with given physical objects

#### We can have an approximate Poisson process with parameter $\lambda$ if the following conditions are satisfied
* The changes occurring in non-overlapping intervals are independent
* The probability of exactly one change occurring in a sufficiently short interval of length $h$ is approximately $\lambda h$
* The probability of two or more changes occurring in a sufficiently short interval is essentially zero

Under these conditions, we assume that the probability of exactly one change in unit interval is $\lambda$.

$P(X=x) = \lim_{n \rightarrow \infty} P(\text{we observe } x \text{changes from } n \text{units with } 1/n \text{length}) = \lim_{n \rightarrow \infty} {}_nC_x (\frac{\lambda}{n})^x (1 - \frac{\lambda}{n})^{n-x} = \frac{\lambda^x e^{-\lambda}}{x!}$

#### Approximation
If $n$ is large and $p$ is small,
$\frac{(np)^x e^{-np}}{x!} \approx {}_nC_x p^x (1-p)^{n-x}$

#### Definition (Poisson distribution)
$X \sim Poisson(m), m > 0 \Leftrightarrow p(x) = \frac{m^x e^{-m}}{x!}, x = 0, 1, 2, \ldots$

* By Maclaurin's Theorem
$e^m = \sum_{x = 0}^{\infty} \frac{m^x}{x!}$

#### mgf
$M(t) = exp(m(e^t - 1)), -\infty < t < \infty$

#### Mean
$E(X) = m$

#### Variance
$Var(X) = m$
