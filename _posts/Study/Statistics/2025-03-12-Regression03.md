---
title: Simple Linear Regression
author: JSH
date: 2025-03-12 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Simple Linear Regression  
<!-- x가 하나 -->

## Regression Analysis
Study a functional relatoinship between variables.
- Response variable $y, Y$ (dependent variable)
- Explanatory variable $x, X$ (independent variable)

To explain the variability of $Y$.

Flow:
- Select plausible explanatory variables
- Collect data
- Assume a functional model (e.g., linear)
- Fit the model
- Check model validity
- Make inference

<!-- make inference: p-value로 가설 검정을 할 수 있다는 의미. 회귀모형의 p-value를 활용하려면 회귀 모델의 가정을 만족해야 하므로 이 가정을 만족하는지 확인하는 것이 반드시 필요하다. -->

## Simple linear regression model
$Y_i = \beta_0 + \beta_1 x_i + \epsilon_i, (x_1, \ldots, x_n$: non-random)
- $\epsilon_1, \ldots, \epsilon_n$: indepdnent random errors
- $E(\epsilon_i) = 0, Var(\epsilon_i) = \sigma^2 (i = 1, \ldots, n)$
- $\epsilon_i \sim N(0, \sigma^2) \Leftrightarrow Y_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2)$  <!-- inference를 위해 필요함. beta를 추정하려 하므로 x는 이미 알고있는 상수로 간주 -->

### LINE Assumption
- Linear
- Indepenent
- Normal
- Equal variance

## Method of estimation

### Least Squares Method
Minimize $\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2$ w.r.t. $\beta_0$ and $\beta_1$

#### Normal equation
$f(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2$
- $\frac{\partial l}{\partial \beta_0} = -2 \sum (y_i - \beta_0 - \beta_1 x_i) = 0 \Rightarrow \sum_{i=1}^n e_i = 0$
- $\frac{\partial l}{\partial \beta_1} = -2 \sum x_i (y_i - \beta_0 - \beta_1 x_i) = 0 \Rightarrow \sum_{i=1}^n x_i e_i = 0 \Rightarrow e_i = y_i - (\hat{\beta_0} + \hat{\beta_1 x_i})$ (residual)
  - $\sum x_i e_i - \bar{x} \sum e_i = \sum (x_i - \bar{x}) e_i = 0$

#### Least square estimates
From the equation $\sum (\hat{y_i} - \hat{\beta_0} - \hat{\beta_1} x_i) = 0$

(i) $n \hat{\beta_0} + \hat{\beta_1} \sum_i x_i = \sum \hat{y_i}$

(ii) $\hat{\beta_0} \sum (x_i - \bar{x}) + \hat{\beta_1} \sum (x_i - \bar{x}) x_i = \sum y_i (x_i - \bar{x})$
  - $\beta_0 \sum (x_i - \bar{x}) = 0$
  - $\hat{\beta_1} = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$

Therefore, $\hat{\beta_1} = S_{xy}/S_{xx}$ and $\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}$ where $S_{xy} = \sum (x_i - \bar{x})(y_i - \bar{y})$ and $S_{xx} = \sum (x_i - \bar{x})^2$

<!-- 한번 유도해보는 것 필요할듯하다 -->

#### Least squares regression fit
$\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}$

Therefore, $\hat{y} = \hat{\beta_0} + \hat{\beta_1} x = \bar{y} + \hat{\beta_1} (x - \bar{x})$

### Unbiased estimation of $\sigma^2$
$\hat{\sigma}^2 = \frac{1}{n-2} \sum_{i=1}^n (y_i - \hat{y_i})^2 = \frac{1}{n-2}$ SSE
- $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i$
- SSE: residual sum of squares (error sum of squares)

$n-2$: degrees of freedom
- $\sum e_i = 0$
- $\sum x_i e_i = 0$
<!-- 전체는 n인데 2개 식에 대한 것을 빼서 n-2가 됨 -->

## Method of inference

### Properties of estimates
- $E(\hat{\beta_1}) = \beta_1, Var(\hat{\beta_1}) = \sigma^2/S_{xx}$
- $E(\hat{\beta_0}) = \beta_0, Var(\hat{\beta_0}) = \sigma^2 (n^{-1} + \bar{x}^2/S_{xx})$
- $Cov(\hat{\beta_0}, \hat{\beta_1}) = -\sigma^2 \cdot \bar{x}/S_{xx}$
- $E(\hat{\sigma}^2) = \sigma^2, Var(e_i) = (1 - p_{ii}) \sigma^2, Cov(e_i, e_j) = -p_{ij} \sigma^2, E(e_i) = 0$ where $p_{ij} = \frac{1}{n} + \frac{(x_i - \bar{x})(x_j - \bar{x})}{S_{xx}}$

### Inference under additional normality assumption

#### Inference for $\beta_1$
$\frac{\hat{\beta_1} - \beta_1}{s.e.(\hat{\beta_1})} \sim t(n-2)$; $s.e.(\hat{\beta_1}) = \sqrt{\hat{var}(\hat{\beta_1})} = \hat{\sigma}/S_{xx}^{1/2}$

- $100 \times (1 - \alpha)%$ C.I.: $[\hat{\beta_1} - t(n-2; \alpha/2) \times s.e.(\hat{\beta_1}) \leq \beta_1 \leq \hat{\beta_1} + t(n-2;\alpha/2) \times s.e.(\hat{\beta_1})]$
- Reject $H_0$: $\beta_1 = \beta_1^0$ in favor of $H_1: \beta_1 \neq \beta_1^0$ iff $\mid \frac{\hat{\beta_1} - \beta_1^0}{s.e.(\hat{\beta_1})} \mid > t(n-2; \alpha/2)$
- p-value <!-- 양측검정 -->

<!-- beta_1 = 0으로 하고 검정할 수 있다. -->
<!-- t(df>=25) ~ N(0, 1) -->

#### Inference for $\beta_0$
$\frac{\hat{\beta_0} - \beta_0}{s.e.(\hat{\beta_0})} \sim t(n-2)$; $s.e.(\hat{\beta_0}) = \sqrt{\hat{var}(\hat{\beta_0})} = \hat{\sigma}(n^{-1} + \bar{x}/S_{xx})^{1/2}$

- $100 \times (1 - \alpha)%$ C.I.: $[\hat{\beta_0} - t(n-2; \alpha/2) \times s.e.(\hat{\beta_0}) \leq \beta_0 \leq \hat{\beta_0} + t(n-2;\alpha/2) \times s.e.(\hat{\beta_0})]$
- Reject $H_0$: $\beta_0 = \beta_0^0$ in favor of $H_1: \beta_0 \neq \beta_0^0$ iff $\mid \frac{\hat{\beta_0} - \beta_0^0}{s.e.(\hat{\beta_0})} \mid > t(n-2; \alpha/2)$

#### Inference for $\mu_0$

$Y_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2)$

$\mu_0 = E(Y \mid x_0) = \beta_0 + \beta_1 x_0$

$\hat{\mu_0} = \hat{\beta_0} + \hat{\beta_1} x_0$

$Var(\hat{\mu_0}) = Var(\hat{\beta_0} + \hat{\beta_1} x_0) = Var(\hat{\beta_0}) + Var(\hat{\beta_1})x_0^2 + 2x_0 Cov(\hat{\beta_0}, \hat{\beta_1})$
<!-- x는 상수취급이므로 variance 밖으로 나오도록 정리할 수 있다. -->
<!-- Var(X+Y) = Var(X) + Var(Y) + 2Cov(X, Y) -->

$\frac{\hat{\mu_0} - \mu_0}{s.e.(\hat{\mu_0})} \sim t(n-2)$; $s.e.(\hat{\mu_0}) = \sqrt{\hat{var}(\hat{\beta_0} + \hat{\beta_1} x_0)} = \hat{\sigma}(n^{-1} + (x_0 - \bar{x})^2/S_{xx})^{1/2}$

- $100 \times (1 - \alpha)%$ C.I.: $[\hat{\mu_0} - t(n-2; \alpha/2) \times s.e.(\hat{\mu_0}) \leq \mu_0 \leq \hat{\mu_0} + t(n-2;\alpha/2) \times s.e.(\hat{\mu_0})]$
- Test (not given)

#### Prediction for $y_0 = \beta_0 + \beta_1 x_0 + \epsilon_0 (\epsilon_0$: indepe of $\epsilon_1, \ldots, \epsilon_n$)
$\hat{y_0} = \hat{\beta_0} + \hat{\beta_1} x_0$

$Var(y_0) = Var(\hat{\mu_0} + \epsilon_0) = Var(\hat{\mu_0}) + Var(\epsilon_0)$ <!-- 독립이므로 Cov = 0 -->

$\frac{\hat{y_0} - y_0}{s.e.(y_0 - \hat{y_0})} \sim t(n-2)$; $s.e.(y_0 - \hat{y_0}) = \hat{\sigma}(1 + n^{-1} + (x_0 - \bar{x})^2/S_{xx})^{1/2}$
<!--prediction인 이유는 오차항 때문에 variance가 생기기 때문이다. 앞의 mu는 평균이기 때문에 오차항을 무시하고, 실제 y 하나하나를 prediction하면 오차항을 포함하므로 다르다 -->

- $100 \times (1 - \alpha)%$ Prediction interval: $[\hat{y_0} - t(n-2; \alpha/2) \times s.e.(y_0 - \hat{y_0}) \leq y_0 \leq \hat{y_0} + t(n-2;\alpha/2) \times s.e.(y_0 - \hat{y_0})]$

Note that $\hat{\mu_0}$ is identical to the predicted response $\hat{y_0}$ at any given $x_0$
