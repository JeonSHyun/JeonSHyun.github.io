---
title: Simple Linear Regression
author: JSH
date: 2025-03-12 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Simple Linear Regression  
<!-- x가 하나 -->

## Regression Analysis
Study a functional relatoinship between variables.
- Response variable $y, Y$ (dependent variable)
- Explanatory variable $x, X$ (independent variable)

To explain the variability of $Y$.

Flow:
- Select plausible explanatory variables
- Collect data
- Assume a functional model (e.g., linear)
- Fit the model
- Check model validity
- Make inference

<!-- make inference: p-value로 가설 검정을 할 수 있다는 의미. 회귀모형의 p-value를 활용하려면 회귀 모델의 가정을 만족해야 하므로 이 가정을 만족하는지 확인하는 것이 반드시 필요하다. -->

## Simple linear regression model
$Y_i = \beta_0 + \beta_1 x_i + \epsilon_i, (x_1, \ldots, x_n$: non-random)
- $\epsilon_1, \ldots, \epsilon_n$: indepdnent random errors
- $E(\epsilon_i) = 0, Var(\epsilon_i) = \sigma^2 (i = 1, \ldots, n)$
- $\epsilon_i \sim N(0, \sigma^2) \Leftrightarrow Y_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2)$  <!-- inference를 위해 필요함. beta를 추정하려 하므로 x는 이미 알고있는 상수로 간주 -->

### LINE Assumption
- Linear
- Indepenent
- Normal
- Equal variance

## Method of estimation

### Least Squares Method
Minimize $\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2$ w.r.t. $\beta_0$ and $\beta_1$

#### Normal equation
$f(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2$
- $\frac{\partial l}{\partial \beta_0} = -2 \sum (y_i - \beta_0 - \beta_1 x_i) = 0 \Rightarrow \sum_{i=1}^n e_i = 0$
- $\frac{\partial l}{\partial \beta_1} = -2 \sum x_i (y_i - \beta_0 - \beta_1 x_i) = 0 \Rightarrow \sum_{i=1}^n x_i e_i = 0 \Rightarrow e_i = y_i - (\hat{\beta_0} + \hat{\beta_1 x_i})$ (residual)
  - $\sum x_i e_i - \bar{x} \sum e_i = \sum (x_i - \bar{x}) e_i = 0$

#### Least square estimates
From the equation $\sum (\hat{y_i} - \hat{\beta_0} - \hat{\beta_1} x_i) = 0$

(i) $n \hat{\beta_0} + \hat{\beta_1} \sum_i x_i = \sum \hat{y_i}$

(ii) $\hat{\beta_0} \sum (x_i - \bar{x}) + \hat{\beta_1} \sum (x_i - \bar{x}) x_i = \sum y_i (x_i - \bar{x})$
  - $\beta_0 \sum (x_i - \bar{x}) = 0$
  - $\hat{\beta_1} = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$

Therefore, $\hat{\beta_1} = S_{xy}/S_{xx}$ and $\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}$ where $S_{xy} = \sum (x_i - \bar{x})(y_i - \bar{y})$ and $S_{xx} = \sum (x_i - \bar{x})^2$

<!-- 한번 유도해보는 것 필요할듯하다 -->

#### Least squares regression fit
$\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}$

Therefore, $\hat{y} = \hat{\beta_0} + \hat{\beta_1} x = \bar{y} + \hat{\beta_1} (x - \bar{x})$

### Unbiased estimation of $\sigma^2$
$\hat{\sigma}^2 = \frac{1}{n-2} \sum_{i=1}^n (y_i - \hat{y_i})^2 = \frac{1}{n-2}$ SSE
- $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i$
- SSE: residual sum of squares (error sum of squares)

$n-2$: degrees of freedom
- $\sum e_i = 0$
- $\sum x_i e_i = 0$
<!-- 전체는 n인데 2개 식에 대한 것을 빼서 n-2가 됨 -->

## Method of inference

### Properties of estimates
- $E(\hat{\beta_1}) = \beta_1, Var(\hat{\beta_1}) = \sigma^2/S_{xx}$
- $E(\hat{\beta_0}) = \beta_0, Var(\hat{\beta_0}) = \sigma^2 (n^{-1} + \bar{x}^2/S_{xx})$
- $Cov(\hat{\beta_0}, \hat{\beta_1}) = -\sigma^2 \cdot \bar{x}/S_{xx}$
- $E(\hat{\sigma}^2) = \sigma^2, Var(e_i) = (1 - p_{ii}) \sigma^2, Cov(e_i, e_j) = -p_{ij} \sigma^2, E(e_i) = 0$ where $p_{ij} = \frac{1}{n} + \frac{(x_i - \bar{x})(x_j - \bar{x})}{S_{xx}}$

### Inference under additional normality assumption

#### Inference for $\beta_1$
$\frac{\hat{\beta_1} - \beta_1}{s.e.(\hat{\beta_1})} \sim t(n-2)$; $s.e.(\hat{\beta_1}) = \sqrt{\hat{var}(\hat{\beta_1})} = \hat{\sigma}/S_{xx}^{1/2}$

- $100 \times (1 - \alpha)%$ C.I.: $[\hat{\beta_1} - t(n-2; \alpha/2) \times s.e.(\hat{\beta_1}) \leq \beta_1 \leq \hat{\beta_1} + t(n-2;\alpha/2) \times s.e.(\hat{\beta_1})]$
- Reject $H_0$: $\beta_1 = \beta_1^0$ in favor of $H_1: \beta_1 \neq \beta_1^0$ iff $\mid \frac{\hat{\beta_1} - \beta_1^0}{s.e.(\hat{\beta_1})} \mid > t(n-2; \alpha/2)$
- p-value <!-- 양측검정 -->

"Test of significance" of explanatory variable
- $H_0: \beta_1 = 0$ vs. $H_1: \beta_1 \neq 0$
- CI: $\hat{\beta} \pm t_{0.05/2}(df) \cdot s.e.(\hat{\beta})$
- p-value / meaning: we've seen a data which can hardly be observed under $H_0$. We may reject $H_0$

<!-- beta_1 = 0으로 하고 검정할 수 있다. -->
<!-- beta_1를 주의깊게 보지, beta_0은 중요하지 않다. -->
<!-- t(df>=25) ~ N(0, 1) -->

#### Inference for $\beta_0$
$\frac{\hat{\beta_0} - \beta_0}{s.e.(\hat{\beta_0})} \sim t(n-2)$; $s.e.(\hat{\beta_0}) = \sqrt{\hat{var}(\hat{\beta_0})} = \hat{\sigma}(n^{-1} + \bar{x}/S_{xx})^{1/2}$

- $100 \times (1 - \alpha)%$ C.I.: $[\hat{\beta_0} - t(n-2; \alpha/2) \times s.e.(\hat{\beta_0}) \leq \beta_0 \leq \hat{\beta_0} + t(n-2;\alpha/2) \times s.e.(\hat{\beta_0})]$
- Reject $H_0$: $\beta_0 = \beta_0^0$ in favor of $H_1: \beta_0 \neq \beta_0^0$ iff $\mid \frac{\hat{\beta_0} - \beta_0^0}{s.e.(\hat{\beta_0})} \mid > t(n-2; \alpha/2)$

#### Inference for $\mu_0$

$Y_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2)$

$\mu_0 = E(Y \mid x_0) = \beta_0 + \beta_1 x_0$

$\hat{\mu_0} = \hat{\beta_0} + \hat{\beta_1} x_0$

$Var(\hat{\mu_0}) = Var(\hat{\beta_0} + \hat{\beta_1} x_0) = Var(\hat{\beta_0}) + Var(\hat{\beta_1})x_0^2 + 2x_0 Cov(\hat{\beta_0}, \hat{\beta_1})$
<!-- x는 상수취급이므로 variance 밖으로 나오도록 정리할 수 있다. -->
<!-- Var(X+Y) = Var(X) + Var(Y) + 2Cov(X, Y) -->

$\frac{\hat{\mu_0} - \mu_0}{s.e.(\hat{\mu_0})} \sim t(n-2)$; $s.e.(\hat{\mu_0}) = \sqrt{\hat{var}(\hat{\beta_0} + \hat{\beta_1} x_0)} = \hat{\sigma}(n^{-1} + (x_0 - \bar{x})^2/S_{xx})^{1/2}$

- $100 \times (1 - \alpha)%$ C.I.: $[\hat{\mu_0} - t(n-2; \alpha/2) \times s.e.(\hat{\mu_0}) \leq \mu_0 \leq \hat{\mu_0} + t(n-2;\alpha/2) \times s.e.(\hat{\mu_0})]$
- Test (not given)

#### Prediction for $y_0 = \beta_0 + \beta_1 x_0 + \epsilon_0 (\epsilon_0$: indepe of $\epsilon_1, \ldots, \epsilon_n$)
$\hat{y_0} = \hat{\beta_0} + \hat{\beta_1} x_0$

$Var(y_0) = Var(\hat{\mu_0} + \epsilon_0) = Var(\hat{\mu_0}) + Var(\epsilon_0)$ <!-- 독립이므로 Cov = 0 -->

$\frac{\hat{y_0} - y_0}{s.e.(y_0 - \hat{y_0})} \sim t(n-2)$; $s.e.(y_0 - \hat{y_0}) = \hat{\sigma}(1 + n^{-1} + (x_0 - \bar{x})^2/S_{xx})^{1/2}$
<!--prediction인 이유는 오차항 때문에 variance가 생기기 때문이다. 앞의 mu는 평균이기 때문에 오차항을 무시하고, 실제 y 하나하나를 prediction하면 오차항을 포함하므로 다르다 -->

- $100 \times (1 - \alpha)%$ Prediction interval: $[\hat{y_0} - t(n-2; \alpha/2) \times s.e.(y_0 - \hat{y_0}) \leq y_0 \leq \hat{y_0} + t(n-2;\alpha/2) \times s.e.(y_0 - \hat{y_0})]$

Note that $\hat{\mu_0}$ is identical to the predicted response $\hat{y_0}$ at any given $x_0$

All these are valid under the model assumptions.
Note that $H_0: \beta_0$ vs. $H_1: \beta_0 \neq 0$ can't be rejected even at 10%.
It means we may start with a simpler model ($y_i = \beta_1 x_i + \epsilon_i$, $\epsilon_i \sim N(0, \sigma^2))$.
Then, all the above inferences should be changed.


## Measuring the quality of fit

### Decomposition of Sum of Squares
#### Deviation
$y_i - \bar{y} = (y_i - \hat{y}_i) + (\hat{y}_i - \bar{y})$

#### Sum of squares
$\sum (y_i - \bar{y})^2 = \sum (y_i - \hat{y}_i)^2 + \sum (\hat{y}_i - \bar{y})^2$
- SST: $\sum (y_i - \bar{y})^2$, df = $n-1$  <!-- SST의 sum이 0이 되어야 하므로 n-1 자유도를 가진다 -->
- SSE: $\sum (y_i - \hat{y}_i)^2$, df = $n-2$  <!-- \sum e_i = 0, \sum x_i e_i = 0을 만족해야 하므로 자유도는 n-2 -->
- SSR: $\sum (\hat{y}_i - \bar{y})^2$, df = 1  <!-- \hat{y}_i - \bar{y} = \hat{\beta_1} (x_i - \bar{x})인데 x는 상수취급. 따라서 beta에 따라 달라짐. 자유도 1 -->

<!-- 자유도의 의미는 자유롭게 움직일 수 있는 데이터의 개수 -->


#### Expansion
$\sum (y_i - \bar{y})^2 = \sum (y_i - \hat{y}_i)^2 + \sum (\hat{y}_i - \bar{y})^2 + \sum 2(y_i - \hat{y}_i)(\hat{y}_i - \bar{y})$

$\sum 2(y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) = 2 \sum e_i \hat{\beta}_1 (x_i - \bar{x}) = 2 \hat{\beta}_1 (\sum x_i e_i - \bar{x} \sum e_i) = 0$

$\because \hat{y}_i = \bar{y} + \hat{\beta}_1 (x_i - \bar{x})$

### SSR
SSR = $\sum (\hat{y_i}-\bar{y})^2 = \hat{\beta_1}^2 \sum (x_i - \bar{x})^2 = S_{xy}^2 / S_{xx} = (\sum \frac{x_i - \bar{x}}{\sqrt{S_{xx}}} y_i)^2$

Remark 1: $\hat{\beta_1} = \frac{S_{xy}}{S_{xx}}$
Remark 2: $S_{xy} = \sum (x_i - \bar{x})(y_i - \bar{y}) = \sum(x_i - \bar{x})y_i$

<!-- \hat{\beta_1} = 0이면 SSR도 0 -->

### Coefficient of determination (Multiple Correlation Coeff)
$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}, 0 \leq R^2 \leq 1$

$R^2$: proportion of variation of $y$ explained by $x$.
<!-- R^2는 실제 y값의 분산 분의 추정치들의 분산. 모델에 의해 설명되는 변동 비율 -->

* $\frac{1}{n} \sum_{i=1}^n \hat{y_i} = \bar{y}$

## Supplement 1
### Geometry of Least Squares Method
Minimize $\sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2$ w.r.t. $\beta_0$ & $\beta_1$

Minimize $\mid y - (\beta_0 1 + \beta_1 x) \mid^2$ w.r.t. $\beta_0$ & $\beta_1$ where $y, 1, x$ are column vectors.

$(y - \beta_0 1 - \beta_1 x)^T (y - \beta_0 1 - \beta_1 x)$ <!-- 내적 -->

#### Perpendicular projection onto a vector
<!-- 여기 다시 읽어봐야할듯 -->
$(y - \beta x) \perp x$

$\therefore X^T (y - X \beta) = 0$

$\therefore \hat{\beta} = (X^T X)^{-1} X^T y$

i.e. $X \cdot \beta = X(X^T X)^{-1} X^T y$: projection of $y$ onto $C(X)$ ($X$의 column space)

<!-- p14에서 x와 1이 만드는 평면과 y의 거리가 최소인 것을 찾는다. = 수직으로 직교하게 내리는 점 = 내적이 0이 되는  -->

- $1(1^T 1)^{-1} 1^T y = \bar{y} \cdot 1$
- $1(1^T 1)^{-1} 1^T x = \bar{x} \cdot 1$
  - $1^T 1 = n$
- $(x - \bar{x} 1)\\{(x - \bar{x}1)^T(x-\bar{x}1) \\}^{-1} (x - \bar{x}1)^T y = (x - \bar{x}1) \hat{\beta}_1$
  - $\because (x -\bar{x} 1)^T (\bar{y} 1) = 0$
- Therefore, $\hat{y} = \bar{y} 1 + \hat{\beta}_1 (x - \bar{x} 1) = \hat{\beta}_0 1 + \hat{\beta}_1 x$ where $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$

#### Meaning of coefficient of determination
SST = $\sum (y_i - \bar(y))^2$

SSE = $\sum (y_i - \hat{y}_i)^2$

SSR = $\sum (\hat{y}_i - \bar{y})^2$

$\cos^2 \theta$ = SSR/SST = $R^2$

$y$ gets closer to the plane $C(1, x)$ which is determined by $1, x$

### Poperties of Variance & Covariance of random variables
$cov(Y, Z) = E[(Y - EY)(Z-EZ)]$

- $cov(\sum_{i=1}^m a_j Y_j, \sum_{j=1}^n b_j Z_j) = \sum_{i=1}^m \sum_{j=1}^n a_i b_j cov(Y_i, Z_j)$
- $var(a_1 Y_1 + \ldots + a_n Y_n) = \sum_1^n a_i^2 var(Y_i) + \sum \sum_{i \neq j} a_i a_j cov(Y_i, Y_j)$
- If $Y$ and $Z$ are independent, $cov(Y, Z) = 0$
- If $Y_1, \ldots, Y_n$ are independent, $var(\sum_1^n a_i Y_i) = \sum_1^n a_i^2 var(Y_i)$

### Expectation, Variance & Covariance of random vectors
For random vector $Y = (Y_1, \ldots, Y_n)^T$ (column vector notation),

$EY = (EY_1, \ldots, EY_n)^T$ (mean vector),

$Var(Y) = \begin{bmatrix} var(Y_1) & cov(Y_1, Y_2) & . & cov(Y_1, Y_n) \\ . & . & . & . \\ cov(Y_n, Y_1) & cov(Y_n, Y_2) & . & var(Y_n) \end{bmatrix}$
$= cov(Y_i, Y_j)$ (variance-covariance matrix)

Note that $var(Y) = E[(Y-EY)(Y-EY)^T], cov(Y_i, Y_j) = E(Y_i - EY_i)(Y_j - EY_j), (a \cdot a^T)_{(i, j)} = a_i \cdot a_j$

- $E(AY + b) = AE(Y) + b$
  - $E(AY + b) = E(\sum_{j=1}^n a_{ij} Y_j + b_i) = \sum_{j=1}^n a_{ij} EY_j + b_i = AEY + b$
- $var(AY + b) = var(AY) = Avar(Y)A^T$
  - $var(AY + b) = E[(AY + b - E(AY + b))(AY + b - E(AY + b))^T] = E[(A(Y - EY))(A(Y - EY))^T] = E[A(Y - EY)(Y - EY)^T A^T] = AE[(Y - EY)(Y - EY)^T]A^T = A var(Y)A^T$

In simple (or multiple) linear regression model,

$E(Y) = X \beta, var(Y) = \sigma^2 I_n$

### Gradient vector
For $(n \times 1)$ vector $c, x$, then $c^T x = (c_1, \ldots, c_n) (x_1, \ldots, x_n)^T = \sum_{i=1}^n c_i x_i$.

Partial derivative of $c^T x$: $\frac{\partial{(c^T x)}}{\partial x} = (\frac{\partial{(c^T x)}}{\partial x_1}, \ldots, \frac{\partial{(c^T x)}}{\partial x_n})^T = (c_1, \ldots, c_n)^T = c$.
Similarty, $\frac{\partial{(x^T c)}}{\partial x} = (\frac{\partial{(x^T c)}}{\partial x_1}, \ldots, \frac{\partial{(x^T c)}}{\partial x_n})^T = (c_1, \ldots, c_n)^T = c$

For any matrix $A$ and $y = (y_1, \ldots, y_n)^T$,
$\frac{\partial{(y^T A y)}}{\partial y} = \sum_i \sum_j y_i a_{ij} y_j = (a_{11} y_1 + \sum_{j=2}^n a_{1j} y_j) + (a_{11} + y_1 + \sum_{i=2}^n y_i a_{i1}) = (A + A^T) y$.
When $A$ is symmetric ($A = A^T$), $\frac{\partial{(y^T A y)}}{\partial y} = (A + A^T) y = 2Ay$

### Properties of Least Squares Estimates
$Y_i$: independent and $x_1, \ldots, x_n$: constants

$EY_i = \beta_0 + \beta_1 x_i, var(Y_i) = \sigma^2 (i = 1, \ldots, n)$.

$\hat{\beta_1} = \frac{S_{xy}}{S_{xx}} = \frac{1}{S_{xx}} \sum_{i = 1}^n (x_i - \bar{x})(Y_i - \bar{Y}) = \sum_{i=1}^n \frac{x_i - \bar{x}}{S_{xx}} Y_i (\because \sum_{i=1}^n (x_i - \bar{x}) \bar{Y} = 0)$

$\hat{\beta_0} = \bar{Y} - \hat{\beta_1} \bar{x} = \sum_{i=1}^n (\frac{1}{n} - \frac{x_i - \bar{x}}{S_{xx}}\bar{x})Y_i$

$e_i = Y_i - (\hat{\beta_0} + \hat{\beta_1}x_i) = Y_i - \bar{Y} - \hat{\beta_1} (x_i - \bar{x}) = Y_i - \sum_{j = 1}^n (\frac{1}{n} + \frac{x_j - \bar{x}}{S_{xx}} (x_i - \bar{x})) Y_j$

#### $\hat{\beta_1}$
$E(\hat{\beta_1}) = \sum_{i=1}^n \frac{x_i - \bar{x}}{S_{xx}} EY_i = \sum_{i=1}^n \frac{x_i - \bar{x}} (\beta_0 + \beta_1 x_i) = \beta_0 \sum_{i=1}^n \frac{x_i - \bar{x}}{S_{xx}} + \beta_1 \sum_{i=1}^n \frac{(x_i - \bar{x})}{S_{xx}} x_i = \beta_1$
- $\sum (x_i - \bar{x}) x_i = \sum (x_i - \bar{x})(x_i - \bar{x}) = S_{xx}$

$var(\hat{\beta_1}) = \sum_{i=1}^n (\frac{x_i - \bar{x}}{S_{xx}})^2 var(Y_i) = \sum_{i=1}^n (\frac{x_i - \bar{x}}{S_{xx}})^2 \sigma^2 = \sigma^2 / S_{xx} (\because Y_1, \ldots, Y_n$ are independent)

#### $\hat{\beta_0}$
$E(\hat{\beta_0}) = \sum_{i=1}^n (n^{-1} - \frac{x_i - \bar{x}}{S_{xx}} \bar{x}) EY_i = \sum_{i=1}^n (n^{-1} - \frac{x_i - \bar{x}}{S_{xx}} \bar{x}) (\beta_0 + \beta_1 x_i) = \beta_0 + \beta_1 \bar{x} - 0 - \beta_1 \bar{x} = \beta_0$

$var(\hat{\beta_0}) = \sum_{i=1}^n (n^{-1} - \frac{x_i - \bar{x}}{S_{xx}} \bar{x})^2 var(Y_i) = (\sum_{i=1}^n n^{-2} - \sum_{i=1}^n 2 \frac{x_i - \bar{x}}{S_{xx}} \bar{x} n^{-1} + \sum_{i=1}^n \frac{(x_i - \bar{x})^2 }{S_{xx}^2} \bar{x}^2) \sigma^2 = (n^{-1} + \frac{\bar{x}^2 }{S_{xx}^2} \sum_{i=1}^n (x_i - \bar{x})^2) \sigma^2 = (n^{-1} + \frac{\bar{x}^2 }{S_{xx}}) \sigma^2$

$cov(\hat{\beta_1}, \hat{\beta_0}) = cov(\sum_{i=1}^n (n^{-1} - \frac{x_i - \bar{x}}{S_{xx}} \bar{x}) Y_i, \sum_{j=1}^n \frac{x_i - \bar{x}}{S_{xx}} Y_j) = \sum_{i=1}^n \sum_{j=1}^n (n^{-1} - \frac{x_i - \bar{x}}{S_{xx}} \bar{x})(\frac{x_j - \bar{x}}{S_{xx}}) cov(Y_i, Y_j) = \sum_{i=1}^n (n^{-1} - \frac{x_i - \bar{x}}{S_{xx}} \bar{x})(\frac{x_i - \bar{x}}{S_{xx}}) \sigma^2 = -\bar{x} \sigma^2 / S_{xx}$.

($\because cov(Y_i, Y_j) = 0$ for $i \neq j$)





