---
title: Simple Linear Regression
author: JSH
date: 2025-03-12 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Simple Linear Regression  
<!-- x가 하나 -->

## Regression Analysis
Study a functional relatoinship between variables.
- Response variable $y, Y$ (dependent variable)
- Explanatory variable $x, X$ (independent variable)

To explain the variability of $Y$.

Flow:
- Select plausible explanatory variables
- Collect data
- Assume a functional model (e.g., linear)
- Fit the model
- Check model validity
- Make inference

<!-- make inference: p-value로 가설 검정을 할 수 있다는 의미. 회귀모형의 p-value를 활용하려면 회귀 모델의 가정을 만족해야 하므로 이 가정을 만족하는지 확인하는 것이 반드시 필요하다. -->

## Simple linear regression model
$Y_i = \beta_0 + \beta_1 x_i + \epsilon_i, (x_1, \ldots, x_n$: non-random)
- $\epsilon_1, \ldots, \epsilon_n$: indepdnent random errors
- $E(\epsilon_i) = 0, Var(\epsilon_i) = \sigma^2 (i = 1, \ldots, n)$
- $\epsilon_i \sim N(0, \sigma^2) \Leftrightarrow Y_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2)$  <!-- inference를 위해 필요함. beta를 추정하려 하므로 x는 이미 알고있는 상수로 간주 -->

### LINE Assumption
- Linear
- Indepenent
- Normal
- Equal variance

## Method of estimation

### Least Squares Method
Minimize $\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2$ w.r.t. $\beta_0$ and $\beta_1$

#### Normal equation
$f(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2$
- $\frac{\partial l}{\partial \beta_0} = -2 \sum (y_i - \beta_0 - \beta_1 x_i) = 0 \Rightarrow \sum_{i=1}^n e_i = 0$
- $\frac{\partial l}{\partial \beta_1} = -2 \sum x_i (y_i - \beta_0 - \beta_1 x_i) = 0 \Rightarrow \sum_{i=1}^n x_i e_i = 0 \Rightarrow e_i = y_i - (\hat{\beta_0} + \hat{\beta_1 x_i})$ (residual)
  - $\sum x_i e_i - \bar{x} \sum e_i = \sum (x_i - \bar{x}) e_i = 0$

#### Least square estimates
From the equation $\sum (\hat{y_i} - \hat{\beta_0} - \hat{\beta_1} x_i) = 0$

(i) $n \hat{\beta_0} + \hat{\beta_1} \sum_i x_i = \sum \hat{y_i}$

(ii) $\hat{\beta_0} \sum (x_i - \bar{x}) + \hat{\beta_1} \sum (x_i - \bar{x}) x_i = \sum y_i (x_i - \bar{x})$
  - $\beta_0 \sum (x_i - \bar{x}) = 0$
  - $\hat{\beta_1} = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$

Therefore, $\hat{\beta_1} = S_{xy}/S_{xx}$ and $\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}$ where $S_{xy} = \sum (x_i - \bar{x})(y_i - \bar{y})$ and $S_{xx} = \sum (x_i - \bar{x})^2$

<!-- 한번 유도해보는 것 필요할듯하다 -->

#### Least squares regression fit
$\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}$

Therefore, $\hat{y} = \hat{\beta_0} + \hat{\beta_1} x = \bar{y} + \hat{\beta_1} (x - \bar{x})$

### Unbiased estimation of $\sigma^2$
$\hat{\sigma}^2 = \frac{1}{n-2} \sum_{i=1}^n (y_i - \hat{y_i})^2 = \frac{1}{n-2}$ SSE
- $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i$
- SSE: residual sum of squares (error sum of squares)

$n-2$: degrees of freedom
- $\sum e_i = 0$
- $\sum x_i e_i = 0$
<!-- 전체는 n인데 2개 식에 대한 것을 빼서 n-2가 됨 -->

## Method of inference

### Properties of estimates
- $E(\hat{\beta_1}) = \beta_1, Var(\hat{\beta_1}) = \sigma^2/S_{xx}$
- $E(\hat{\beta_0}) = \beta_0, Var(\hat{\beta_0}) = \sigma^2 (n^{-1} + \bar{x}^2/S_{xx})$
- $Cov(\hat{\beta_0}, \hat{\beta_1}) = -\sigma^2 \cdot \bar{x}/S_{xx}$
- $E(\hat{\sigma}^2) = \sigma^2, Var(e_i) = (1 - p_{ii}) \sigma^2, Cov(e_i, e_j) = -p_{ij} \sigma^2, E(e_i) = 0$ where $p_{ij} = \frac{1}{n} + \frac{(x_i - \bar{x})(x_j - \bar{x})}{S_{xx}}$

### Inference under additional normality assumption

#### Inference for $\beta_1$
$\frac{\hat{\beta_1} - \beta_1}{s.e.(\hat{\beta_1})} \sim t(n-2)$; $s.e.(\hat{\beta_1}) = \sqrt{\hat{var}(\hat{\beta_1})} = \hat{\sigma}/S_{xx}^{1/2}$

- $100 \times (1 - \alpha)%$ C.I.: $[\hat{\beta_1} - t(n-2; \alpha/2) \times s.e.(\hat{\beta_1}) \leq \beta_1 \leq \hat{\beta_1} + t(n-2;\alpha/2) \times s.e.(\hat{\beta_1})]$
- Reject $H_0$: $\beta_1 = \beta_1^0$ in favor of $H_1: \beta_1 \neq \beta_1^0$ iff $\mid \frac{\hat{\beta_1} - \beta_1^0}{s.e.(\hat{\beta_1})} \mid > t(n-2; \alpha/2)$
- p-value <!-- 양측검정 -->

"Test of significance" of explanatory variable
- $H_0: \beta_1 = 0$ vs. $H_1: \beta_1 \neq 0$
- CI: $\hat{\beta} \pm t_{0.05/2}(df) \cdot s.e.(\hat{\beta})$
- p-value / meaning: we've seen a data which can hardly be observed under $H_0$. We may reject $H_0$

<!-- beta_1 = 0으로 하고 검정할 수 있다. -->
<!-- beta_1를 주의깊게 보지, beta_0은 중요하지 않다. -->
<!-- t(df>=25) ~ N(0, 1) -->

#### Inference for $\beta_0$
$\frac{\hat{\beta_0} - \beta_0}{s.e.(\hat{\beta_0})} \sim t(n-2)$; $s.e.(\hat{\beta_0}) = \sqrt{\hat{var}(\hat{\beta_0})} = \hat{\sigma}(n^{-1} + \bar{x}/S_{xx})^{1/2}$

- $100 \times (1 - \alpha)%$ C.I.: $[\hat{\beta_0} - t(n-2; \alpha/2) \times s.e.(\hat{\beta_0}) \leq \beta_0 \leq \hat{\beta_0} + t(n-2;\alpha/2) \times s.e.(\hat{\beta_0})]$
- Reject $H_0$: $\beta_0 = \beta_0^0$ in favor of $H_1: \beta_0 \neq \beta_0^0$ iff $\mid \frac{\hat{\beta_0} - \beta_0^0}{s.e.(\hat{\beta_0})} \mid > t(n-2; \alpha/2)$

#### Inference for $\mu_0$

$Y_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2)$

$\mu_0 = E(Y \mid x_0) = \beta_0 + \beta_1 x_0$

$\hat{\mu_0} = \hat{\beta_0} + \hat{\beta_1} x_0$

$Var(\hat{\mu_0}) = Var(\hat{\beta_0} + \hat{\beta_1} x_0) = Var(\hat{\beta_0}) + Var(\hat{\beta_1})x_0^2 + 2x_0 Cov(\hat{\beta_0}, \hat{\beta_1})$
<!-- x는 상수취급이므로 variance 밖으로 나오도록 정리할 수 있다. -->
<!-- Var(X+Y) = Var(X) + Var(Y) + 2Cov(X, Y) -->

$\frac{\hat{\mu_0} - \mu_0}{s.e.(\hat{\mu_0})} \sim t(n-2)$; $s.e.(\hat{\mu_0}) = \sqrt{\hat{var}(\hat{\beta_0} + \hat{\beta_1} x_0)} = \hat{\sigma}(n^{-1} + (x_0 - \bar{x})^2/S_{xx})^{1/2}$

- $100 \times (1 - \alpha)%$ C.I.: $[\hat{\mu_0} - t(n-2; \alpha/2) \times s.e.(\hat{\mu_0}) \leq \mu_0 \leq \hat{\mu_0} + t(n-2;\alpha/2) \times s.e.(\hat{\mu_0})]$
- Test (not given)

#### Prediction for $y_0 = \beta_0 + \beta_1 x_0 + \epsilon_0 (\epsilon_0$: indepe of $\epsilon_1, \ldots, \epsilon_n$)
$\hat{y_0} = \hat{\beta_0} + \hat{\beta_1} x_0$

$Var(y_0) = Var(\hat{\mu_0} + \epsilon_0) = Var(\hat{\mu_0}) + Var(\epsilon_0)$ <!-- 독립이므로 Cov = 0 -->

$\frac{\hat{y_0} - y_0}{s.e.(y_0 - \hat{y_0})} \sim t(n-2)$; $s.e.(y_0 - \hat{y_0}) = \hat{\sigma}(1 + n^{-1} + (x_0 - \bar{x})^2/S_{xx})^{1/2}$
<!--prediction인 이유는 오차항 때문에 variance가 생기기 때문이다. 앞의 mu는 평균이기 때문에 오차항을 무시하고, 실제 y 하나하나를 prediction하면 오차항을 포함하므로 다르다 -->

- $100 \times (1 - \alpha)%$ Prediction interval: $[\hat{y_0} - t(n-2; \alpha/2) \times s.e.(y_0 - \hat{y_0}) \leq y_0 \leq \hat{y_0} + t(n-2;\alpha/2) \times s.e.(y_0 - \hat{y_0})]$

Note that $\hat{\mu_0}$ is identical to the predicted response $\hat{y_0}$ at any given $x_0$

All these are valid under the model assumptions.
Note that $H_0: \beta_0$ vs. $H_1: \beta_0 \neq 0$ can't be rejected even at 10%.
It means we may start with a simpler model ($y_i = \beta_1 x_i + \epsilon_i$, $\epsilon_i \sim N(0, \sigma^2))$.
Then, all the above inferences should be changed.


## Measuring the quality of fit

### Decomposition of Sum of Squares
#### Deviation
$y_i - \bar{y} = (y_i - \hat{y}_i) + (\hat{y}_i - \bar{y})$

#### Sum of squares
$\sum (y_i - \bar{y})^2 = \sum (y_i - \hat{y}_i)^2 + \sum (\hat{y}_i - \bar{y})^2$
- SST: $\sum (y_i - \bar{y})^2$, df = $n-1$  <!-- SST의 sum이 0이 되어야 하므로 n-1 자유도를 가진다 -->
- SSE: $\sum (y_i - \hat{y}_i)^2$, df = $n-2$  <!-- \sum e_i = 0, \sum x_i e_i = 0을 만족해야 하므로 자유도는 n-2 -->
- SSR: $\sum (\hat{y}_i - \bar{y})^2$, df = 1  <!-- \hat{y}_i - \bar{y} = \hat{\beta_1} (x_i - \bar{x})인데 x는 상수취급. 따라서 beta에 따라 달라짐. 자유도 1 -->

<!-- 자유도의 의미는 자유롭게 움직일 수 있는 데이터의 개수 -->


#### Expansion
$\sum (y_i - \bar{y})^2 = \sum (y_i - \hat{y}_i)^2 + \sum (\hat{y}_i - \bar{y})^2 + \sum 2(y_i - \hat{y}_i)(\hat{y}_i - \bar{y})$

$\sum 2(y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) = 2 \sum e_i \hat{\beta}_1 (x_i - \bar{x}) = 2 \hat{\beta}_1 (\sum x_i e_i - \bar{x} \sum e_i) = 0$

$\because \hat{y}_i = \bar{y} + \hat{\beta}_1 (x_i - \bar{x})$

### SSR
SSR = $\sum (\hat{y_i}-\bar{y})^2 = \hat{\beta_1}^2 \sum (x_i - \bar{x})^2 = S_{xy}^2 / S_{xx} = (\sum \frac{x_i - \bar{x}}{\sqrt{S_{xx}}} y_i)^2$

Remark 1: $\hat{\beta_1} = \frac{S_{xy}}{S_{xx}}$
Remark 2: $S_{xy} = \sum (x_i - \bar{x})(y_i - \bar{y}) = \sum(x_i - \bar{x})y_i$

<!-- \hat{\beta_1} = 0이면 SSR도 0 -->

### Coefficient of determination (Multiple Correlation Coeff)
$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}, 0 \leq R^2 \leq 1$

$R^2$: proportion of variation of $y$ explained by $x$.
<!-- R^2는 실제 y값의 분산 분의 추정치들의 분산. 모델에 의해 설명되는 변동 비율 -->

* $\frac{1}{n} \sum_{i=1}^n \hat{y_i} = \bar{y}$

## Supplement 1
### Geometry of Least Squares Method
Minimize $\sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2$ w.r.t. $\beta_0$ & $\beta_1$

Minimize $\mid y - (\beta_0 1 + \beta_1 x) \mid^2$ w.r.t. $\beta_0$ & $\beta_1$ where $y, 1, x$ are column vectors.

$(y - \beta_0 1 - \beta_1 x)^T (y - \beta_0 1 - \beta_1 x)$ <!-- 내적 -->

#### Perpendicular projection onto a vector
$(y - \beta x) \perp x$

$\therefore X^T (y - X \beta) = 0$

$\therefore \hat{\beta} = (X^T X)^{-1} X^T y$

i.e. $X \cdot \beta = X(X^T X)^{-1} X^T y$: projection of $y$ onto $C(X)$ ($X$의 column space)

<!-- p14에서 x와 1이 만드는 평면과 y의 거리가 최소인 것을 찾는다. = 수직으로 직교하게 내리는 점 = 내적이 0이 되는  -->




