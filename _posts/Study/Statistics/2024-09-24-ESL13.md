---
title: Prototype Methods and Nearest-Neighbors
author: JSH
date: 2024-09-24 08:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, ESL]
use_math: true
---

# Prototype Methods and Nearest-Neighbors

## Prototype
A prototype is a representative data point of a class or cluster.
Prototype methods are used in both clustering and classification.
Classification of a query point x is made to the class of the closest prototype.
Closest is usually defined by Euclidean distance, after each feature has been standardized.
The main challenge is to figure out how many prototypes to use and where to put them.

## Prototype Methods

### K-means Clustering
A method for finding clusters and cluster cneters in a set of unlabeled data.
K-means procedure iteratively moves the centers to minimize the total within cluster variance.

#### Algorithm
* Choose an initial set of center. Typically, they are randomly selected from the training set.
* For each center, identify the subset of training points that is closer to it than any other center.
* Compute the mean of each feature for the data points in each cluster, and this mean vector becomes the new center for that cluster.
* Iterate step 2 & 3 until convergence.

#### Classification
* Apply K-mean clustering to the training data in each class, using R prototypes per class.
* Assign a class label to each of the K x R prototypes.
* Classify a new feature x to the class of the closest prototype.

For each class, the other classes do not have a say in positioning of the prototypes for that class.
This leads to potential misclassification errors for points near the boundaries.

### Learning Vector Quantization (LVQ)
The training points attract prototypes of the correct class, and repel other prototypes.
The learning rate $\epsilon$ is following the guidelines for stochastic approximation learning rates.
The prototypes have tended to move away from the decision boundaries, and away from prototypes of competing classes.

#### Algorithm
* Choose $R$ initial prototypes for each class: $m_1(k), m_2(k), \ldots, m_R(k), k = 1, 2, \ldots, K$, for example, by sampling $R$ training points at random from each class
* Sample a training point $x_i$ randomly (with replacement), and let $(j, k)$ index the closest prototype $m_j(k)$ to $x_i$
  * If $g_i = k$ (i.e., they are in the same class), move the prototype towards the training point: $m_j(k) \rightarrow m_j(k) + \epsilon (x_i - m_j(k))$, where $\epsilon$ is the learning rate
  * If $g_i \neq k$ (i.e., they are in different classes), move the prototype away from the training point: $m_j(k) \rightarrow m_j(k) - \epsilon (x_i - m_j(k))$
* Repeat step 2, decreasing the learning rate $\epsilon$ with each iteration towards zero

### Gaussian Mixture Model (GMM)
Each cluster is described in terms of a Gaussian density, which has a centroid and a covariance matrix.

#### Expectation-Maximization (EM) Algorithm
* Randomly initialize $\Phi, \mu, \Sigma$
* E-step: Assign each observationa responsibility for each cluster

  $w_j^{(i)} := p(z^{(i)} = j \mid x^{(i)}; \Phi, \mu, \Sigma) = \frac{p(x^{(i)} \mid z^{(i)} = j; \mu, \Sigma) p(z^{(i)} = j; \Phi)}{\sum_{l = 1}^k p(x^{(i)} \mid z^{(i)} = l; \mu, \Sigma) p(z^{(i)} = l; \Phi)}$
* M-step: Update the parameters

  $\Phi_j := \frac{1}{m} \sum_{i = 1}^m w_j^{(i)}$

  $\mu_j := \frac{\sum_{i = 1}^m w_j^{(i)} x^{(i)}}{\sum_{i = 1}^m w_j^{(i)}}$

  $\Sigma_j := \frac{\sum_{i = 1}^m w_j^{(i)} (x^{(i)} - \mu_j) (x^{(i)} - \mu_j)^T}{\sum_{i = 1}^m w_j^{(i)}}$
* Iterate steps E & M until convergence

## k-Nearest-Neighbors (k-NN)
Given a query point $x_0$, find the k training points $x_{(r)}, r = 1, \ldots, k$ closest in distance to $x_0$, and then classify using majority vote among the k neighbors.
Ties are broken at random.

In 1-nearest-neighbor classification, each training point is a prototype.

When k is too small, it is more sensitive to individual points, increasing the risk of overfitting due to noise.
When k is too large, it becomes smoother and less sensitive to noise, but too large k can cause underfitting by making the model overly simplistic.

### Invariant Metrics and Tangent Distance
Invariant metrics are distance measures that remain the same or change very little, even when certain transformations are applied.
Tangent distance, a type of invariant metric, can be used in k-NN's distance calculations.
This enables KNN to operate more accurately in image or pattern recognition tasks, even when small transformations like rotatoin or translation occur.

Using the shortest Euclidean distance between curves formed by rotated versions of each image is the most ideal method.
However, calculating this distance in actual images is very challenging, and errors such as misclassifying a 6 as 9 can occur.
Therefore, using Tangent Vectors is more suitable, as it considers only small rotations.

## Adaptive Nearest-Neighbor Methods
This method is designed to overcome the limitations of NN algorithms that use a fixed k value and to work flexibly according to the density or distribution of the data.

* Friedman proposed a method in which rectangular neighborhoods are found adaptively by successively carving away edges of a box containing the training data.
* Discriminant Adaptive Nearest Neighbor (DANN)
  * Discriminant Analysis (LDA)
  * Mahalanobis Distance: $d_M (x, y) = \sqrt{(x - y)^T \Sigma^{-1} (x - y)}$
