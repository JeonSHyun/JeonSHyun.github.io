---
title: Distribution of functions of random variables
author: JSH
date: 2024-08-14 10:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, IMS]
use_math: true
---

# Distributions of functions of random variables

## Transformations of Bivariate Random Variables
For $Y = (Y_1, Y_2) = g(X_1, X_2) = (g_1(X_1, X_2), g_2(X_1, X_2))$,
<!-- random variable이 2개가 아니라 n개 있는 경우에도 동일하게 가능 -->

### Discrete-type
#### One-to-one case
When $g(x_1, x_2)$: one-to-one transformation that maps $S_X$ onto $S_Y$

$p_{Y_1, Y_2}(y_1, y_2) = p_{X_1, X_2}(g^{-1}(y_1, y_2)), (y_1, y_2) \in S_Y$

#### Many-to-one case
When $g(x_1, x_2)$: many-to-one

$p_{Y_1, Y_2}(y_1, y_2) = \sum_{g(x_1, x_2) = (y_1, y_2)} p_{X_1, X_2}(x_1, x_2)$

#### Example
Let $X_1$ and $X_2$ have the joint pmf
$p_{X_1, X_2}(x_1, x_2) = \frac{\mu_1^{x_1} \mu_2^{x_2} e^{-\mu_1} e^{-\mu_2}}{x_1 ! x_2 !}, x_1 = 0, 1, 2, \ldots, x_2 = 0, 1, 2, \ldots$
where $\mu_1 > 0, \mu_2 > 0$.

We wish to find the pmf of $Y_1 = X_1 + X_2$.

$X_1 \sim Poission(\mu_1), X_2 \sim Poisson(\mu_2)$
<!-- (X_1, X_2) -> Y_1 인 many-to-one case. -->

$Y_1 = X_1 + X_2, Y_2 = X_2$   <!-- Y2를 이와 같이 정의하는 트릭을 쓴다 -->

<!-- (X_1, X_2) -> (Y_1, Y_2) 인 경우에는 one-to-one이 된다. 1번째 step -->

$\sum_{y_2} f_{Y_1, Y_2}(y_1, y_2) = f_{Y_1}(y_1)$  <!-- 2번째 step -->

$p_{Y_1, Y_2}(y_1, y_2) = p_{X_1, X_2}(y_1 - y_2, y_2) = \frac{\mu_1^{y_1 - y_2} \mu_2^{y_2} e^{-\mu_1} e^{-\mu_2}}{(y_1 - y_2)! y_2 !}$

Support: $y_2 \leq y_1, y_2 \geq 0$

$\sum_{y_2 = 0}^{y_1} p_{Y_1, Y_2}(y_1, y_2) = \sum_{y_2 = 0}^{y_1} \frac{\mu_1^{y_1 - y_2} \mu_2^{y_2}}{(y_1 - y_2)! y_2!} e^{-\mu_1 - \mu_2} = [\sum_{y_2 = 0}^{y_1} \frac{y_1 !}{(y_1 - y_2)! y_2 !} \mu_1^{y_1 - y_2} \mu_2^{y_2}] \frac{e^{-\mu_1 -\mu_2}}{y_1 !} = e^{-\mu_1 - \mu_2} \frac{(\mu_1 + \mu_2)^{y_1}}{y_1 !}$

$\Rightarrow Poisson(\mu_1 + \mu_2)$

Therefore, if $X_1 \sim Poisson(\mu_1)$ and $X_2 \sim Poisson(\mu_2)$, $X_1 + X_2 \sim Poisson(\mu_1 + \mu_2)$

<!-- 포아송 분포의 여러 개의 합도 같은 형식 -->

### Continuous-type
#### cdf technique
For $Z = g(X, Y), f_Z(z) = \frac{d}{dz} F_Z(z), F_Z(z) = P(Z \leq z) = P(g(X, Y) \leq z)$


#### Example
Choose at random a point $(X, Y)$ from the unit square $S = \\{(x, y): 0 < x, y < 1 \\}.$
Find the pdf of $Z = X + Y$.
* joint pdf of $(X, Y)$.
  * $F_{X, Y}(x, y) = P(X \leq x, Y \leq y) = xy$ if $0 < x < 1$ and $0 < y < 1$, 0 if $x \leq 0$ or $y \leq 0$, 1 if $x \geq 1, y \geq 1$
  * $f_{X, Y}(x, y) = 1$ if $0 < x, y < 1$ 0 o. w.
* cdf and pdf of $Z$
  * $F_Z(z) = P(Z \leq z) = P(X + Y \leq z) = \int_0^z \int_0^{z - x} f_{X, Y}(x, y) dy dx$
    * if $0 \leq z < 1: F_Z(z) = \int_0^z \int_0^{z - x} 1 dy dx = z^2/2$
    * if $1 \leq z < 2: F_Z(z) = 1 - 1/2(2 - z)^2$
    * if $z \geq 2: 1$
    * if $z < 0: 0$
  * $f_z(z)$
    * if $0 \leq z < 1: z$
    * if $1 \leq z < 2: 2 - z$
    * 0 o.w.

<!-- 다중적분은 면적과 같다. 그림을 그리고 경우를 나누어서 할 것 -->
<!-- 이 부분에 여러가지 예시를 들어주면서 다중적분을 시도하니까 잘 기억해두기... -->


#### (variable) transformation technique
For $(Y_1, Y_2) = g(X_1, X_2) = (g_1(X_1, X_2), g_2(X_1, X_2)$,

##### One-to-one case
Suppose $g$: 1-to-1,
$f_{Y_1, Y_2}(y_1, y_2) = f_{X_1, X_2}(g^{-1}(y_1, y_2)) \mid J \mid$ if $(y_1, y_2) \in S_Y$, and 0 if o.w.  <!-- |J|는 J의 det. ad-bc -->

where $J = \begin{vmatrix} \frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} \\ \frac{\partial x_2}{\partial y_1} & \frac{\partial x_2}{\partial y_2} \end{vmatrix}$: Jacobian,
and $(x_1, x_2) = g^{-1}(y_1, y_2)$

##### Many-to-one case
$\\{S_0, S_1, \ldots, S_r \\}$: partition of $S_X$ and $P(X \in S_0) = 0$.

Suppose $g_i$ is 1-to-1 mapping from $S_i$ onto $T_i$ such that $g_i(x_1, x_2) = g(x_1, x_2)$ for $(x_1, x_2) \in S_i$,
$f_{Y_1, Y_2}(y_1, y_2) = \sum_{(y_1, y_2) \in T_i} f_{X_1, X_2}(g^{-1}(y_1, y_2)) \mid J \mid$ if $(y_1, y_2) \in S_Y - T_0$ and 0 if o.w.

where $T_0 = \\{(y_1, y_2): (y_1, y_2) = g(x_1, x_2), (x_1, x_2) \in S_0 \\}, (\overset{*}{g_{i1}}(y_1, y_2), \overset{*}{g_{i2}}(y_1, y_2)) = g_i^{-1}(y_1, y_2), 
$J = \begin{vmatrix} \frac{\partial \overset{*}{g_{i1}}}{\partial y_1} & \frac{\partial \overset{*}{g_{i1}}}{\partial y_2} \\ \frac{\partial \overset{*}{g_{i2}}}{\partial y_1} & \frac{\partial \overset{*}{g_{i2}}}{\partial y_2} \end{vmatrix}$

#### Example
Suppose $(X_1, X_2)$ have the joint pdf, $f_{X_1, X_2}(x_1, x_2) = 2$ if $0 < x_1 < x_2 < 1$ and 0 if o.w.

Find the joint pdf of $Y_1 = X_1/X_2$ and $Y_2 = X_2$, and the marginal pdf of $Y_2$?  <!-- one-to-one case -->

$X_1 = Y_1 Y_2, X_2 = Y_2, J = y_2$

$f_{Y_1, Y_2}(y_1, y_2) = f_{X_1, X_2}(y_1 y_2, y_2) \mid J \mid = \mid 2 y_2 \mid$ if $0 < y_1 y_2 < y_2$, 0 if o.w.

For $0 < y_1 y_2 < y_2$
* $(y_1, y_2) = (+, +)$
  * $y_1 y_2 < y_2 \Leftrightarrow y_2 (1 - y_1) > 0$
  * Therefore, $0 < y_1 < 1, 0 < y_2 < 1$
* $(y_1, y_2) = (-, -)$
  * $1 - y_1 < 0$ : No result

$f_{Y_2}(y_2) = \int_{-\infty}^{\infty} f_{Y_1, Y_2}(y_1, y_2) dy_1 = \int_0^1 2 y_2 dy_1 = 2 y_2$

#### Example
$f_{X_1, X_2}(x_1, x_2) = \frac{1}{\Gamma(\alpha) \Gamma(\beta) \theta^{\alpha + \beta}} x_1^{\alpha - 1} x_2^{\beta - 1} \exp (-\frac{x_1 + x_2}{\theta}) I_{(0, \infty)}(x_1) I_{(0, \infty)}(x_2)$

$Y_1 = \frac{X_1}{X_1 + X_2}, Y_2 = X_1 + X_2$

$X_1 = Y_1 Y_2, X_2 = Y_2 - Y_1 Y_2, J = y_2(1 - y_1) + y_1 y_2 = y_2$

$f_{Y_1, Y_2}(y_1, y_2) = f_{X_1, X_2}(y_1 y_2, y_2 - y_1 y_2) \cdot \mid y_2 \mid = \frac{1}{\Gamma(\alpha) \Gamma(\beta) \theta^{\alpha + \beta}} (y_1 y_2)^{\alpha - 1} (y_2 - y_1 y_2)^{\beta - 1} \exp (-\frac{y_2}{\theta}) \mid y_2 \mid$

$= \frac{1}{\Gamma(\alpha) \Gamma(\beta) \theta^{\alpha + \beta}} y_1^{\alpha - 1} (1 - y_1)^{\beta - 1} y_2^{\alpha + \beta - 1} \exp (-\frac{y_2}{\theta})$

$= \frac{1}{\Gamma(\alpha + \beta) \theta^{\alpha + \beta}} y_2^{\alpha + \beta - 1} e^{-y_2/\theta} y_1^{\alpha - 1} (1 - y_1)^{\beta - 1} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}$

Support: $y_1 y_2 > 0, y_2 (1 - y_1) > 0 \Rightarrow y_2 > 0, 0 < y_1 < 1$

* $\int \frac{1}{\Gamma(\alpha + \beta) \theta^{\alpha + \beta}} y_2^{\alpha + \beta - 1} e^{-y_2/\theta} = 1$  <!-- 감마분포이기 때문에 -->

Therefore, $f_{Y_1}(y_1) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} y_1^{\alpha - 1} (1 - y_1)^{\beta - 1}$ and $0 < y_1 < 1$

which is Beta distribution

* $X_1 \sim \Gamma(\alpha, \theta), X_2 \sim \Gamma(\beta, \theta)$
* $\frac{X_1}{X_1 + X_2} \sim Beta(\alpha, \beta)$


## Independent random variables

### Random samples
Random samples refere to taking a number of independent random variables from the same probability distribution. 
If $X_1, \ldots, X_n$ are independent, the joint pdf is the product of the respective pdf, that is, $f_1(x_1)f_2(x_2) \ldots f_n(x_n)$.
If $X_1, \ldots, X_n$ are random sample of size $n$ and we assume that $X_i$ is from $f(x)$, then the joint pdf is $f(x_1)f(x_2) \ldots f(x_n)$.

<!-- 독립인 경우에 joint pdf는 각 marginal pdf들의 곱으로 표현된다 -->

### Expectation of random samples
Let $X_1, \ldots, X_n$ be $n$ independent random variables. Then,
* $E(u(X_1, \ldots, X_n)) = \int_{x_1, \ldots, x_n} u(x_1, \ldots, x_n) f_1(x_1) \ldots f_n(x_n) dx_1 \ldots dx_n$
* $E(u_1(X_1) \ldots u_n(X_n)) = E[u_1(X_1)] \ldots E[u_n(X_n)]$

#### Example
If $X_1, X_2, \ldots, X_n$ are $n$ independent random variables with respective means, $\mu_1, \ldots, \mu_n$ and variances $\sigma_1^2, \ldots, \sigma_n^2$.
Then, what are the mean and the variance of $Y = \frac{1}{n} \sum_{i = 1}^n X_i, Y = \sum_{i = 1}^n a_i X_i$ ?

$E[\frac{1}{n} \sum_{i = 1}^n X_i] = \frac{1}{n} \sum_{i = 1}^n E(X_i) = \frac{1}{n} \sum_{i = 1}^n \mu_i$

$Var(\frac{1}{n} \sum_{i = 1}^n X_i) = \frac{1}{n^2} Var(\sum_{i = 1}^n E(X_i)) = \frac{1}{n^2} [\sigma_1^2 + \ldots + \sigma_n^2]$  <!-- 공분산은 독립이므로 0 -->

$E[\sum_{i = 1}^n a_i X_i] = \sum_{i = 1}^n a_i \mu_i$

$Var(\sum_{i = 1}^n a_i X_i) = \sum_{i = 1}^n a_i^2 \sigma_i^2$

* $Var(a_1 X_1 + a_2 X_2) = a_1^2 Var(X_1) + a_2^2 Var(X_2) + 2 a_1 a_2 Cov(X_1, X_2)$
* $Cov(X_1, X_2) = E((X_1 - \mu_1)(X_2 - \mu_2))$


### Statistic
Any function of the sample observations, $X_1, \ldots, X_n$, is called a statistic.
For example, $\bar{X} = \frac{1}{n} \sum_{i = 1}^n X_i$ is a statistic.  <!-- 통계량은 확률변수의 함수 -->


## The moment-generating function technique
Use the fact: $\exists h>0, M_X(t) = M_Y(t), -h < t < h \Leftrightarrow F_X(w) = F_Y(w) \forall w \in R$

### mgf of random variables
If $X_1, X_2, \ldots, X_n$ are $n$ independent random variables with respective mgf $M_{X_i}(t), i = 1, \ldots, n$.
Then mgf of $Y = \sum_{i=1}^n a_i X_i$ is $M_Y(t) = \prod_{i=1}^n M_{X_i}(a_i t)$.  <!-- \prod는 전부 곱한다는 뜻 -->

* M_Y(t) = E(e^{Yt}) = E(e^{\sum_{i = 1}^n a_i X_i t}) = \prod_{i = 1}^n E(e^{X_i (a_i t)}) = \prod_{i=1}^n M_{X_i}(a_i t)$

#### Example
For $\mu_1, \mu_2 > 0, p_{X_1, X_2}(x_1, x_2) = \frac{\mu_1^{x_1} \mu_2^{x_2} e^{-\mu_1} e^{-\mu_2}}{x_1 ! x_2 !}, x_1 = 0, 1, 2, \ldots, x_2 = 0, 1, 2, \ldots.$

Let $Y = X_1 + X_2$ and find the mgf of $Y$.

$X_1, X_2$ are independent, and $X_1 \sim Poisson(\mu_1), X_2 \sim Poisson(\mu_2)$

$M_{X_1}(t) = \exp(\mu_1(e^t - 1)), M_{X_2}(t) = \exp(\mu_2(e^t - 1))$

$M_Y(t) = \exp(\mu_1(e^t - 1) + \mu_2(e^t - 1)) = \exp((\mu_1 + \mu_2)(e^t - 1))$

Therefore, $X_1 + X_2 \sim Poisson(\mu_1 + \mu_2)$


#### Example
$f_{X_1, X_2}(x_1, x_2) = 1/4 \exp(-\frac{x_1 + x_2}{2}), 0 < x_1 < \infty, 0 < x_2 < \infty$, and 0 o.w.

Find the pdf of $Y = 1/2(X_1 - X_2)$ by mgf technique.

$X_1 \sim \Gamma(1, 2), X_2 \sim \Gamma(1, 2)$

$M_{X_1}(t) = (1 - 2t)^-1, M_{X_2}(t) = (1 - 2t)^-1$

$M_Y(t) = E(e^{1/2(X_1 - X_2)t}) = E(e^{1/2 X_1 t})E(e^{-1/2 X_2 t}) = (1 - t)^{-1} (1 + t)^{-1} = (1 - t^2)^{-1}$ : double exponential distribution

* if $Y = X_1 + X_2$
  * $M_Y(t) = E(e^{X_1 + X_2}t) = E(e^{X_1 t})E(e^{X_2 t}) = (1 - 2t)^-2$
  * $X_1 + X_2 \sim \Gamma(2, 2)$


### Related property
* Let $X_1, \ldots, X_n$ be independent random variables with $X_i \sim B(m_i, p)$. Then, $X_1 + \ldots + X_n \sim B(m_1 + m_2 + \ldots + m_n, p)$
* Let $X_1, \ldots, X_n$ be independent random variables with $X_i \sim Poisson(m_i)$. Then, $X_1 + \ldots + X_n \sim Poisson(m_1 + \ldots + m_n)$
* Let $X_1, \ldots, X_n$ be independent random variables with $X_i \sim \Gamma (\alpha_i, \beta)$. Then, $X_1 + \ldots + X_n \sim \Gamma (\alpha_1 + \alpha_2 + \ldots + \alpha_n, \beta)$
* Let $X_1, \ldots, X_n$ be independent random variables with $X_i \sim \chi^2(DF = r_i)$. Then, $X_1 + \ldots + X_n \sim \chi^2(r_1 + r_2 + \ldots + r_n)$


## Random functions associated with normal distribution

### Related properties
* For $X \sim N(\mu, \sigma^2)$,
  * $aX + b \sim N(a \mu + b, a^2 \sigma^2)$
  * $Z = \frac{X - \mu}{\sigma} \sim N(0, 1)$
* For $X_1, \ldots, X_n \sim iid N(\mu, \sigma^2)$
  * $\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i \sim N(\mu, \frac{\sigma^2}{n})$
* Suppose $X_1, \ldots, X_n$ are independent random variables with $X_i \sim N(\mu_i, \sigma_i^2)$
  * $a_1 X_1 + \ldots + a_n X_n \sim N(a_1 \mu_1 + \ldots + a_n \mu_n, a_1^2 \sigma_1^2 + \ldots + a_n^2 \sigma_n^2)$
* For $Z \sim N(0, 1)$
  * $Z^2 \sim \chi^2(1)$
* For $Z_1, \ldots, Z_r \sim iid N(0, 1)$
  * $V = Z_1^2 + \ldots + Z_r^2 \sim \chi^2 (r)$

#### Example
Let $X_1, X_2, \ldots, X_n \sim iid N(\mu, \sigma^2)$ and define $\bar(X) = \frac{1}{n} \sum_{i=1}^n X_i, S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2$.
Then,
* $\bar{X}$ & $S^2$ are independent
* $\frac{(n-1)S^2}{\sigma^2} \sim \chi^2 (n-1)$
* $\frac{\bar{X} - \mu}{\sqrt{S^2 / n}} \sim T(n - 1)$  <!-- 자유도가 n-1인 t분포. 증명은 수업을 다시 듣거나 솔루션.. -->


### Student $t$-distribution
$T \sim t(r)$ for $r$: positive integer $\Leftrightarrow$ distributoin of $T = \frac{W}{\sqrt{V/r}}$

where $W$ and $V$ are independent and $W \sim N(0, 1)$ & $V \sim \chi^2 (r)$
* pdf: $f(x) = \frac{\Gamma((r+1)/2)}{\Gamma(1/2) \Gamma(r/2)}(1 + \frac{t^2}{r})^{-\frac{r+1}{2}} \frac{1}{\sqrt{r}}, -\infty < x < \infty$
* mean: $E(T) = 0$ where $r > 1$
* variance: $Var(T) = \frac{r}{r-2}$ where $r > 2$

<!-- t 분포는 자유도가 굉장히 크면 표준정규분포랑 비슷해진다. 자유도가 30 이상이면 표준정규분포와 비슷해진다고 이야기한다 -->


### F-distribution
$F \sim F(r_1, r_2)$ for $r_1$ & $r_2$: positive integers

$\Leftrightarrow$ distribution of $F = \frac{U/r_1}{V/r_2}$ where $U, V$ are independent and $U \sim \chi^2(r_1), V \sim \chi^2(r_2)$

* pdf: $f(x) = \frac{\Gamma((r_1 + r_2)/2)}{\Gamma(r_1/2) \Gamma(r_2/2)} (\frac{r_1}{r_2})^{-\frac{r_1}{2}} \frac{x^{r_1/2 - 1}}{(1 + r_1 x/r_2)^{(r_1 + r_2)/2}}, 0 < x < \infty$
* mean: $E(F) = \frac{r_2}{r_2 - 2}$ where $r_2 > 2$


#### Example
Let
* $X_1, \ldots, X_n \sim iid N(\mu_1, \sigma^2)$ & $Y_1, \ldots, Y_m \sim iid N(\mu_2, \sigma^2)$
* $X_1, \ldots, X_n$ and $Y_1, \ldots, Y_m$ are independent samples
* Define $S_1^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2, S_2^2 = \frac{1}{m-1} \sum_{j=1}^m (Y_j - \bar{Y})^2$

Then, $F = \frac{S_1^2}{S_2^2} \sim F(n-1, m-1)$

* $\frac{n - 1}{\sigma^2}S_1^2 \sim \chi^2 (n - 1)$
* $\frac{m - 1}{\sigma^2}X_2^2 \sim \chi^2 (m - 1)$
