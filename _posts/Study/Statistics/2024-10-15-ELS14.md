---
title: Unsupervised Learning
author: JSH
date: 2024-10-15 08:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, ESL]
use_math: true
---

# Unsupervised Learning

## Introduction

### Supervised learning
Supervised learning is a method in which a model is trained to make predictions by learning from a dataset that includes both input data and the corresponding labels

### Unsupervised learning
Unsupervised learning is a method where only input data is provided, without labels or correct answers.
The lack of clear metrics to measure the effectiveness of algorithms leads to the proposal of various learning methods.


## Association Rules
Association rule analysis has emerged as a popular tool for mining commercial data bases.
The goal is to find joint values of the variables $X = (X_1, X_2, \ldots, X_p)$ that appear most frequently in the data base.

$Pr(\cap_{j = 1}^p (X_j \in s_j)), X_j \in \\{0, 1\\}$


### The Apriori algorithm
One of the major advances in data mining technology.
A method for finding frequent item sets and generating association rules based on them.

#### The metrics for measuring association rules
* Support: $T(A \Rightarrow B)$
* Confidence: $C(A \Rightarrow B) = \frac{T(A \Rightarrow B)}{T(A)}$
* Lift: $L(A \Rightarrow B) = \frac{C(A \Rightarrow B)}{T(B)}$


### Unsupervised as supervised learning
Let $g(x)$ be the unknown data probability density to be estimated, and $g_0(x)$ be a specified probability density function used for reference.

$N$ random samples were drawn from $g(x)$ and $N_0$ samples can be drawn from $g_0(x)$ using Monte Carlo methods. 
They results in a random sample drawn from the mixture density $(g(x) + g_0(x))/2$.

If one assigns the value $Y = 1$ 1 to each sample point drawn from $g(x)$ and $Y = 0$ those drawn from $g_0(x)$, then,

$\mu(x) = E(Y \mid x) = \frac{g(x)}{g(x) + g_0(x)} = \frac{g(x) / g_0(x)}{1 + g(x) / g_0(x)}$

The resulting estimate $\hat{\mu}(x)$ can be inverted to provide an estimate for $g(x)$

$\hat{g}(x) = g_0(x) \frac{\hat{\mu}(x)}{1 - \hat{\mu}(x)}$


## Cluster Analysis

### Clustering algorithms
Clustering is to gind groups of objects such that the objects in a group will be similar to one another and different from the objects in other groups.

Cluster analysis is an unsupervised learning technique that groups data objects based on their similarity.
The main objective is to make objects within clusters as similar as possible while maximizing the differences between clusters.

#### Degree of similarity (or dissimilarity) between the individual objects
* Proximity Matrices
* Dissimilarities based on attributes
  * $D(x_i, x_{j'}) = \sum_{j = 1}^p d_j(x_{ij}, x_{i'j}) \rightarrow d_j(x_{ij}, x_{i'j}) = (x_{ij} - x_{i'j})^2$
  * Quantitative variables: Monotone-increasing function for absolute difference. $d_j(x_{ij}, x_{i'j}) = l(\mid x_{ij} - x_{i'j})$
  * Ordinal variables: Scaling to $\frac{i - 1/2}{M}, i = 1, \ldots, M$ and treat as quantitative variables
  * Categorical variables: If the variable assumes $M$ distinct values, these can be arranged in a symmetric $M \times M$ matrix with elements $L_{rr'} = L_{r'r}, L_{rr} = 0, L_{rr'} \geq 0$
* Object dissimilarity
  * Overall measure of dissimilarity: $D(x_i, x_{i'}) = \sum_{j = 1}^p w_j \cdot d_j (x_{ij}, x_{i'j}); \sum_{j = 1}^p w_j = 1$, where $w_j =$ weight regulating the relative influence of $j$th attribute in overall dissimilarity
  * $\bar{D} = \frac{1}{N^2} \sum_{i = 1}^N \sum_{i' = 1}^N D(x_i, x_{i'}) = \sum_{j = 1}w_j \cdot \bar{d_j}$, with $\bar{d_j} = \frac{1}{N^2} \sum_{i = 1}^N \sum_{i' = 1}^N d_j (x_{ij}, x_{i'j})$
  * The relative influence of the $j$th attribute is $w_j \cdot \bar{d_j}$, and setting $w_j \sim 1/\bar{d_j}$ would give all attributes equal influence


### K-means clustering

#### Properties of K-means clustering
If the $i$ th observation is in the $k$ th cluster, the $i \in C_k$.
Each observation belongs to at least one of the $k$ clusters.
The clusters are non-overlapping; no observation belongs to more than one cluster.

The within-cluster variation for cluster $C_k$ is a measure $W(C_k)$ of the amount by which the observations within a cluster differ from each other.

$minimize_{C_1, \ldots, C_K} (\sum_{k = 1}^K W(C_k)), W(C_k) = \frac{1}{\mid C_k \mid} \sum_{i, i' \in C_k} \sum_{j = 1}^p (x_{ij} - x_{i'j})^2$.

$x$: independent variable, $\mid C_k \mid$: number of samples in cluster $k$ , $i: i$ th sample, $j: j$ th independent variable.

#### Algorithms of k-means clustering
* Randomly assign a number, from 1 to $K$, to each of the observations. These serve as initial cluster assignments for the observations.
* Iterate until the cluster assignments stop changing
  * For each of the $K$ clusters, compute the cluster centroid. The $k$ th cluster centroid is the vector of the $p$ feature means for the observations in the $k$ th cluster
  * Assign each observation to the cluster whose centroid is clsest (where closest is defined using Euclidean distance)


### Hierarchical clustering
One potential disadvantage of $K$ means clustering is that it requires us to pre specify the number of clusters $K$ .
Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of $K$.
Hierarchical clustering has an added advantage over K means clustering in that it results in an attractive tree based representation of the observations, called a dendrogram.

#### Algorithms of hierarchical clustering
* Begin with $n$ observations and a measure (such as Euclidean distance) of all the \binom{n}{2} = n(n-1)/2$ pairwise dissimilarities. Treat each observation as its own cluster
* For $i = n, n-1, \ldots, 2$:
  * Examine all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters indicates the height in the dendrogram at which the fusion should be placed
  * Compute the new pairwise inter-cluster dissimilarities among the $i-1$ remaining clusters.

#### Linkage methods
* Single Linkage (SL): minimal inter-cluster dissimilarity
  * Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities.
* Complete Linkage (CL): maximal inter-cluster dissimilarity
  * cluster dissimilarity. Compute all pairwise
dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.
* Average Linkage (AL): mean inter-cluster dissimilarity
  * cluster dissimilarity. Compute all pairwise
dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.
