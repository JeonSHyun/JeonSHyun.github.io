---
title: Additive models, trees, and related methods
author: JSH
date: 2024-07-16 10:08:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, ESL]
use_math: true
---

# Generalized Additive Models
## Multiple linear regression models
$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \epsilon_i$

## General additive model
$y_i = \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \ldots + f_p(x_{ip}) + \epsilon_i$


# Fitting Additive Models
## General additive model
$Y = \alpha + \sum_{j=1}^p f_j(X_j) + \epsilon_i$

## penalized sum of squares (PRSS)
PRSS = $\sum_{i=1}^N (y_i - \alpha - \sum_{j=1}^p f_j(X_j))^2 + \sum_{j=1}^p \lambda_j \int f''_j (t_j)^2 dt_j$

$\rightarrow f_j$ miniming PRSS is a cubic spline in the component $X_j$

To find the solution (each $f_j$), a simple iterative procedure backfitting algorithm is used

<!--- 그래서 general additive model을 fitting할 때, backfitting algorithm으로 각 변수의 함수 f를 구한 뒤, PRSS로 페널티 람다를 최적화한다는 게 지금까지 내가 이해한 내용이다--->

## The Backfitting Algorithm for Additive Models
* Initialize: $\hat{\alpha} = \frac{1}{N} \sum_1^N y_i, \hat{f_j} \equiv 0, \forall i, j$
* Cycle: $j = 1, 2, \ldots, p, 1, 2, \ldots, p, \ldots,$
  
  $\hat{f_j} \leftarrow S_j[\\{y_i - \hat{\alpha} - \sum_{k \neq j} \hat{f_k}(x_{ik}) \\}_1^N]$
  
  $\hat{f_j} \leftarrow \frac{1}{N} \sum_{i=1}^N \hat{f_j} (x_{ij})$
  
  until the function $\hat{f_j}$ change less than a prespecified threshold

  $S_j$: smoothing

Repeatedly updating the fit fore each predictor, holding the others fixed, by using partial residual.

## Pros and Cons of GAMs
* Automatically modelling non-linear relationships that standard linear regression will miss
* By additivity, we can examine the effect of each $X_j$ on $Y$ individually while holding all of the other variables fixed
* The smoothness of the function $f_j$ for the variable $X_j$ can be summarized via degrees of freedom $df_{\lambda}$
* GAM is restricted to be additive, thereby important interactions can be missed. But, we can manually add interaction terms to the GAM.


# Tree-based Methods

## The Basics of Decision Trees
Stratification or segmentation of predictor space into a number of regions.

The set of splitting rules used to segment the predictor space can be summarized in a tree (decision tree)

## Tree-based Regression and Classification (CART)
### Regression tree



