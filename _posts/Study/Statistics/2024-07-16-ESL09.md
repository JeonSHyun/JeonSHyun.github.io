---
title: Additive models, trees, and related methods
author: JSH
date: 2024-07-16 10:08:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, ESL]
use_math: true
---

# Generalized Additive Models
## Multiple linear regression models
$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \epsilon_i$

## General additive model
$y_i = \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \ldots + f_p(x_{ip}) + \epsilon_i$


# Fitting Additive Models
## General additive model
$Y = \alpha + \sum_{j=1}^p f_j(X_j) + \epsilon_i$

## penalized sum of squares (PRSS)
PRSS = $\sum_{i=1}^N (y_i - \alpha - \sum_{j=1}^p f_j(X_j))^2 + \sum_{j=1}^p \lambda_j \int f''_j (t_j)^2 dt_j$

$\rightarrow f_j$ miniming PRSS is a cubic spline in the component $X_j$

To find the solution (each $f_j$), a simple iterative procedure backfitting algorithm is used

<!--- 그래서 general additive model을 fitting할 때, backfitting algorithm으로 각 변수의 함수 f를 구한 뒤, PRSS로 페널티 람다를 최적화한다는 게 지금까지 내가 이해한 내용이다--->

## The Backfitting Algorithm for Additive Models
* Initialize: $\hat{\alpha} = \frac{1}{N} \sum_1^N y_i, \hat{f_j} \equiv 0, \forall i, j$
* Cycle: $j = 1, 2, \ldots, p, 1, 2, \ldots, p, \ldots,$
  
  $\hat{f_j} \leftarrow S_j[\\{y_i - \hat{\alpha} - \sum_{k \neq j} \hat{f_k}(x_{ik}) \\}_1^N]$
  
  $\hat{f_j} \leftarrow \frac{1}{N} \sum_{i=1}^N \hat{f_j} (x_{ij})$
  
  until the function $\hat{f_j}$ change less than a prespecified threshold

  $S_j$: smoothing

Repeatedly updating the fit fore each predictor, holding the others fixed, by using partial residual.

## Pros and Cons of GAMs
* Automatically modelling non-linear relationships that standard linear regression will miss
* By additivity, we can examine the effect of each $X_j$ on $Y$ individually while holding all of the other variables fixed
* The smoothness of the function $f_j$ for the variable $X_j$ can be summarized via degrees of freedom $df_{\lambda}$
* GAM is restricted to be additive, thereby important interactions can be missed. But, we can manually add interaction terms to the GAM.


# Tree-based Methods

## The Basics of Decision Trees
Stratification or segmentation of predictor space into a number of regions.

The set of splitting rules used to segment the predictor space can be summarized in a tree (decision tree)

## Tree-based Regression and Classification (CART)

### Regression tree

#### Prediction via stratification of the feature space
* Divide the predictor space $X_1, X_2, \ldots, X_p$ into $J$ distinct and non-overlapping regions, $R_1, R_2, \ldots, R_J$
* The same prediction is made for all observations belonging to regions $R_j$, and the predictoin uses average or mode of the responses in $R_j$

  $f(x) = \sum_{j=1}^J c_j I(x \in R_j), \hat{c_j} = ave(y_i|x_i \in R_j)$

#### Recursive binary splitting by "top-down greedy" approach
* Starting with all of the data, consider a splitting variable $j$ and split point $s$, and define the pair of half planes: $R_1(j, s) = \\{X|X_j \leq s \\}, R_2(j, s) = \\{X|X_j > s \\}$
* Seek the value of $j$ and $s$ that makes the tree having the lowest RSS

  $min_{j, s}[min_{c_1} \sum_{x_i \in R_1 (j, s)} (y_i - c_1)^2 + min_{c_2} \sum_{x_i \in R_2(j, s)} (y_i - c_2)^2]$

* Iterate above processes until a stopping criterion is reached

#### Stopping criterion
Build the tree until the size of terminal node exceeds some threshold

Cost-complexity pruning
* Define a subtree $T \subset T_0$ that can be obtained by pruning $T_0$
* Select a tree-size tuning parameter $\alpha$ that minimizes below equation using a validation set

  $C_{\alpha}(T) = \sum_{j=1}^{|T|} \sum_{x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha |T|, |T| =$ number of terminal nodes

### Classification tree
Predictions are the majority class $k(m)$ in $R_m$

$k(m) = argmax_k(\hat{p_{mk}}), \hat{p_{mk}} = \frac{1}{N_m} \sum_{x_i \in R_m} I(y_i = k)$ (= class proportions)

#### Recursive binary splitting
* Misclassification rate: $\frac{1}{N_m} \sum_{x_i \in R_m} I(y_i \neq k) = 1 - \hat{p}_{mk(m)}$
* Gini index: $\sum_{k=1}^K \hat{p}_{mk} (1-\hat{p}_{mk})$
* Cross-entropy: $-\sum_{k=1}^K \hat{p}_{mk} log\hat{p}_{mk}$

# Patient Rule Induction Method (PRIM): Bump Hunting
* CART: partition feature space into boxes, trying to make different response averages for each box
* PRIM: find boxes in the feature space, but seeks boxes in which the response average is high (= bump hunting)

## Patient Rule Inductin Method
* Start with all of the training data, and a maximal box containing all of the data
* Consider shrinking the box by compressing one face, so as to peel off as $\alpha$ proportoin that produces the highest response mean in the remaining box (= top-down peeling off)
* Repeat step 2 until some minimal number of observations remain in the box (=bottom-up pasting)
* Expand the box along any face, as long as the resulting box mean increases
* Step 1-4 give a sequence of boxes, with different numbers of observations in each box. Use cross-validation to choose a member of the sequence. Call the box $B_1$
* Remove the data in box $B_1$ from the dataset and repeat steps 2-5 to obtain a second box, and continue to get as many boxes as desired


# Multivariate Adaptive Regressoin Splines (MARS)
MARS uses expansions in piecewise linear basis functions of the form $(x-t)_+$ and $(t-x)_+$, called reflected pair

Each function is piecewise linear, with a knot at the value $t$

$(x-t)_+ = \begin{cases} x - t, & x > t, \\ 0, & o.w. \end{cases}$

$(t-x)_+ = \begin{cases} t - x, & x < t, \\ 0, & o.w. \end{cases}$

The idea is to form reflected pairs for each input $X_j$ with knots at each observation $x_{ij}$ of that input.
Therefore, the collection of basis function is $C = \\{(X_j - t)_+, (t - X_j)_+ \\}_{t \in \\{x_{1j}, x_{2j}, \ldots, x_{Nj}}, j = 1, 2, \ldots, p}$

Instead of using the original inputs, we are allowed to us functions from the set $C$ and their products

$f(X) = \beta_0 + \sum_{m=1}^M \beta_m h_m(X), h_m(X)$ is a form of functions in $C$ and $\beta_m$ is estimated by minimizing SSR

## MARS forward model-building procedure
* The model starts with only the constant function $h_0(X) = 1$, and al functions in the set $C$ are candidate functions
* Consider a new basis function pair all products of a function $h_m$ in the model set $M$ with one of the reflected pairs in $C$
* Add to the model $M$ the term of the form $\hat{\beta}_{M+1} h_l(X) \cdot (X_j - t)_+ + \hat{\beta}_{M+2} h_l(X) \cdot (t - X_j)_+, h_l \in M$ that produces the largest decrease in training error
* The winning products are added to the model and the process is continued until the model set $M$ contains some present maximum number of terms

## MARS backward deletion for avoiding overfitting
At the end of forward model-bulding process, a large model of the form is produces: $f(X) = \beta_0 + \sum_{m=1}^M \beta_m h_m(X)$

Remove a term that causes the smallest increase in SSR from the model at each stage, producing an estimated best model $\hat{f}_{\lambda}$ ($\lambda$ = # of terms)

One could use GCV (Generalized cross-validation) to estimate the optimal value of $\lambda$

GCV($\lambda$) = $\frac{\sum_{i=1}^N (y_i = \hat{f}_{\lambda}(x_i))^2}{(1 - M(\lambda)/N)^2}$

## Relationship of MARS to CART
* Replace the piecewise linear basis functions by step functions, $I(x-t > 0)$ and $I(x-t \leq 0) \rightarrow$ Multiplying a pair of reflected step functions in MARS = splitting a node at the step in CART
* When a model term is involved in a multiplication by a candidate term, it gets replaced by the interaction
* MARS weighs in additive effets, while CART does in the binary-tree representations


