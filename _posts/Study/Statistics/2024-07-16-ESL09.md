---
title: Additive models, trees, and related methods
author: JSH
date: 2024-07-16 10:08:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, ESL]
use_math: true
---

# Generalized Additive Models
## Multiple linear regression models
$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \epsilon_i$

## General additive model
$y_i = \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \ldots + f_p(x_{ip}) + \epsilon_i$


# Fitting Additive Models
## General additive model
$Y = \alpha + \sum_{j=1}^p f_j(X_j) + \epsilon_i$

## penalized sum of squares (PRSS)
PRSS = $\sum_{i=1}^N (y_i - \alpha - \sum_{j=1}^p f_j(X_j))^2 + \sum_{j=1}^p \lambda_j \int f''_j (t_j)^2 dt_j$

$\rightarrow f_j$ miniming PRSS is a cubic spline in the component $X_j$

To find the solution (each $f_j$), a simple iterative procedure backfitting algorithm is used

<!--- 그래서 general additive model을 fitting할 때, backfitting algorithm으로 각 변수의 함수 f를 구한 뒤, PRSS로 페널티 람다를 최적화한다는 게 지금까지 내가 이해한 내용이다--->

## The Backfitting Algorithm for Additive Models
* Initialize: $\hat{\alpha} = \frac{1}{N} \sum_1^N y_i, \hat{f_j} \equiv 0, \forall i, j$
* Cycle: $j = 1, 2, \ldots, p, 1, 2, \ldots, p, \ldots,$
  
  $\hat{f_j} \leftarrow S_j[\\{y_i - \hat{\alpha} - \sum_{k \neq j} \hat{f_k}(x_{ik}) \\}_1^N]$
  
  $\hat{f_j} \leftarrow \frac{1}{N} \sum_{i=1}^N \hat{f_j} (x_{ij})$
  
  until the function $\hat{f_j}$ change less than a prespecified threshold

  $S_j$: smoothing

Repeatedly updating the fit fore each predictor, holding the others fixed, by using partial residual.

## Pros and Cons of GAMs
* Automatically modelling non-linear relationships that standard linear regression will miss
* By additivity, we can examine the effect of each $X_j$ on $Y$ individually while holding all of the other variables fixed
* The smoothness of the function $f_j$ for the variable $X_j$ can be summarized via degrees of freedom $df_{\lambda}$
* GAM is restricted to be additive, thereby important interactions can be missed. But, we can manually add interaction terms to the GAM.


# Tree-based Methods

## The Basics of Decision Trees
Stratification or segmentation of predictor space into a number of regions.

The set of splitting rules used to segment the predictor space can be summarized in a tree (decision tree)

## Tree-based Regression and Classification (CART)

### Regression tree

#### Prediction via stratification of the feature space
* Divide the predictor space $X_1, X_2, \ldots, X_p$ into $J$ distinct and non-overlapping regions, $R_1, R_2, \ldots, R_J$
* The same prediction is made for all observations belonging to regions $R_j$, and the predictoin uses average or mode of the responses in $R_j$

  $f(x) = \sum_{j=1}^J c_j I(x \in R_j), \hat{c_j} = ave(y_i|x_i \in R_j)$

#### Recursive binary splitting by "top-down greedy" approach
* Starting with all of the data, consider a splitting variable $j$ and split point $s$, and define the pair of half planes: $R_1(j, s) = \\{X|X_j \leq s \\}, R_2(j, s) = \\{X|X_j > s \\}$
* Seek the value of $j$ and $s$ that makes the tree having the lowest RSS

  $min_{j, s}[min_{c_1} \sum_{x_i \in R_1 (j, s)} (y_i - c_1)^2 + min_{c_2} \sum_{x_i \in R_2(j, s)} (y_i - c_2)^2]$

* Iterate above processes until a stopping criterion is reached

#### Stopping criterion
Build the tree until the size of terminal node exceeds some threshold

Cost-complexity pruning
* Define a subtree $T \subset T_0$ that can be obtained by pruning $T_0$
* Select a tree-size tuning parameter $\alpha$ that minimizes below equation using a validation set

  $C_{\alpha}(T) = \sum_{j=1}^{|T|} \sum_{x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha |T|, |T| =$ number of terminal nodes

### Classification tree
Predictions are the majority class $k(m)$ in $R_m$

$k(m) = argmax_k(\hat{p_{mk}}), \hat{p_{mk}} = \frac{1}{N_m} \sum_{x_i \in R_m} I(y_i = k)$ (= class proportions)

#### Recursive binary splitting
* Misclassification rate: $\frac{1}{N_m} \sum_{x_i \in R_m} I(y_i \neq k) = 1 - \hat{p}_{mk(m)}$
* Gini index: $\sum_{k=1}^K \hat{p}_{mk} (1-\hat{p}_{mk})$
* Cross-entropy: $-\sum_{k=1}^K \hat{p}_{mk} log\hat{p}_{mk}$

# Patient Rule Induction Method (PRIM): Bump Hunting

