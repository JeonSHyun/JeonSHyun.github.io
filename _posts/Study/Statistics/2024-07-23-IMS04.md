---
title: Bivariate Distribution
author: JSH
date: 2024-07-23 10:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, IMS]
use_math: true
---

# Bivariate Distribution

## Distributions of Two Random Variables

### Random Vector and Space

#### Random Vector
$X = (X_1, X_2)^t$: r. vector $\Leftrightarrow X_1, X_2$: random variable on the sample space $S$

#### Space of r. vector $X$
For a r. vector $X = (X_1, X_2)^t, S_p(X) = \\{(x_1, x_2): x_1 = X_1(c), x_2 = X_2(c), c \in S \\}$

### Joint distribution of r. vector

#### Joint cumulative distribution function
For $\forall (x_1, x_2) \in R^2, F_{X_1, X_2} (x_1, x_2) = P(X_1 \leq x_1, X_2 \leq x_2) = P(\\{c: X_1(c) \leq x_1 \\} \cap \\{c: X_2(c) \leq x_2 \\})$

#### Properties
* $F_{X_1, X_2}(x_1, x_2) - F_{X_1, X_2}(y_1, x_2) + F_{X_1, X_2}(y_1, y_2) = P(y_1 < X_1 \leq x_1, y_2 < X_2 \leq x_2)$
* $\lim_{x_1, x_2 \rightarrow \infty} F(x_1, x_2) = 1$
* $\lim_{x_1, x_2 \rightarrow -\infty} F(x_1, x_2) = 0$

#### Discrete random vector
* $X = (X_1, X_2)^t$: discrete r. vector $\Leftrightarrow S_p(X)$ is finite or countable
* $p_{X_1, X_2}(x_1, x_2)$: joint probability mass function

  $\Leftrightarrow p_{X_1, X_2}(x_1, x_2) = \begin{cases} P(X = x_1, X_2 = x_2), & (x_1, x_2) \in S_p(X) \\ 0, & \text{o.w.} \end{cases}$
* Poperties of joint pmf (pdf)
  * $0 \leq p_{X_1, X_2}(x_1, x_2) \leq 1$
  * $\sum_{x_1, x_2}p_{X_1, X_2}(x_1, x_2) = 1$
  * $P((X_1, X_2) \in B) = \sum_{(x_1, x_2) \in B} p_{X_1, X_2}(x_1, x_2)$
* Marginal pdf: the pmf associated to $X_i$ alone

  $p_{X_1}(x_1) = P(X_1 = x_1) = P(X_1 = x_1, X_2 \in S_p(X_2)) = \sum_{x_2}p_{X_1, X_2}(x_1, x_2)$
* Independence of two random variables

  When $p(x_1, x_2)$: joint pmf of $X_1$ and $X_2$; $p_i (x_i)$: marginal pmf of $X_i$,

  $X_1$ and $X_2$ are independent $\Leftrightarrow p(x_1, x_2) = p_1(x_1) \cdot p_2(x_2)$ for $x_1 \in S_p(X)$ and $x_2 \in S_p(Y)$

* Dependence of two random variables

  $X_1$ and $X_2$ are dependent $\Leftrightarrow X_1$ and $X_2$ are not independent

#### Continuous random vector
* $X = (X_1, X_2)^t$: continuous r. vector $\Leftrightarrow$ its cdf $F_{X_1, X_2}(x_1, x_2)$ is continuous
* $f_{X_1, X_2}(x_1, x_2)$: joint probability density function (joint pdf)

  $P((X_1, X_2) \in B) = \int \int_B f_{X_1, X_2}(x_1, x_2)dx_1 dx_2$
* Properties of joint pdf
  * $f_{X_1, X_2}(x_1, x_2) \geq 0$
  * $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X_1, X_2}(x_1, x_2)dx_1 dx_2 = 1$
  * $f_{X_1, X_2}(x_1, x_2) = \begin{cases} \frac{\partial^2}{\partial x_1 \partial x_2} F_{X_1, X_2}(x_1, x_2), & (x_1, x_2) \in D \\ 0, & \text{o.w.} \end{cases}$

    Where $D = \\{(x_1, x_2) | \exists \frac{\partial^2}{\partial x_1 \partial x_2}F_{X_1, X_2}(x_1, x_2) \\}$
* Support of $X$: $S_X = \\{(x_1, x_2) | f_{X_1, X_2}(x_1, x_2) > 0, (x_1, x_2) \in S_p(X) \\}$
* Marginal pdf

  For $X = (X_1, X_2)$: continuous, $f_{X_1}(x_1) = \int_{-\infty}^{\infty} f_{X_1, X_2}(x_1, x_2)dx_2$
* When $f(x_1, x_2)$: joint pdf of $X_1$ and $X_2$; $f_i(x_i)$: marginal pdf of $X_i$,

  $X_1$ and $X_2$ are independent $\Leftrightarrow f(x_1, x_2) = f_1(x_1) \cdot f_2(x_2)$
* Dependent of two random variables

  $X_1$ and $X_2$ are dependent $\Leftrightarrow X_1$ and $X_2$ are not independent

#### Remark 1
Calculation of probability by using the marginal pdf
* Discrete: $P(X_1 \in B_1) = \sum_{x_1 \in B}p_{X_1}(x_1)$
* Continuous: $P(X_1 \in B_1) = \int_{B_1}f_{X_1}(x_1)dx_1$

#### Remark 2
$X_1$ and $X_2$ are independet if
* $P((X_1, X_2) \in A) = 0$ where $A = \\{(x_1, x_2): f(x_1, x_2) \neq f_1(x_1) \cdot f_2(x_2) \\}$
* $f(x_1, x_2) = g(x_1)h(x_2)$ where $g(x_1), x_1 \in S_{X_1}$ and $h(x_2), x_2 \in S_{X_2}$

### Expectation

#### Expectation of $Y = g(X_1, X_2)$
For some real valued function $g: R^2 \rightarrow R$ and $Y = g(X_1, X_2)$,
* $X$: discrete,

  If $\sum_{x_1} \sum_{x_2} |g(x_1, x_2)| p_{X_1, X_2}(x_1, x_2) < \infty, E(Y) = \sum_{x_1} \sum_{x_2} g(x_1, x_2) p_{X_1, X_2} (x_1, x_2)$

* $X$: continuous,

  If $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} |g(x_1, x_2)| f_{X_1, X_2} (x_1, x_2) dx_1 dx_2 < \infty, E(Y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x_1, x_2) f_{X_1, X_2} (x_1, x_2) dx_1 dx_2$

* Property

  For r. vector $(X_1, X_2)$, define $Y_1 = g_1(X_1, X_2)$, and $Y_2 = g_2(X_1, X_2)$. 

  For $\forall k_1, k_2 \in R, E(k_1 Y_1 + k_2 Y_2) = k_1 E(Y_1) + k_2 E(Y_2)$


## Correlation Coefficient

### Covariance
$\sigma_{12} = Cov(X, Y) = E[(X - \mu_1)(Y - \mu_2)] = E(XY) - E(X)E(Y)$, where $\mu_1 = E(X)$ and $\mu_2 = E(Y)$

### Correlation
$\rho = Corr(X, Y) = \frac{Cov(X, Y)}{\sqrt{Var(X)} \sqrt{Var(Y)}} = \frac{Cov(X, Y)}{\sigma_1 \sigma_2}$

If $X$ and $Y$ are independent, then their covariance and correlations are zero.
However, zero correlation does not imply independence.


## Conditional Distribution

### Conditional pmf
For $x_2 \in S_{X_2}, p_{X_1|X_2}(x_1|x_2) = \frac{p_{X_1, X_2}(x_1, x_2)}{p_{X_2}(x_2)}$

Conditional pmf of the discrete type of a random variable $X_1$, given that the discrete type of random variable $X_2 = x_2$.

### Abbreviation for expressing the pdfs
$p_{1, 2}(x_1, x_2) = p_{X_1, X_2}(x_1, x_2), p_1(x_1) = p_{X_1}(x_1), p_{1|2}(x_1|x_2) = p_{X_1|X_2}(x_1|x_2)$

$f_{1, 2}(x_1, x_2) = f_{X_1, X_2}(x_1, x_2), f_1(x_1) = f_{X_1}(x_1), f_{1|2}(x_1|x_2) = f_{X_1|X_2}(x_1|x_2)$

### Properties of the conditional pdfs

#### Discrete
* $0 \leq p_{1|2}(x_1|x_2) \leq 1$ for $x_1 \in S_{X_1}$
* $\sum_{x_1}p_{1|2}(x_1|x_2) = 1$
* $P(X_1 \in B | X_2 = x_2) = \sum_{x_1 \in B}p_{1|2}(x_1|x_2)$

#### Continuous
* $0 \leq f_{1|2}(x_1|x_2), -\infty < x_1 < \infty$
* $\int_{-\infty}^{\infty} f_{1|2}(x_1|x_2) dx_1 = 1$
* $P(X_1 \in B | X_2 = x_2) = \int_B f_{1|2}(x_1|x_2) dx_1$

### Conditional expectation
$E(u(X_1, X_2)|X_2 = x_2) = \begin{cases} \sum_{x_1}u(x_1, x_2)p_{1|2}(x_1|x_2), & \text{(discrete)} \int_{-\infty}^{\infty} \\ u(x_1, x_2)f_{1|2}(x_1|x_2)dx_1, & \text{(continuous)} \end{cases}$

### Conditional mean and variance
Mean and variance of the conditional distribution of $X_1$, given $X_2 = x$
* Conditional mean: $E(X_1|X_2 = x)$
* Conditional variance: $Var(X_1|X_2 = x) = E[(X_1 - E(X_1|X_2 = x))^2 | X_2 = x]$

### Properties of conditional expectations
* $E(a X_1 + b | X_2 = x) = aE(X_1|X_2 = x) + b$
* $E(g_1(X_1) g_2(X_2) | X_2 = x) = g_2(x)E(g_1(X_1)|X_2 = x)$
* $E(g_1(X_1) + g_2(X_1)|X_2 = x) = E(g_1(X_1)|X_2 = x) + E(g_2(X_1)|X_2 = x)$
* $Var(X_1|X_2 = x) = E(X_1^2|X_2 = x) - \\{E(X_1|X_2 = x)\\}^2$

### Linear conditional mean
Suppose $Var(X) < \infty$, and $Var(Y) < \infty$.

If $E(Y|X) = a + bX$, then, 

$E(Y|X) = \mu_2 + \rho \frac{\sigma_2}{sigma_1}(X - \mu_1), E(Var(Y|X)) = \sigma_2^2(1 - \rho^2)$

where $\mu_1 = E(X), \mu_2 = E(Y), \sigma_1^2 = Var(X), \sigma_2^2 = Var(Y), \rho = corr(X, Y)$

### Properties with respect to conditional mean and variance
* $E(E(X_2|X_1)) = E(X_2)$
* $Var(X_2) = Var(E(X_2|X_1)) + E(Var(X_2|X_1))$

  $\Rightarrow Var(E(X_2|X_1)) \leq Var(X_2), E(X_2|X_1)$ is more precise than $X_2$

### Definition of Moment Generating Function of a Random Vector
Let $X = (X_1, X_2)^t$: a r. vector. If $E(e^{t_1 X_1 + t_2 X_2}$ exists for $-h_1 < t_1 < h_1, -h_2 < t_2 < h_2$ where $h_1 > 0$, and $h_2 > 0$,

$M_{X_1, X_2}(t_1, t_2) = E(e^{t_1 X_1 + t_2 X_2}) = E(e^{t^t X})$: joint mgf of $X = (X_1, X_2)^t$

where $t = (t_1, t_2)^t$

### Properties
* $M_{X_1}(t_1) = M_{X_1, X_2}(t_1, 0), M_{X_2}(t_2) = M_{X_1, X_2}(0, t_2)$: marginal mgfs
* $E(X_1^p X_2^q) = \frac{\partial^{p+q}}{\partial t_1^p \partial t_2^q} M_{X_1, X_2}(t_1, t_2)\mid_{t_1 = t_2 = 0}$; joint moment
* $\exists h_1 > 0, h_2 > 0$

  $M_{X_1, X_2}(t_1, t_2) = M_{Y_1, Y_2}(t_1, t_2), -h_1 < t_1 < h_1, -h_2 < t_2 < h_2$

  $\Leftrightarrow F_{X_1, X_2}(w_1, w_2) = F_{Y_1, Y_2}(w_1, w_2) \forall (w_1, w_2) \in R^2$

### Definition (multinomial distribution)
$(X_1, \ldots, X_{k-1} \sim \text{Multi} (n, p_1, \ldots, p_{k-1})$

$\Leftrightarrow p_{X_1, \ldots, X_{k-1}} (x_1, \ldots, x_{k-1}) = \binom{n}{x_1, \ldots, x_{k-1}, x_k} p_1^{x_1} \ldots p_{k-1}^{x_{k-1}} p_k^{x_k}$,

where $x_k = n - \sum_{i=1}^{k-1}x_i$ and $p_k = 1 - \sum_{i=1}^{k-1}p_i$

* mgf: $M_{X_1, \ldots, X_{k-1}}(t_1, \ldots, t_{k-1}) = (p_1 e^{t_1} + \ldots + p_{k-1} e^{t_{k-1}} + p_k)^n, -\infty < t_i < \infty$
* Mean: $E(X_i) = np_i$
* Variances and covariances: $Cov(X_i, X_j) = \begin{cases} np_i(1-p_i), & i=j \\ -np_i p_j, & i \neq j \end{cases}$

### Properties
If $(X_1, \ldots, X_{k-1}) \sim Multi(n, p_1, \ldots, p_{k-1})$ and $X_k = n - \sum_{i=1}^{k-1} X_i, p_k = 1 - \sum_{i=1}^{k-1} p_i$
* $X_i \sim B(n, p_i) (i = 1, \ldots, k)$
* $(X_2, \ldots, X_{k-1})|X_1 = x_1 \sim Multi(n - x_1, \frac{p_2}{1 - p_1}, \ldots, \frac{p_{k-1}}{1 - p_1})$
* $(X_1, X_2) \sim Multi(n, p_1, p_2) = Trinomial(n, p_1, p_2)$


## The Multivariate Normal Distribution

### Expected Value of a Random Vector
* For $X = (X_1, X_2)^t$, if $E|X_1| < \infty, E|X_2| < \infty, E(X) \equiv (E(X_1), E(X_2))^t$
* For $X^t = (X_1, X_2)$, if $E|X_1| < \infty, E|X_2| < \infty, E(X^t) \equiv (E(X_1), E(X_2))$
* For $X = \begin{matrix} X_11 & X_12 \\ X_21 & X_22 \end{matrix}$, if $E|X_11| < \infty, E|X_12| < \infty, E|X_21| < \infty, E|X_22| < \infty, E \equiv \begin{matrix} EX_11 & EX_12 \\ EX_21 & EX_22 \end{matrix}$

### Variance/covariance of a Random Vector
For $X = (X_1, X_2)^t, cov(X) \equiv E[(X-EX)(X-EX)^t] = \begin{matrix} var(X_1) & cov(X_1, X_2) \\ cov(X_1, X_2) & var(X_2) \end{matrix}$

where $E|X_1^2| < \infty, E|X_2^2| < \infty$

### Property
For the random vector, $X (n\times 1)$, and matrix (or vector) $A$,
* $E(AX) = AE(X)$ where $A \in R^{k \times n}$
* $E(X^t A) = E(X)^t A$ where $A \in R^{n \times m}$

For the random vector, $X$, and matrix (or vector) $A$,
$cov(AX) = A cov(X) A^t$


### Bivariate normal


