---
title: Bivariate Distribution
author: JSH
date: 2024-07-23 10:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, IMS]
use_math: true
---

# Bivariate Distribution

## Distributions of Two Random Variables

### Random Vector and Space

#### Random Vector
$X = (X_1, X_2)^t$: r. vector $\Leftrightarrow X_1, X_2$: random variable on the sample space $S$ <!-- random vector는 random variable의 모임 -->

#### Space of r. vector $X$
For a r. vector $X = (X_1, X_2)^t, S_p(X) = \\{(x_1, x_2): x_1 = X_1(c), x_2 = X_2(c), c \in S \\}$
<!-- sample space에 들어가는 각각의 element들에 대해서 확률변수 X_1, X_2에 대입시켜서 나온 값들의 집합 -->

#### Example
When a coin is to be tossed three times,
* Sample space: $S = \\{HHH, HHT, HTH, \ldots, TTT \\}$
* If $X_1 =$ number of heads during hte first two tossing, and $X_2 =$ number of heads during the three tossing
* $X_1 \in \\{0, 1, 2 \\}, X_2 \in \\{0, 1, 2, 3 \\}$
* For $X = (X_1, X_2)^t, S_p(X) = \\{(0, 0), (0, 1), (1, 1), (1, 2), (2, 2), (2, 3) \\}$


### Joint distribution of r. vector

#### Joint cumulative distribution function
For $\forall (x_1, x_2) \in R^2$, 
$F_{X_1, X_2} (x_1, x_2) = P(X_1 \leq x_1, X_2 \leq x_2) = P(\\{c: X_1(c) \leq x_1 \\} \cap \\{c: X_2(c) \leq x_2 \\})$

#### Properties
* $F_{X_1, X_2}(x_1, x_2) - F_{X_1, X_2}(y_1, x_2) + F_{X_1, X_2}(y_1, y_2) = P(y_1 < X_1 \leq x_1, y_2 < X_2 \leq x_2)$  <!-- 그림을 그려서 생각해보기 -->
  * $P(X_1 \leq x_1, X_2 \leq x_2) - P(X_1 \leq x_1, X_2 \leq y_2) - P(X_1 \leq y_1, X_2 \leq x_2) + P(X_1 \leq y_1, X_2 \leq y_2)$
* $\lim_{x_1, x_2 \rightarrow \infty} F(x_1, x_2) = 1$
* $\lim_{x_1, x_2 \rightarrow -\infty} F(x_1, x_2) = 0$


#### Discrete random vector
* $X = (X_1, X_2)^t$: discrete r. vector $\Leftrightarrow S_p(X)$ is finite or countable
* $p_{X_1, X_2}(x_1, x_2)$: joint probability mass function

  $\Leftrightarrow p_{X_1, X_2}(x_1, x_2) = P(X = x_1, X_2 = x_2)$ if $(x_1, x_2) \in S_p(X)$, 0 in o.w.
* Poperties of joint pmf (pdf)
  * $0 \leq p_{X_1, X_2}(x_1, x_2) \leq 1$
  * $\sum_{x_1, x_2}p_{X_1, X_2}(x_1, x_2) = 1$
  * $P((X_1, X_2) \in B) = \sum_{(x_1, x_2) \in B} p_{X_1, X_2}(x_1, x_2)$
* Marginal pdf: the pmf associated to $X_i$ alone  <!-- r.v.가 여러개 있을 때 한개에 대한 pdf (vs. 두개를 동시에 볼 때는 joint) -->

  $p_{X_1}(x_1) = P(X_1 = x_1) = P(X_1 = x_1, X_2 \in S_p(X_2)) = \sum_{x_2}p_{X_1, X_2}(x_1, x_2)$   <!-- X_2의 모든 space에 대해 더하라는 의미 -->
* Independence of two random variables

  When $p(x_1, x_2)$: joint pmf of $X_1$ and $X_2$; $p_i (x_i)$: marginal pmf of $X_i$,

  $X_1$ and $X_2$ are independent $\Leftrightarrow p(x_1, x_2) = p_1(x_1) \cdot p_2(x_2)$ for $x_1 \in S_p(X)$ and $x_2 \in S_p(Y)$  <!-- joint pmf가 marginal pmf의 곱으로 표현될 때 독립이라고 한다 -->

* Dependence of two random variables

  $X_1$ and $X_2$ are dependent $\Leftrightarrow X_1$ and $X_2$ are not independent


#### Example
Roll a pair of unbiased dice.
For each of the 36 sample points with probability 1/36, let $X$ denote the smaller and $Y$ the larger outcome on the dice.
What is the joint pmf for $X$ and $Y$?
What are the marginal pmfs for $X$ and $Y$ respectively?
Are they independent?

* $f_{X, Y}(x, y) = 2/36$ if $x < y$, and $1/36$ if $x = y$.
* $f_X(x) = \sum_{y = x}^6 f_{X, Y}(x, y) = \frac{1 + 2(6 - x)}{36}$
* $f_Y(y) = \sum_{x = 1}^y f_{X, Y}(x, y) = \frac{2(y - 1) + 1}{36}$
* $f_{X, Y}(x, y) \neq f_X(x)f_Y(y)$


#### Example
Let the joint pmf of $X$ and $Y$ be defined by $f(x, y) = \frac{x + y}{21}, x = 1, 2, 3, y = 1, 2$.

Then what are the marginal pmfs for $X$ and $Y$ respectively, and are they independent?

* $f_X(x) = \sum_{y = 1}^2 f(x, y) = \frac{x + 1 + x + 2}{21}$
* $f_Y(y) = \sum_{x = 1}^3 f(x, y) = \frac{1 + y + 2 + y}{21}$
* $f_X(x)f_Y(y) \neq f_{X, Y}(x, y)$

<!-- f_{X, Y}(x, y) = f_X(x)f_Y(y)가 되려면 joint pmf가 x와 y의 합으로 이루어져있으면 안되고, 곱으로 이루어져 있어야한다 -->
<!-- space도 x와 y가 관련있으면 안된다 -->


#### Continuous random vector
* $X = (X_1, X_2)^t$: continuous r. vector $\Leftrightarrow$ its cdf $F_{X_1, X_2}(x_1, x_2)$ is continuous
* $f_{X_1, X_2}(x_1, x_2)$: joint probability density function (joint pdf)

  $P((X_1, X_2) \in B) = \int \int_B f_{X_1, X_2}(x_1, x_2)dx_1 dx_2$
* Properties of joint pdf
  * $f_{X_1, X_2}(x_1, x_2) \geq 0$
  * $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X_1, X_2}(x_1, x_2) dx_1 dx_2 = 1$
  * $f_{X_1, X_2}(x_1, x_2) = \frac{\partial^2}{\partial x_1 \partial x_2} F_{X_1, X_2}(x_1, x_2)$ if $(x_1, x_2) \in D$, and 0 if o.w.

    Where $D = \\{(x_1, x_2) \mid \exists \frac{\partial^2}{\partial x_1 \partial x_2} F_{X_1, X_2}(x_1, x_2) \\}$
* Support of $X$ : $S_X = \\{(x_1, x_2) \mid f_{X_1, X_2}(x_1, x_2) > 0, (x_1, x_2) \in S_p(X) \\}$
* Marginal pdf

  For $X = (X_1, X_2)$: continuous, $f_{X_1}(x_1) = \int_{-\infty}^{\infty} f_{X_1, X_2}(x_1, x_2)dx_2$  <!-- x_2에 대해 전체 적분 -->
* When $f(x_1, x_2)$: joint pdf of $X_1$ and $X_2$; $f_i(x_i)$: marginal pdf of $X_i$,

  $X_1$ and $X_2$ are independent $\Leftrightarrow f(x_1, x_2) = f_1(x_1) \cdot f_2(x_2)$
* Dependent of two random variables

  $X_1$ and $X_2$ are dependent $\Leftrightarrow X_1$ and $X_2$ are not independent


#### Example
Suppose $f(x_1, x_2) = 6x_1^2 x_2$ if $0 < x_1 < 1, 0 < x_2 < 1$ and 0 o.w.  <!-- pdf가 x_1과 x_2의 곱으로 표현되고 support도 x_1와 x_2의 연관이 없으므로 이것만 보고도 두 r.v.가 독립임을 알 수 있다 -->
Calculate $P(0 < X_1 < 3/4, 1/3 < X_2 < 2)$

$P(0 < X_1 < 3/4, 1/3 < X_2 < 2) = \int_A \int f(x_1, x_2) dx_1 dx_2 = \int_0^{\frac{3}{4}} \int_{\frac{1}{3}}^1 6x_1^2 x_2 dx_2 dx_1$

$= \int_0^{\frac{3}{4}} [3x_1^2 x_2^2]^1_{\frac{1}{3}} dx_1 = \int_0^{\frac{3}{4}} 8/3 x_1^2 dx_1$

$= [8/9 x_1^3]^{\frac{3}{4}}_0 = 3/8$

* $P(0 < X_1 < 3/4, 1/3 < X_2 < 2) = P(0 < X_1 < 3/4)P(1/3 < X_2 < 2)$


#### Example
Let $X$ and $Y$ have the joint pdf $f(x, y) = \frac{3}{2} x^2 (1- \mid y \mid), -1 < x < 1, -1 < y < 1.

What is the probability that $(X, Y)$ falls into $A = \\{(x, y): 0 < x < 1, 0 < y < x \\}$  <!-- 그림을 그려서 확인해봐야 할 것 -->

$\int_A \int f(x, y) dx dy = \int_0^1 \int_y^1 3/2 x^2 (1 - \mid y \mid) dx dy

$= \int_0^1 (1\2 - 1\2y - 1\2y^3 + 1\2y^4) dy = 9/40$


#### Remark 1
Calculation of probability by using the marginal pdf
* Discrete: $P(X_1 \in B_1) = \sum_{x_1 \in B}p_{X_1}(x_1) = \sum_{x_1 \in B} \sum_{x_2 \in S_p(X_2)} P_{X_1, X_2}(x_1, x_2)$
* Continuous: $P(X_1 \in B_1) = \int_{B_1}f_{X_1}(x_1)dx_1 = \int_{B_1} \int_{-\infty}^{\infty} f_{X_1, X_2} (x_1, x_2) dx_2 dx_1$

#### Remark 2
$X_1$ and $X_2$ are independet if
* $P((X_1, X_2) \in A) = 0$ where $A = \\{(x_1, x_2): f(x_1, x_2) \neq f_1(x_1) \cdot f_2(x_2) \\}$
* $f(x_1, x_2) = g(x_1)h(x_2)$ where $g(x_1), x_1 \in S_{X_1}$ and $h(x_2), x_2 \in S_{X_2}$

<!-- I_A(x_1) 이라는 뜻은 1 if x_1 \in A, 0 o.w. 라는 의미이다 -->

#### Example
* $f(x_1, x_2) = (x_1 + x_2) I_{ \\{(x_1, x_2) \mid 0 < x_1 < 1, 0 < x_2 < 1 \\}} (x_1, x_2)$ : $X_1$ and $X_2$ are dependent
* $f(x_1, x_2) = 8x_1 x_2$ if $0 < x_1 < x_2 < 1$ and 0 o. w.: $X_1$ and $X_2$ are dependent
* $f(x_1, x_2) = 4x_1 x_2$ if $0 < x_1 < 1, 0 < x_2 < 1$ and 0 o. w.: $X_1$ and $X_2$ are independent


### Expectation

#### Expectation of $Y = g(X_1, X_2)$
For some real valued function $g: R^2 \rightarrow R$ and $Y = g(X_1, X_2)$,
* $X$: discrete,

  If $\sum_{x_1} \sum_{x_2} |g(x_1, x_2)| p_{X_1, X_2}(x_1, x_2) < \infty, E(Y) = \sum_{x_1} \sum_{x_2} g(x_1, x_2) p_{X_1, X_2} (x_1, x_2)$

* $X$: continuous,

  If $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} |g(x_1, x_2)| f_{X_1, X_2} (x_1, x_2) dx_1 dx_2 < \infty, E(Y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x_1, x_2) f_{X_1, X_2} (x_1, x_2) dx_1 dx_2$

* Property

  For r. vector $(X_1, X_2)$, define $Y_1 = g_1(X_1, X_2)$, and $Y_2 = g_2(X_1, X_2)$. 

  For $\forall k_1, k_2 \in R, E(k_1 Y_1 + k_2 Y_2) = k_1 E(Y_1) + k_2 E(Y_2)$

<!-- 다중적분을 할 때는 꼭 그래프를 그리는 습관을 들이자-->


## Correlation Coefficient

### Covariance
$\sigma_{12} = Cov(X, Y) = E[(X - \mu_1)(Y - \mu_2)] = E(XY) - E(X)E(Y)$, where $\mu_1 = E(X)$ and $\mu_2 = E(Y)$

### Correlation
$\rho = Corr(X, Y) = \frac{Cov(X, Y)}{\sqrt{Var(X)} \sqrt{Var(Y)}} = \frac{Cov(X, Y)}{\sigma_1 \sigma_2}$

If $X$ and $Y$ are independent, then their covariance and correlations are zero.
However, zero correlation does not imply independence.


## Conditional Distribution

### Conditional pmf
For $x_2 \in S_{X_2}, p_{X_1|X_2}(x_1|x_2) = \frac{p_{X_1, X_2}(x_1, x_2)}{p_{X_2}(x_2)}$

Conditional pmf of the discrete type of a random variable $X_1$, given that the discrete type of random variable $X_2 = x_2$.

### Abbreviation for expressing the pdfs
$p_{1, 2}(x_1, x_2) = p_{X_1, X_2}(x_1, x_2), p_1(x_1) = p_{X_1}(x_1), p_{1|2}(x_1|x_2) = p_{X_1|X_2}(x_1|x_2)$

$f_{1, 2}(x_1, x_2) = f_{X_1, X_2}(x_1, x_2), f_1(x_1) = f_{X_1}(x_1), f_{1|2}(x_1|x_2) = f_{X_1|X_2}(x_1|x_2)$

### Properties of the conditional pdfs

#### Discrete
* $0 \leq p_{1|2}(x_1|x_2) \leq 1$ for $x_1 \in S_{X_1}$
* $\sum_{x_1}p_{1|2}(x_1|x_2) = 1$
* $P(X_1 \in B | X_2 = x_2) = \sum_{x_1 \in B}p_{1|2}(x_1|x_2)$

#### Continuous
* $0 \leq f_{1|2}(x_1|x_2), -\infty < x_1 < \infty$
* $\int_{-\infty}^{\infty} f_{1|2}(x_1|x_2) dx_1 = 1$
* $P(X_1 \in B | X_2 = x_2) = \int_B f_{1|2}(x_1|x_2) dx_1$

### Conditional expectation
$E(u(X_1, X_2)|X_2 = x_2) = \begin{cases} \sum_{x_1}u(x_1, x_2)p_{1|2}(x_1|x_2), & \text{(discrete)} \int_{-\infty}^{\infty} \\ u(x_1, x_2)f_{1|2}(x_1|x_2)dx_1, & \text{(continuous)} \end{cases}$

### Conditional mean and variance
Mean and variance of the conditional distribution of $X_1$, given $X_2 = x$
* Conditional mean: $E(X_1|X_2 = x)$
* Conditional variance: $Var(X_1|X_2 = x) = E[(X_1 - E(X_1|X_2 = x))^2 | X_2 = x]$

### Properties of conditional expectations
* $E(a X_1 + b | X_2 = x) = aE(X_1|X_2 = x) + b$
* $E(g_1(X_1) g_2(X_2) | X_2 = x) = g_2(x)E(g_1(X_1)|X_2 = x)$
* $E(g_1(X_1) + g_2(X_1)|X_2 = x) = E(g_1(X_1)|X_2 = x) + E(g_2(X_1)|X_2 = x)$
* $Var(X_1|X_2 = x) = E(X_1^2|X_2 = x) - \\{E(X_1|X_2 = x)\\}^2$

### Linear conditional mean
Suppose $Var(X) < \infty$, and $Var(Y) < \infty$.

If $E(Y|X) = a + bX$, then, 

$E(Y|X) = \mu_2 + \rho \frac{\sigma_2}{sigma_1}(X - \mu_1), E(Var(Y|X)) = \sigma_2^2(1 - \rho^2)$

where $\mu_1 = E(X), \mu_2 = E(Y), \sigma_1^2 = Var(X), \sigma_2^2 = Var(Y), \rho = corr(X, Y)$

### Properties with respect to conditional mean and variance
* $E(E(X_2|X_1)) = E(X_2)$
* $Var(X_2) = Var(E(X_2|X_1)) + E(Var(X_2|X_1))$

  $\Rightarrow Var(E(X_2|X_1)) \leq Var(X_2), E(X_2|X_1)$ is more precise than $X_2$

### Definition of Moment Generating Function of a Random Vector
Let $X = (X_1, X_2)^t$: a r. vector. If $E(e^{t_1 X_1 + t_2 X_2}$ exists for $-h_1 < t_1 < h_1, -h_2 < t_2 < h_2$ where $h_1 > 0$, and $h_2 > 0$,

$M_{X_1, X_2}(t_1, t_2) = E(e^{t_1 X_1 + t_2 X_2}) = E(e^{t^t X})$: joint mgf of $X = (X_1, X_2)^t$

where $t = (t_1, t_2)^t$

### Properties
* $M_{X_1}(t_1) = M_{X_1, X_2}(t_1, 0), M_{X_2}(t_2) = M_{X_1, X_2}(0, t_2)$: marginal mgfs
* $E(X_1^p X_2^q) = \frac{\partial^{p+q}}{\partial t_1^p \partial t_2^q} M_{X_1, X_2}(t_1, t_2)\mid_{t_1 = t_2 = 0}$; joint moment
* $\exists h_1 > 0, h_2 > 0$

  $M_{X_1, X_2}(t_1, t_2) = M_{Y_1, Y_2}(t_1, t_2), -h_1 < t_1 < h_1, -h_2 < t_2 < h_2$

  $\Leftrightarrow F_{X_1, X_2}(w_1, w_2) = F_{Y_1, Y_2}(w_1, w_2) \forall (w_1, w_2) \in R^2$

### Definition (multinomial distribution)
$(X_1, \ldots, X_{k-1} \sim \text{Multi} (n, p_1, \ldots, p_{k-1})$

$\Leftrightarrow p_{X_1, \ldots, X_{k-1}} (x_1, \ldots, x_{k-1}) = \binom{n}{x_1, \ldots, x_{k-1}, x_k} p_1^{x_1} \ldots p_{k-1}^{x_{k-1}} p_k^{x_k}$,

where $x_k = n - \sum_{i=1}^{k-1}x_i$ and $p_k = 1 - \sum_{i=1}^{k-1}p_i$

* mgf: $M_{X_1, \ldots, X_{k-1}}(t_1, \ldots, t_{k-1}) = (p_1 e^{t_1} + \ldots + p_{k-1} e^{t_{k-1}} + p_k)^n, -\infty < t_i < \infty$
* Mean: $E(X_i) = np_i$
* Variances and covariances: $Cov(X_i, X_j) = \begin{cases} np_i(1-p_i), & i=j \\ -np_i p_j, & i \neq j \end{cases}$

### Properties
If $(X_1, \ldots, X_{k-1}) \sim Multi(n, p_1, \ldots, p_{k-1})$ and $X_k = n - \sum_{i=1}^{k-1} X_i, p_k = 1 - \sum_{i=1}^{k-1} p_i$
* $X_i \sim B(n, p_i) (i = 1, \ldots, k)$
* $(X_2, \ldots, X_{k-1})|X_1 = x_1 \sim Multi(n - x_1, \frac{p_2}{1 - p_1}, \ldots, \frac{p_{k-1}}{1 - p_1})$
* $(X_1, X_2) \sim Multi(n, p_1, p_2) = Trinomial(n, p_1, p_2)$


## The Multivariate Normal Distribution

### Expected Value of a Random Vector
* For $X = (X_1, X_2)^t$, if $E|X_1| < \infty, E|X_2| < \infty, E(X) \equiv (E(X_1), E(X_2))^t$
* For $X^t = (X_1, X_2)$, if $E|X_1| < \infty, E|X_2| < \infty, E(X^t) \equiv (E(X_1), E(X_2))$
* For $X = \begin{matrix} X_11 & X_12 \\ X_21 & X_22 \end{matrix}$, if $E|X_11| < \infty, E|X_12| < \infty, E|X_21| < \infty, E|X_22| < \infty, E \equiv \begin{matrix} EX_11 & EX_12 \\ EX_21 & EX_22 \end{matrix}$

### Variance/covariance of a Random Vector
For $X = (X_1, X_2)^t, cov(X) \equiv E[(X-EX)(X-EX)^t] = \begin{matrix} var(X_1) & cov(X_1, X_2) \\ cov(X_1, X_2) & var(X_2) \end{matrix}$

where $E|X_1^2| < \infty, E|X_2^2| < \infty$

### Property
For the random vector, $X (n\times 1)$, and matrix (or vector) $A$,
* $E(AX) = AE(X)$ where $A \in R^{k \times n}$
* $E(X^t A) = E(X)^t A$ where $A \in R^{n \times m}$

For the random vector, $X$, and matrix (or vector) $A$,
$cov(AX) = A cov(X) A^t$


### Bivariate normal
#### Deifnition
$X \sim N_2((\mu_1, \mu_2)^t, \begin{matrix} \sigma_1^2 & \rho \sigma_1 \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{matrix})$ for $X = (X_1, X_2)^t$

$\Leftrightarrow f_X(x_1, x_2) = \frac{1}{2 \pi \sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \times \exp [-\frac{1}{2(1-\rho^2)} \\{(\frac{x_1 - \mu_1}{\sigma_1})^2 - 2 \rho (\frac{x_1 - \mu_1}{\sigma_1})(\frac{x_2 - \mu_2}{\sigma_2}) + (\frac{x_2 - \mu_2}{\sigma_2})^2 \\}]$

* Mean: $E(X_i) = \mu_i$
* Variance: $var(X_i) = \sigma_i^2$
* Covariance: $cov(X_1, X_2) = \rho \sigma_1 \sigma_2$
* Conditional distribution: $X_1|X_2 \sim N(\mu_1 + \rho \frac{\sigma_1}{\sigma_2} (X_2 - \mu_2), \sigma_1^2 (1 - \rho^2))$

### Standard multivariate normal
#### Definition
$Z \sim N_n(0, I_n)$ for $Z = (Z_1, \ldots, Z_n)^t$ (random vector $Z$ has a multivariate normal dist. with mean vector 0 and covariance matrix $I_n$).

Then, $f_Z(z) = (2 \pi)^{-n/2} \exp \\{ -\frac{1}{2}z^t z \\}$ for $z \in R^n$

$\Leftrightarrow Z_1, \ldots, Z_n \sim iid N(0, 1)$

* Mean: $E(Z) = 0$
* Covariance; $Cov(Z) = I_n$
* mgf: $M_Z(t) = E[\exp(t^t Z)] = \exp \\{(1/2)t^t t \\}$ for all $t \in R^n$

### Multivariate normal
#### Definition
$X \sim N_n(\mu, \Sigma)$ for $X = (X_1, \ldots, X_n)^t$ and $\Sigma$: symmetric positive definite matrix (definition: $x^t Ax > 0$ for all $x \neq 0$)

Then, $f_X(x) = \det (2 \pi \Sigma)^{-1/2} \exp \\{-\frac{1}{2} (x - \mu)^t \Sigma^{-1} (x - \mu) \\}$ for $x \in R^n$

$\Leftrightarrow$ distribution of $X = \Sigma^{1/2} Z + \mu$ where $\Sigma^{1/2}$: real symmetric and $\Sigma^{1/2} \Sigma^{1/2} = \Sigma$

* Mean: $E(X) = \mu$
* Cov: $Cov(X) = \Sigma$
* mgf: $M_X(t) = \exp \\{t^t \mu + (1/2)t^t \Sigma t \\}$ for all $t \in R^n$

### Related Properties
Suppose $X = (X_1^t, X_2^t)^t \sim N_n(\mu, \Sigma)$ where $X_1: n_1 \times 1, X_2: n_2 \tims 1 & n = n_1 + n_2$
* $Y = AX + b \sim N_m(A \mu + b, A \Sigma A^t)$ where $A: (m \times n)$ and $b \in R^m$
* $Cov(X_1, X_2) = 0 \Leftrightarrow X_1 & X_2$: independent
* $W = (X - \mu)^t \Sigma^{-1} (X - \mu) \sim \chi^2(n)$

#### Remarks
If a real symmetric matrix $A$ is positive definite matrix (definition: $x^t Ax > 0$ for all $x \neq 0$)
* $\Leftrightarrow$ All eigenvalues of $A$ are >0
* $\Leftrightarrow A = BB^t$ for some non-singular (and symmetric) $B$
* $\Rightarrow A$ is non-singular
