---
title: Bivariate Distribution
author: JSH
date: 2024-07-23 10:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, IMS]
use_math: true
---

# Bivariate Distribution

## Distributions of Two Random Variables

### Random Vector and Space

#### Random Vector
$X = (X_1, X_2)^t$: r. vector $\Leftrightarrow X_1, X_2$: random variable on the sample space $S$ <!-- random vector는 random variable의 모임 --> 

#### Space of r. vector $X$
For a r. vector $X = (X_1, X_2)^t, S_p(X) = \\{(x_1, x_2): x_1 = X_1(c), x_2 = X_2(c), c \in S \\}$
<!-- sample space에 들어가는 각각의 element들에 대해서 확률변수 X_1, X_2에 대입시켜서 나온 값들의 집합 -->

#### Example
When a coin is to be tossed three times,
* Sample space: $S = \\{HHH, HHT, HTH, \ldots, TTT \\}$
* If $X_1 =$ number of heads during hte first two tossing, and $X_2 =$ number of heads during the three tossing
* $X_1 \in \\{0, 1, 2 \\}, X_2 \in \\{0, 1, 2, 3 \\}$
* For $X = (X_1, X_2)^t, S_p(X) = \\{(0, 0), (0, 1), (1, 1), (1, 2), (2, 2), (2, 3) \\}$


### Joint distribution of r. vector

#### Joint cumulative distribution function
For $\forall (x_1, x_2) \in R^2$, 
$F_{X_1, X_2} (x_1, x_2) = P(X_1 \leq x_1, X_2 \leq x_2) = P(\\{c: X_1(c) \leq x_1 \\} \cap \\{c: X_2(c) \leq x_2 \\})$

#### Properties
* $F_{X_1, X_2}(x_1, x_2) - F_{X_1, X_2}(y_1, x_2) + F_{X_1, X_2}(y_1, y_2) = P(y_1 < X_1 \leq x_1, y_2 < X_2 \leq x_2)$  <!-- 그림을 그려서 생각해보기 -->
  * $P(X_1 \leq x_1, X_2 \leq x_2) - P(X_1 \leq x_1, X_2 \leq y_2) - P(X_1 \leq y_1, X_2 \leq x_2) + P(X_1 \leq y_1, X_2 \leq y_2)$
* $\lim_{x_1, x_2 \rightarrow \infty} F(x_1, x_2) = 1$
* $\lim_{x_1, x_2 \rightarrow -\infty} F(x_1, x_2) = 0$


#### Discrete random vector
* $X = (X_1, X_2)^t$: discrete r. vector $\Leftrightarrow S_p(X)$ is finite or countable
* $p_{X_1, X_2}(x_1, x_2)$: joint probability mass function

  $\Leftrightarrow p_{X_1, X_2}(x_1, x_2) = P(X = x_1, X_2 = x_2)$ if $(x_1, x_2) \in S_p(X)$, 0 in o.w.
* Poperties of joint pmf (pdf)
  * $0 \leq p_{X_1, X_2}(x_1, x_2) \leq 1$
  * $\sum_{x_1, x_2}p_{X_1, X_2}(x_1, x_2) = 1$
  * $P((X_1, X_2) \in B) = \sum_{(x_1, x_2) \in B} p_{X_1, X_2}(x_1, x_2)$
* Marginal pdf: the pmf associated to $X_i$ alone  <!-- r.v.가 여러개 있을 때 한개에 대한 pdf (vs. 두개를 동시에 볼 때는 joint) -->

  $p_{X_1}(x_1) = P(X_1 = x_1) = P(X_1 = x_1, X_2 \in S_p(X_2)) = \sum_{x_2}p_{X_1, X_2}(x_1, x_2)$   <!-- X_2의 모든 space에 대해 더하라는 의미 -->
* Independence of two random variables

  When $p(x_1, x_2)$: joint pmf of $X_1$ and $X_2$; $p_i (x_i)$: marginal pmf of $X_i$,

  $X_1$ and $X_2$ are independent $\Leftrightarrow p(x_1, x_2) = p_1(x_1) \cdot p_2(x_2)$ for $x_1 \in S_p(X)$ and $x_2 \in S_p(Y)$  <!-- joint pmf가 marginal pmf의 곱으로 표현될 때 독립이라고 한다 -->

* Dependence of two random variables

  $X_1$ and $X_2$ are dependent $\Leftrightarrow X_1$ and $X_2$ are not independent


#### Example
Roll a pair of unbiased dice.
For each of the 36 sample points with probability 1/36, let $X$ denote the smaller and $Y$ the larger outcome on the dice.
What is the joint pmf for $X$ and $Y$?
What are the marginal pmfs for $X$ and $Y$ respectively?
Are they independent?

* $f_{X, Y}(x, y) = 2/36$ if $x < y$, and $1/36$ if $x = y$.
* $f_X(x) = \sum_{y = x}^6 f_{X, Y}(x, y) = \frac{1 + 2(6 - x)}{36}$
* $f_Y(y) = \sum_{x = 1}^y f_{X, Y}(x, y) = \frac{2(y - 1) + 1}{36}$
* $f_{X, Y}(x, y) \neq f_X(x)f_Y(y)$


#### Example
Let the joint pmf of $X$ and $Y$ be defined by $f(x, y) = \frac{x + y}{21}, x = 1, 2, 3, y = 1, 2$.

Then what are the marginal pmfs for $X$ and $Y$ respectively, and are they independent?

* $f_X(x) = \sum_{y = 1}^2 f(x, y) = \frac{x + 1 + x + 2}{21}$
* $f_Y(y) = \sum_{x = 1}^3 f(x, y) = \frac{1 + y + 2 + y}{21}$
* $f_X(x)f_Y(y) \neq f_{X, Y}(x, y)$

<!-- f_{X, Y}(x, y) = f_X(x)f_Y(y)가 되려면 joint pmf가 x와 y의 합으로 이루어져있으면 안되고, 곱으로 이루어져 있어야한다 -->
<!-- space도 x와 y가 관련있으면 안된다 -->


#### Continuous random vector
* $X = (X_1, X_2)^t$: continuous r. vector $\Leftrightarrow$ its cdf $F_{X_1, X_2}(x_1, x_2)$ is continuous
* $f_{X_1, X_2}(x_1, x_2)$: joint probability density function (joint pdf)

  $P((X_1, X_2) \in B) = \int \int_B f_{X_1, X_2}(x_1, x_2)dx_1 dx_2$
* Properties of joint pdf
  * $f_{X_1, X_2}(x_1, x_2) \geq 0$
  * $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X_1, X_2}(x_1, x_2) dx_1 dx_2 = 1$
  * $f_{X_1, X_2}(x_1, x_2) = \frac{\partial^2}{\partial x_1 \partial x_2} F_{X_1, X_2}(x_1, x_2)$ if $(x_1, x_2) \in D$, and 0 if o.w.

    Where $D = \\{(x_1, x_2) \mid \exists \frac{\partial^2}{\partial x_1 \partial x_2} F_{X_1, X_2}(x_1, x_2) \\}$
* Support of $X$ : $S_X = \\{(x_1, x_2) \mid f_{X_1, X_2}(x_1, x_2) > 0, (x_1, x_2) \in S_p(X) \\}$
* Marginal pdf

  For $X = (X_1, X_2)$: continuous, $f_{X_1}(x_1) = \int_{-\infty}^{\infty} f_{X_1, X_2}(x_1, x_2)dx_2$  <!-- x_2에 대해 전체 적분 -->
* When $f(x_1, x_2)$: joint pdf of $X_1$ and $X_2$; $f_i(x_i)$: marginal pdf of $X_i$,

  $X_1$ and $X_2$ are independent $\Leftrightarrow f(x_1, x_2) = f_1(x_1) \cdot f_2(x_2)$
* Dependent of two random variables

  $X_1$ and $X_2$ are dependent $\Leftrightarrow X_1$ and $X_2$ are not independent


#### Example
Suppose $f(x_1, x_2) = 6x_1^2 x_2$ if $0 < x_1 < 1, 0 < x_2 < 1$ and 0 o.w.  <!-- pdf가 x_1과 x_2의 곱으로 표현되고 support도 x_1와 x_2의 연관이 없으므로 이것만 보고도 두 r.v.가 독립임을 알 수 있다 -->
Calculate $P(0 < X_1 < 3/4, 1/3 < X_2 < 2)$

$P(0 < X_1 < 3/4, 1/3 < X_2 < 2) = \int_A \int f(x_1, x_2) dx_1 dx_2 = \int_0^{\frac{3}{4}} \int_{\frac{1}{3}}^1 6x_1^2 x_2 dx_2 dx_1$

$= \int_0^{\frac{3}{4}} [3x_1^2 x_2^2]^1_{\frac{1}{3}} dx_1 = \int_0^{\frac{3}{4}} 8/3 x_1^2 dx_1$

$= [8/9 x_1^3]^{\frac{3}{4}}_0 = 3/8$

* $P(0 < X_1 < 3/4, 1/3 < X_2 < 2) = P(0 < X_1 < 3/4)P(1/3 < X_2 < 2)$


#### Example
Let $X$ and $Y$ have the joint pdf $f(x, y) = \frac{3}{2} x^2 (1- \mid y \mid), -1 < x < 1, -1 < y < 1.

What is the probability that $(X, Y)$ falls into $A = \\{(x, y): 0 < x < 1, 0 < y < x \\}$  <!-- 그림을 그려서 확인해봐야 할 것 -->

$\int_A \int f(x, y) dx dy = \int_0^1 \int_y^1 3/2 x^2 (1 - \mid y \mid) dx dy$

$= \int_0^1 (1/2 - 1/2y - 1/2y^3 + 1/2y^4) dy = 9/40$


#### Remark 1
Calculation of probability by using the marginal pdf
* Discrete: $P(X_1 \in B_1) = \sum_{x_1 \in B}p_{X_1}(x_1) = \sum_{x_1 \in B} \sum_{x_2 \in S_p(X_2)} P_{X_1, X_2}(x_1, x_2)$
* Continuous: $P(X_1 \in B_1) = \int_{B_1}f_{X_1}(x_1)dx_1 = \int_{B_1} \int_{-\infty}^{\infty} f_{X_1, X_2} (x_1, x_2) dx_2 dx_1$

#### Remark 2
$X_1$ and $X_2$ are independet if
* $P((X_1, X_2) \in A) = 0$ where $A = \\{(x_1, x_2): f(x_1, x_2) \neq f_1(x_1) \cdot f_2(x_2) \\}$
* $f(x_1, x_2) = g(x_1)h(x_2)$ where $g(x_1), x_1 \in S_{X_1}$ and $h(x_2), x_2 \in S_{X_2}$

<!-- I_A(x_1) 이라는 뜻은 1 if x_1 \in A, 0 o.w. 라는 의미이다 -->

#### Example
* $f(x_1, x_2) = (x_1 + x_2) I_{ \\{(x_1, x_2) \mid 0 < x_1 < 1, 0 < x_2 < 1 \\}} (x_1, x_2)$ : $X_1$ and $X_2$ are dependent
* $f(x_1, x_2) = 8x_1 x_2$ if $0 < x_1 < x_2 < 1$ and 0 o. w.: $X_1$ and $X_2$ are dependent
* $f(x_1, x_2) = 4x_1 x_2$ if $0 < x_1 < 1, 0 < x_2 < 1$ and 0 o. w.: $X_1$ and $X_2$ are independent


### Expectation

#### Expectation of $Y = g(X_1, X_2)$
For some real valued function $g: R^2 \rightarrow R$ and $Y = g(X_1, X_2)$,
* $X$: discrete,

  If $\sum_{x_1} \sum_{x_2} \mid g(x_1, x_2) \mid p_{X_1, X_2}(x_1, x_2) < \infty, E(Y) = \sum_{x_1} \sum_{x_2} g(x_1, x_2) p_{X_1, X_2} (x_1, x_2)$

* $X$: continuous,

  If $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \mid g(x_1, x_2) \mid f_{X_1, X_2} (x_1, x_2) dx_1 dx_2 < \infty, E(Y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x_1, x_2) f_{X_1, X_2} (x_1, x_2) dx_1 dx_2$

* Property

  For r. vector $(X_1, X_2)$, define $Y_1 = g_1(X_1, X_2)$, and $Y_2 = g_2(X_1, X_2)$. 

  For $\forall k_1, k_2 \in R, E(k_1 Y_1 + k_2 Y_2) = k_1 E(Y_1) + k_2 E(Y_2)$

<!-- 다중적분을 할 때는 꼭 그래프를 그리는 습관을 들이자-->


## Correlation Coefficient

### Covariance
$\sigma_{12} = Cov(X, Y) = E[(X - \mu_1)(Y - \mu_2)] = E(XY) - E(X)E(Y)$, where $\mu_1 = E(X)$ and $\mu_2 = E(Y)$

cf) $Var(X) = E((X - \mu)^2)$

* $Cov(X, Y) = E[(X - \mu_1)(Y - \mu_2)] = E[XY - X \mu_2 - Y \mu_1 + \mu_1 \mu_2] = E(XY) - \mu_1 \mu_2 - \mu_1 \mu_2 + \mu_1 \mu_2 = E(XY) - E(X)E(Y)$


### Correlation
<!-- covariance의 단위에 따른 차이를 scaling하는 게 correlation -->

$\rho = Corr(X, Y) = \frac{Cov(X, Y)}{\sqrt{Var(X)} \sqrt{Var(Y)}} = \frac{Cov(X, Y)}{\sigma_1 \sigma_2}$

If $X$ and $Y$ are independent, then their covariance and correlations are zero.
However, zero correlation does not imply independence.

#### Example
Let $X$ and $Y$ have the joint pmf $f(x, y) = \frac{1}{3}, (x, y) = (0, 1), (1, 0), (2, 1).

Then $X$ and $Y$ are not independent but their covariance is 0.

$f_X(x) = 1/3$ if $x = 0, 1, 2$, $f_Y(y) = 2/3$ if $y = 1$ and $1/3$ if $y = 0$.

$E(XY) = 2/3, E(X) = 1, E(Y) = 2/3$.

$Cov(X, Y) = E(XY) - E(X)E(Y) = 0$


## Conditional Distribution

### Conditional pmf
For $x_2 \in S_{X_2}, p_{X_1 \mid X_2}(x_1 \mid x_2) = \frac{p_{X_1, X_2}(x_1, x_2)}{p_{X_2}(x_2)}$

Conditional pmf of the discrete type of a random variable $X_1$, given that the discrete type of random variable $X_2 = x_2$.

### Conditional pdf
For $x_2 \in S_{X_2}, f_{X_1 \mid X_2}(x_1 \mid x_2) = \frac{f_{X_1, X_2}(x_1, x_2)}{f_{X_2}(x_2)}$

Conditional pdf of the continuous type of a random variable $X_1$, given that the continuous type of random variable $X_2 = x_2$

### Abbreviation for expressing the pdfs
$p_{1, 2}(x_1, x_2) = p_{X_1, X_2}(x_1, x_2), p_1(x_1) = p_{X_1}(x_1), p_{1 \mid 2}(x_1 \mid x_2) = p_{X_1 \mid X_2}(x_1 \mid x_2)$

$f_{1, 2}(x_1, x_2) = f_{X_1, X_2}(x_1, x_2), f_1(x_1) = f_{X_1}(x_1), f_{1 \mid 2}(x_1 \mid x_2) = f_{X_1 \mid X_2}(x_1 \mid x_2)$


### Properties of the conditional pdfs

#### Discrete
* $0 \leq p_{1 \mid 2}(x_1 \mid x_2) \leq 1$ for $x_1 \in S_{X_1}$
* $\sum_{x_1}p_{1 \mid 2}(x_1 \mid x_2) = 1$
* $P(X_1 \in B \mid X_2 = x_2) = \sum_{x_1 \in B}p_{1 \mid 2}(x_1 \mid x_2)$

#### Continuous
* $0 \leq f_{1 \mid 2}(x_1 \mid x_2), -\infty < x_1 < \infty$
* $\int_{-\infty}^{\infty} f_{1 \mid 2}(x_1 \mid x_2) dx_1 = 1$
* $P(X_1 \in B \mid X_2 = x_2) = \int_B f_{1 \mid 2}(x_1 \mid x_2) dx_1$


### Conditional expectation
#### Discrete
$E(u(X_1, X_2) \mid X_2 = x_2) = \sum_{x_1}u(x_1, x_2)p_{1 \mid 2}(x_1 \mid x_2)$

#### Continuous
$E(u(X_1, X_2) \mid X_2 = x_2) = u(x_1, x_2)f_{1 \mid 2}(x_1 \mid x_2)dx_1$


### Conditional mean and variance
Mean and variance of the conditional distribution of $X_1$, given $X_2 = x$
* Conditional mean: $E(X_1 \mid X_2 = x)$
  * $E(X_1 \mid X_2 = x) = \int x_1 f_{1 \mid 2}(x_1 \mid x_2) dx_1$
* Conditional variance: $Var(X_1 \mid X_2 = x) = E[(X_1 - E(X_1 \mid X_2 = x))^2 \mid X_2 = x]$

<!-- conditional mean, variance 모두 x만의 함수가 된다. x_1는 적분해서 없어지기 때문에 결국에는 x의 함수 꼴로 나타 -->

### Properties of conditional expectations
* $E(a X_1 + b \mid X_2 = x) = aE(X_1 \mid X_2 = x) + b$
* $E(g_1(X_1) g_2(X_2) \mid X_2 = x) = g_2(x)E(g_1(X_1) \mid X_2 = x)$  <!-- 다음과 같이 X_2가 fix가 되어있을 때 X_2는 상수취급을 해서 기댓값 밖으로 빠져나온다 -->
* $E(g_1(X_1) + g_2(X_1) \mid X_2 = x) = E(g_1(X_1) \mid X_2 = x) + E(g_2(X_1) \mid X_2 = x)$
* $Var(X_1 \mid X_2 = x) = E(X_1^2 \mid X_2 = x) - \\{E(X_1 \mid X_2 = x)\\}^2$


#### Example
Let $X$ and $Y$ have the joint pmf $f(x, y) = \frac{x + y}{21}, x = 1, 2, 3, y = 1, 2.

We know that $f_1(x) = \frac{2x + 3}{21}, x = 1, 2, 3$, and $f_2(y) = \frac{3y + 6}{21}, y = 1, 2$.

Then what are the conditional pmf, conditional mean and variance of $X$ given $Y = y$?

$f_{1 \mid 2} = \frac{f(x, y)}{f(y)} = \frac{x + y}{3y + 6}$

$E(X \mid Y = y) = \sum_{x = 1}^3 x f_{1 \mid 2}(x \mid y) = 1 \cdot \frac{1 + y}{3y + 6} + 2 \cdot \frac{2 + y}{3y + 6} + 3 \cdot \frac{3 + y}{3y + 6} = \frac{14 + 6y}{3y + 6}$

$E(X^2 \mid Y = y) = 1^2 \cdot \frac{1 + y}{3y + 6} + 2^2 \cdot \frac{2 + y}{3y + 6} + 3^2 \cdot \frac{3 + y}{3y + 6} = \frac{36 + 14y}{3y + 6}$

$Var(X \mid Y = y) = \frac{36 + 14y}{3y + 6} - (\frac{14 + 6y}{3y + 6})^2$


#### Example
Let $X$ and $Y$ have the trinomial pmf with parameters $(n, p_1, p_2)$.

That is $f(x, y) = \frac{n!}{x!y!(n - x - y)!} p_1^x p_2^y (1 - p_1 - p_2)^{n - x - y}, x = 0, \ldots, n, y = 0, \ldots, n, 0 \leq x + y \leq n$

Then what are the conditional pmf, mean and variance of $Y$ given $X = x$ ?

$f_1(x) = \sum_{y = 0}^{n - x} f(x, y) = \sum_{y = 0}^{n - x} \frac{n!}{x!y!(n - x - y)!} p_1^x p_2^y (1 - p_1 - p_2)^{n - x - y}$

$= \frac{n! p_1^x}{x! (n - x)!} \sum_{y = 0}^{n - x} \frac{(n - x)!}{y! (n - x - y)!} p_2^y (1 - p_1 - p_2)^{n - x - y}$

$= \frac{n!}{x! (n - x)!} p_1^x (1 - p_1)^{n - x}$

Therefore, $f_{2 \mid 1} (y \mid x) = \frac{f(x, y)}{f_1(x)} = \frac{(n - x)!}{y! (n - x - y)!} (\frac{p_2}{1 - p_1})^y (1 - \frac{p_2}{1 - p_1})^{n - x - y}$

$= B(n-X, \frac{p_2}{1 - p_1})$

$E(Y \mid X = x) = (n - x) \frac{p_2}{1 - p_1}$

$Var(Y \mid X = x) = (n - x) \frac{p_2}{1 - p_1} (1 - \frac{p_2}{1 - p_1})$

* $\sum_{y = 0}^{n - x} \frac{(n - x)!}{y! (n - x - y)!} p_2^y (1 - p_1 - p_2)^{n - x - y} = (p_2 + 1 - p_1 - p_2)^{n - x}$


### Linear conditional mean
Suppose $Var(X) < \infty$, and $Var(Y) < \infty$.

If $E(Y \mid X) = a + bX$, then, 

$E(Y \mid X) = \mu_2 + \rho \frac{\sigma_2}{sigma_1}(X - \mu_1), E(Var(Y \mid X)) = \sigma_2^2(1 - \rho^2)$

where $\mu_1 = E(X), \mu_2 = E(Y), \sigma_1^2 = Var(X), \sigma_2^2 = Var(Y), \rho = corr(X, Y)$

* $E[Var(Y \mid X)] = \int_{-\infty}^{\infty} Var(Y \mid X = x) \cdot f_X(x) dx$


### Properties with respect to conditional mean and variance   
<!-- 둘다 아주 중요하다고 하심 -->
* $E(E(X_2 \mid X_1)) = E(X_2)$  <!-- 유도는 솔루션 참고. 다 풀어주시긴 하셨다 -->
* $Var(X_2) = Var(E(X_2 \mid X_1)) + E(Var(X_2 \mid X_1))$

  $\Rightarrow Var(E(X_2 \mid X_1)) \leq Var(X_2), E(X_2 \mid X_1)$ is more precise than $X_2$


### Definition of Moment Generating Function of a Random Vector
Let $X = (X_1, X_2)^t$: a r. vector. If $E(e^{t_1 X_1 + t_2 X_2}$ exists for $-h_1 < t_1 < h_1, -h_2 < t_2 < h_2$ where $h_1 > 0$, and $h_2 > 0$,

$M_{X_1, X_2}(t_1, t_2) = E(e^{t_1 X_1 + t_2 X_2}) = E(e^{t^t X})$: joint mgf of $X = (X_1, X_2)^t$

where $t = (t_1, t_2)^t$

### Properties
* $M_{X_1}(t_1) = M_{X_1, X_2}(t_1, 0), M_{X_2}(t_2) = M_{X_1, X_2}(0, t_2)$: marginal mgfs
* $E(X_1^p X_2^q) = \frac{\partial^{p+q}}{\partial t_1^p \partial t_2^q} M_{X_1, X_2}(t_1, t_2)\mid_{t_1 = t_2 = 0}$; joint moment
* $\exists h_1 > 0, h_2 > 0$

  $M_{X_1, X_2}(t_1, t_2) = M_{Y_1, Y_2}(t_1, t_2), -h_1 < t_1 < h_1, -h_2 < t_2 < h_2$

  $\Leftrightarrow F_{X_1, X_2}(w_1, w_2) = F_{Y_1, Y_2}(w_1, w_2) \forall (w_1, w_2) \in R^2$


### Definition (multinomial distribution)
$(X_1, \ldots, X_{k-1} \sim \text{Multi} (n, p_1, \ldots, p_{k-1})$

$\Leftrightarrow p_{X_1, \ldots, X_{k-1}} (x_1, \ldots, x_{k-1}) = \binom{n}{x_1, \ldots, x_{k-1}, x_k} p_1^{x_1} \ldots p_{k-1}^{x_{k-1}} p_k^{x_k}$,

where $x_k = n - \sum_{i=1}^{k-1}x_i$ and $p_k = 1 - \sum_{i=1}^{k-1}p_i$

* mgf: $M_{X_1, \ldots, X_{k-1}}(t_1, \ldots, t_{k-1}) = (p_1 e^{t_1} + \ldots + p_{k-1} e^{t_{k-1}} + p_k)^n, -\infty < t_i < \infty$  <!-- 솔루션 확인하기 -->
* Mean: $E(X_i) = np_i$
* Variances and covariances: $Cov(X_i, X_j) = np_i(1-p_i)$ if $i=j$ and $-np_i p_j$ if $i \neq j$

### Properties
If $(X_1, \ldots, X_{k-1}) \sim Multi(n, p_1, \ldots, p_{k-1})$ and $X_k = n - \sum_{i=1}^{k-1} X_i, p_k = 1 - \sum_{i=1}^{k-1} p_i$
* $X_i \sim B(n, p_i) (i = 1, \ldots, k)$
* $(X_2, \ldots, X_{k-1}) \mid X_1 = x_1 \sim Multi(n - x_1, \frac{p_2}{1 - p_1}, \ldots, \frac{p_{k-1}}{1 - p_1})$
* $(X_1, X_2) \sim Multi(n, p_1, p_2) = Trinomial(n, p_1, p_2)$

<!-- multinomial은 1개면 binomial, 2개면 trinomial -->

#### Example
In manufacturing a certain item, it is found that in normal production about 95% of the items are good ones, 4% are seconds, and 1% are defective.
A company has a program of quality control by statistical methods, and each hour an online inspector observes 20 items selected at random, counting the number $X$ of seconds and the number $Y$ of defectives.
Suppose that the production is normal.
Find the probability that in this sample of size $n = 20$, at least two seconds or at least two defective items are discovered.

$P(X \geq 2 or Y \geq 2) = 1 - P(X \leq 1$ and $Y \leq 1) = 1 - P((X, Y) \in \\{(0, 0), (0, 1), (1, 0), (1, 1) \\}) = 0.204$

$P_{X, Y}(x, y) = \frac{20!}{x!y!(20 - x - y)!} (0.04)^x (0.01)^y (0.95)^{20 - x - y}$


## The Multivariate Normal Distribution

### Expected Value of a Random Vector
* For $X = (X_1, X_2)^t$, if $E \mid X_1 \mid < \infty, E \mid X_2 \mid < \infty, E(X) \equiv (E(X_1), E(X_2))^t$  <!-- column vector -->
* For $X^t = (X_1, X_2)$, if $E \mid X_1 \mid < \infty, E \mid X_2 \mid < \infty, E(X^t) \equiv (E(X_1), E(X_2))$  <!-- row vector -->
* For $X = \begin{matrix} X_11 & X_12 \\ X_21 & X_22 \end{matrix}$, if $E \mid X_11 \mid < \infty, E \mid X_12 \mid < \infty, E \mid X_21 \mid < \infty, E \mid X_22 \mid < \infty, E \equiv \begin{matrix} EX_11 & EX_12 \\ EX_21 & EX_22 \end{matrix}$

### Variance/covariance of a Random Vector
For $X = (X_1, X_2)^t, cov(X) \equiv E[(X-EX)(X-EX)^t] = \begin{matrix} var(X_1) & cov(X_1, X_2) \\ cov(X_1, X_2) & var(X_2) \end{matrix}$

where $E \mid X_1^2 \mid < \infty, E \mid X_2^2 \mid < \infty$

### Property
For the random vector, $X (n\times 1)$, and matrix (or vector) $A$ ,
* $E(AX) = AE(X)$ where $A \in R^{k \times n}$
* $E(X^t A) = E(X)^t A$ where $A \in R^{n \times m}$

For the random vector, $X$, and matrix (or vector) $A$,
$cov(AX) = A cov(X) A^t$

* $Cov(AX) = E[(AX - E(AX))(AX - E(AX))^t] = E(A(X-E(X))(X-E(X))^t A^t) = A E[(X - EX)(X - EX)^t] A^t = Acov(X)A^t$

<!-- (AB)^t = B^t A^t -->


### Bivariate normal
#### Deifnition
$X \sim N_2((\mu_1, \mu_2)^t, \begin{matrix} \sigma_1^2 & \rho \sigma_1 \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{matrix})$ for $X = (X_1, X_2)^t$  <!-- \rho: 상관계 -->

$\Leftrightarrow f_X(x_1, x_2) = \frac{1}{2 \pi \sigma_1 \sigma_2 \sqrt{1 - \rho^2}} \times \exp [-\frac{1}{2(1-\rho^2)} \\{(\frac{x_1 - \mu_1}{\sigma_1})^2 - 2 \rho (\frac{x_1 - \mu_1}{\sigma_1})(\frac{x_2 - \mu_2}{\sigma_2}) + (\frac{x_2 - \mu_2}{\sigma_2})^2 \\}]$

* Mean: $E(X_i) = \mu_i$
* Variance: $var(X_i) = \sigma_i^2$
* Covariance: $cov(X_1, X_2) = \rho \sigma_1 \sigma_2$
* Conditional distribution: $X_1 \mid X_2 \sim N(\mu_1 + \rho \frac{\sigma_1}{\sigma_2} (X_2 - \mu_2), \sigma_1^2 (1 - \rho^2))$
* Marginal pdf
  * $f_{X_1}(x_1) \sim N(\mu_1, \sigma_1^2)$
  * $f_{X_2}(x_2) \sim N(\mu_2, \sigma_2^2)$
<!-- X1, X2가 bivariate normal이면 X1|X2도 normal -->

#### Example
If $X$ and $Y$ have a bivariate normal distribution with correlation coefficient $\rho$, then $X$ and $Y$ are independent if and only if $\rho = 0$
<!-- 정규분포에 대해서만 만족하는 성질. 일반적으로 만족하지는 않는다. rho = 0을 f_X(x_1, x_2)에 넣으면 두 pdf의 곱의 형태로 나타남 -->

### Standard multivariate normal
#### Definition
$Z \sim N_n(0, I_n)$ for $Z = (Z_1, \ldots, Z_n)^t$ (random vector $Z$ has a multivariate normal dist. with mean vector 0 and covariance matrix $I_n$).  <!-- 공분산이 0 -->

Then, $f_Z(z) = (2 \pi)^{-n/2} \exp \\{ -\frac{1}{2}z^t z \\}$ for $z \in R^n$  <!-- z^t z = \sum_{i=1}^n z_i^2 -->

$\Leftrightarrow Z_1, \ldots, Z_n \sim iid N(0, 1)$

* Mean: $E(Z) = 0$
* Covariance; $Cov(Z) = I_n$
* mgf: $M_Z(t) = E[\exp(t^t Z)] = \exp \\{(1/2)t^t t \\}$ for all $t \in R^n$ <!-- exp{(1/2)t^t t} = exp{1/2 t_1^2  + \ldots + 1/2 t_n^2} -->


### Multivariate normal
#### Definition
$X \sim N_n(\mu, \Sigma)$ for $X = (X_1, \ldots, X_n)^t$ and $\Sigma$: symmetric positive definite matrix (definition: $x^t Ax > 0$ for all $x \neq 0$)

Then, $f_X(x) = \det (2 \pi \Sigma)^{-1/2} \exp \\{-\frac{1}{2} (x - \mu)^t \Sigma^{-1} (x - \mu) \\}$ for $x \in R^n$  <!-- det(a b // c d) = ad - bc -->

$\Leftrightarrow$ distribution of $X = \Sigma^{1/2} Z + \mu$ where $\Sigma^{1/2}$: real symmetric and $\Sigma^{1/2} \Sigma^{1/2} = \Sigma$

* Mean: $E(X) = \mu$
* Cov: $Cov(X) = \Sigma$
* mgf: $M_X(t) = \exp \\{t^t \mu + (1/2)t^t \Sigma t \\}$ for all $t \in R^n$

### Related Properties
Suppose $X = (X_1^t, X_2^t)^t \sim N_n(\mu, \Sigma)$ where $X_1: n_1 \times 1, X_2: n_2 \tims 1$ and $n = n_1 + n_2$
* $Y = AX + b \sim N_m(A \mu + b, A \Sigma A^t)$ where $A: (m \times n)$ and $b \in R^m$
* $Cov(X_1, X_2) = 0 \Leftrightarrow X_1$ and $X_2$: independent
* $W = (X - \mu)^t \Sigma^{-1} (X - \mu) \sim \chi^2(n)$

<!-- 정확하게 알지 못해도 적어도 찾아보면 알 수 있어야 한다 -->

#### Remarks
If a real symmetric matrix $A$ is positive definite matrix (definition: $x^t Ax > 0$ for all $x \neq 0$)
* $\Leftrightarrow$ All eigenvalues of $A$ are >0
* $\Leftrightarrow A = BB^t$ for some non-singular (and symmetric) $B$
* $\Rightarrow A$ is non-singular <!-- singular: inverse matrix 존재하지 않는다는 뜻 -->
