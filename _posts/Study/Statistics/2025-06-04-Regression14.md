---
title: Statistical Inference with Resampling Methods
author: JSH
date: 2025-06-04 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Statistical Inference with Resampling Methods
## Fisher’s Tea Taster
* 8 cups of tea are prepared
* four with tea poured first
* four with milk poured first
* The cups are presented to her in random order

## Permutation solution
Mark a strip of paper with eight guesses about the order of the "tea-first" and "milk-first" cups -- let's say T T T T M M M M.

Make a deck of eight cards, four marked "T" and four marked "M".

Deal out these eight cards successively in all possible orderings (permutations).

Record how many of those permutations show >= 6 matches.

## Approximate Permutation
Shuffle the deck and deal it out along the strip of paper with the marked guesses, record the number of matches.
Repeat many times.

## Extension to multiple samples, populations
Fisher went on to apply the same idea to agricultural experiments involving two or more samples. 
The question became:
"How likely is it that random arrangements of the observed data would produce samples differing as much as the observed samples differ?"

In the 1930's, Fisher and Pitman showed that the inference for a permutation test extended to cover not just random re-arrangements of a fixed set of finite elements, but also samples from larger populations.

## Formula-based analogs
Fisher and Pitman showed that the t-distribution and chisquared distribution are good approximations for sufficiently large and/or normally-distributed samples.

However, when data is of un-known distribution or sample size is small, re-sampling tests are recommended.

## Resampling Procedures
The resampling method frees researchers from two limitations of conventional statistics:
* data conform to a bell-shaped curve
* need to focus on statistical measures whose theoretical properties can be analyzed mathematically

The resampling method addresses a key problem in statistics: 
how to infer the truth from a sample of data that may be incomplete or drawn from an ill-defined population.

Using resampling methods, you're trying to get something for nothing.
You use the same numbers over and over again until you get an answer that you can't get any other way.
In order to do that, you have to assume something, and you may live to regret that hidden assumption later on.

### Bootstrap
* Application
  * Standard deviation
  * Confidence interval
  * Hypothesis testing
  * Bias
* Sampling procedure used
  * Samples drawn at random, with replacement

### Jackknife
* Application
  * Standard deviation
  * Confidence interval
  * Bias
* Sampling procedure used
  * Samples consist of full data set with one observation left out

### Permutation
* Application
  * Hypothesis testing
* Sampling procedure used
  * Samples drawn at random, without replacement

### Cross-validation
* Application
  * Model validation
* Sampling procedure used
  * Data is randomly divided into two or more subsets, with results validated across sub-samples

## Permutation Tests
In classical hypothesis testing, we start with assumptions about the underlying distribution and then derive the sampling distribution of the test statistic under $H_0$.

In Permutation testing, the initial assumptions are not needed (except exchangeability), and the sampling distribution of the test statistic under $H_0$ is computed by using permutations of the data.

### Example
The Permutation test is a technique that bases inference on experiments within the observed dataset.

In a medical experiment, rats are randomly assigned to a treatment (Tx) or control (C) group.
The outcome $X_i$ is measured in the ith rat.

Under $H_0$, the outcome does not depend on whether a rat carries the label Tx or C.
Under $H_1$,  the outcome tends to different, say larger for rats labeled Tx.

A test statistic T measures the difference in observed outcomes for the two groups.
T may be the difference in the two group means (or medians), denoted as t for the observed data.

Under $H_0$, the individual labels of Tx and C are unimportant, since they have no impact on the outcome.
Since they are unimportant, the label can be randomly shuffled among the rats without changing the joint null distribution of the data.

Shuffling the data creates a “new” dataset. 
It has the same rats, but with the group labels changed so as to appear as there were different group assignments.

#### Procedure
* Let t be the value of the test statistic from the original dataset
* Let $t_1$ be the value of the test statistic computed from a one dataset with permuted labels
* Consider all M possible permutations of the labels, obtaining the test statistics, $t_1, \ldots, t_M$
* Under $H_0$, $t_1, \ldots, t_M$ are all generated from the same underlying distribution that generated t

Thus, t can be compared to the permuted data test statistics, $t_1, \ldots, t_M$, to test the hypothesis and obtain a p-value or to construct confidence limits for the statistic.

## Bootstrap
### Nonparametric bootstrap
Notations
* Random sample: $x = (x_1, x_2, \ldots, x_n)$
* Empirical distribution $\hat{F}$, places mass of $1/n$ at each observed data value.
* Bootstrap sample: Random sample of size $n$, drawn from $\hat{F}$, denoted as $x* = (x_1 *, x_2 *, \ldots, x_n *)$
* Bootstrap replicate of $\hat{\theta}$: $\hat{\theta} * = s(x*)$

#### Bootstrap steps
* Select bootstrap sample $x* = (x_1 *, x_2 * , \ldots, x_n *)$ consisting of $n$ data values drawn with replacement from the original data set.
* Evaluate $\hat{\theta} * = s(x*)$ for the bootstrap sample
* Repeat steps 2 and 3 $B$ times each
* Estimate the standard error $se_F (\hat{\theta})$ by the sample standard deviation of the B replications:
  * $SE_B = \sqrt{\frac{\sum_{i=1}^B (\theta_i * - \bar{\theta *})^2}{B-1}}$

### Parametric bootstrap
Resampling makes no assumptions about the population distribution. 
The bootstrap covered thus far is a nonparametric bootstrap.

If we have information about the population distribution, this can be used in resampling. 
In this case, when we draw randomly from the sample we can use population distribution.

For example, if we know that the population distribution is normal, then estimate its parameters using the sample mean and variance. 
Then approximate the population distribution with the sample distribution and use it to draw new samples.

As expected, if the assumption about population distribution is correct then the parametric bootstrap will perform better than the nonparametric bootstrap. 
If not correct, then the nonparametric bootstrap will perform better.

### How many Bootstrap Replications, B?
A fairly small number, $B=25$, is sufficient to be informative (Efron).

$B=50$ is typically sufficient to provide a crude estimate of the SE, but $B>200$ is generally used.

CIs require larger values of $B$, $B$ no less than 500, with $B=1000$ recommended.




