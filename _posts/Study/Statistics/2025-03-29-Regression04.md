---
title: Multiple Linear Regression
author: JSH
date: 2025-03-29 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Multiple Linear Regression

## Data structure and the model

$y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i, i = 1, \ldots, n$. ($\tilde{Y} = X \tilde{\beta} + \tilde{\epsilon}$)
* $\epsilon_1, \ldots, \epsilon_n$: independent with $E(\epsilon_i) = 0$ and $var(\epsilon_i) = \sigma^2$
* $\beta_0, \ldots, \beta_p, \sigma^2 > 0$: unknown. we need to estimate $(p+1) + 1$ parameters
* $X = (\tilde{1}, \tilde{x_1}, \ldots, \tilde{x_p}), rank(X) = p+1, X$: given where $\tilde{x_j} = (x_{1j}, \ldots, x_{nj})^T$

## Least squares estimates
Minimize $\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_{i1} - \ldots - \beta_p x_{ip})^2$ w.r.t. $\beta_0, \ldots, \beta_p$

Normal equation: $e_i = y_i - (\hat{\beta_0} + \hat{\beta_1} x_{i1} + \ldots + \hat{\beta_p} x_{ip}) = y_i - \hat{y_i}$
* $\sum e_i = 0 \Rightarrow \hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x_1} - \ldots - \hat{\beta_p} \bar{x_p}$
* $\sum x_{i1} e_i = 0 \Rightarrow \sum (x_{i1} - \bar{x_1}) e_i = 0 \Rightarrow S_{11} \hat{\beta_1} + \ldots + S_{1p} \hat{\beta_p} = S_{y1}$
* $\sum x_{ip} e_i = 0 \Rightarrow \sum (x_{ip} - \bar{x_p}) e_i = 0 \Rightarrow S_{p1} \hat{\beta_1} + \ldots + S_{pp} \hat{\beta_p} = S_{yp}$
<!-- 유도하는 방법 다시 듣고 확인하기!!!! -->

where $S_{ij} = \sum_{a = 1}^n (x_{ai} - \bar{x_i})(x_{aj} - \bar{x_j}), S_{yj} = \sum_{a = 1}^n (y_{a} - \bar{y})(x_{aj} - \bar{x_j})$

Therefore, least squares regression fit: $\hat{y} = \hat{\beta_0} + \hat{\beta_1} x_1 + \ldots + \hat{\beta_p} x_p$

Estimate (unbiased) of $\theta^2$: $\hat{\theta}^2 = \frac{1}{n - p - 1} \sum_{i=1}^n (y_i - \hat{y_i})^2 = \frac{1}{n - p - 1}$ SSE

## Matrix approach
For $\tilde{y} = (y_1, \ldots, y_n)^T, \tilde{x_j} = (x_{1j}, \ldots, x_{nj})^T (j = 1, \ldots, p), X = (\tilde{1}, \tilde{x_1}, \ldots, \tilde{x_p}), \tilde{\beta} = (\tilde{\beta_0}, \ldots, \tilde{\beta_p})^T$, and $\tilde{\epsilon} = (\epsilon_1, \ldots, \epsilon_n)^T$

* Model: $\tilde{y} = X \tilde{\beta} + \tilde{\epsilon}$
* Assumptions: $\epsilon_1, \ldots, \epsilon_n$ are independent with $E(\epsilon_i) = 0$ and $var(\epsilon_i) = \sigma^2$
* Least square estimate: $\tilde{\hat{\beta}} = \arg \min_{\tilde{\beta}} (\tilde{y} - X \tilde{\beta})^T (\tilde{y} - X \tilde{\beta})$

$(\tilde{y} - X \tilde{\beta})^T (\tilde{y} - X \tilde{\beta}) = (\tilde{y}^T - \tilde{\beta}^T X^T)(\tilde{y} - X \tilde{\beta}) = \tilde{y}^T \tilde{y} - \tilde{y}^T X \tilde{\beta} - \tilde{\beta}^T X^T \tilde{y} + \tilde{\beta}^T X^T X \tilde{\beta}$

$\frac{\partial (\tilde{\beta}^T X^T \tilde{y})}{\partial \tilde{\beta}} = X^T \tilde{y}$

Therefore, $\frac{\partial}{\partial \tilde{\beta}} (\tilde{y} - X \tilde{\beta})^T (\tilde{y} - X \tilde{\beta}) = 2X^T X \tilde{\beta} - X^T \tilde{y} - X^T \tilde{y} = \tilde{0}$

$\hat{\tilde{\beta}} = (X^T X)^{-1} X^T \tilde{y}$

$rank(X^T X) = rank(XX^T) = rank(X) = rank(X^T) = p + 1$
* $X: n \times (p + 1)$
* $(X^T X)^{-1}: (p + 1) \cdot n \times n \cdot (p + 1) = (p + 1)(p + 1)$

$\tilde{\hat{y}} = X \hat{\tilde{\beta}} = X(X^T X)^{-1} X^T \tilde{y}$

<!-- multiple regression의 기하학적 의미도 simple regression에서와 동일하다. (y hat도 y 점에서 Xbeta에 수직으로 내린 점이다!) -->

### In case of $p=1$
$\tilde{\hat{\beta}} = \begin{bmatrix} \hat{\beta_0} \\\ \hat{\beta_1} \end{bmatrix} = \begin{bmatrix} n & \sum x_j \\\ \sum x_j & \sum x_j^2 \end{bmatrix}^{-1} \begin{bmatrix} \sum y_j \\\ \sum x_j y_j \end{bmatrix}$
$= \begin{bmatrix} \frac{(\sum s_j^2)(\sum y_j) - (\sum x_j)(\sum x_j y_j)}{n \sum x_j^2 - (\sum x_j)^2} \\\ \frac{n \sum x_j y_j - (\sum x_j)(\sum y_j)}{n \sum x_j^2 - (\sum x_j)^2} \end{bmatrix}$

Note: 
$\begin{bmatrix} n & \sum x_j \\\ \sum x_j & \sum x_j^2 \end{bmatrix}^{-1} = \frac{1}{n \sum x_j^2 - (\sum x_j)^2} \begin{bmatrix} \sum x_j^2 & -\sum x_j \\\ -\sum x_j & n \end{bmatrix}$

This is coincident with the result of simple linear regression.

## Method of inference





