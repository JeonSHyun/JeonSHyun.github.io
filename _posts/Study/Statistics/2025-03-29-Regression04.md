---
title: Multiple Linear Regression
author: JSH
date: 2025-03-29 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Multiple Linear Regression

## Data structure and the model

$y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i, i = 1, \ldots, n$. ($\tilde{Y} = X \tilde{\beta} + \tilde{\epsilon}$)
* $\epsilon_1, \ldots, \epsilon_n$: independent with $E(\epsilon_i) = 0$ and $var(\epsilon_i) = \sigma^2$
* $\beta_0, \ldots, \beta_p, \sigma^2 > 0$: unknown. we need to estimate $(p+1) + 1$ parameters
* $X = (\tilde{1}, \tilde{x_1}, \ldots, \tilde{x_p}), rank(X) = p+1, X$: given where $\tilde{x_j} = (x_{1j}, \ldots, x_{nj})^T$

## Least squares estimates
Minimize $\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_{i1} - \ldots - \beta_p x_{ip})^2$ w.r.t. $\beta_0, \ldots, \beta_p$

Normal equation: $e_i = y_i - (\hat{\beta_0} + \hat{\beta_1} x_{i1} + \ldots + \hat{\beta_p} x_{ip}) = y_i - \hat{y_i}$
* $\sum e_i = 0 \Rightarrow \hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x_1} - \ldots - \hat{\beta_p} \bar{x_p}$
* $\sum x_{i1} e_i = 0 \Rightarrow \sum (x_{i1} - \bar{x_1}) e_i = 0 \Rightarrow S_{11} \hat{\beta_1} + \ldots + S_{1p} \hat{\beta_p} = S_{y1}$
* $\sum x_{ip} e_i = 0 \Rightarrow \sum (x_{ip} - \bar{x_p}) e_i = 0 \Rightarrow S_{p1} \hat{\beta_1} + \ldots + S_{pp} \hat{\beta_p} = S_{yp}$
<!-- 유도하는 방법 다시 듣고 확인하기!!!! -->

where $S_{ij} = \sum_{a = 1}^n (x_{ai} - \bar{x_i})(x_{aj} - \bar{x_j}), S_{yj} = \sum_{a = 1}^n (y_{a} - \bar{y})(x_{aj} - \bar{x_j})$

Therefore, least squares regression fit: $\hat{y} = \hat{\beta_0} + \hat{\beta_1} x_1 + \ldots + \hat{\beta_p} x_p$

Estimate (unbiased) of $\theta^2$: $\hat{\theta}^2 = \frac{1}{n - p - 1} \sum_{i=1}^n (y_i - \hat{y_i})^2 = \frac{1}{n - p - 1}$ SSE

## Matrix approach
For $\tilde{y} = (y_1, \ldots, y_n)^T, \tilde{x_j} = (x_{1j}, \ldots, x_{nj})^T (j = 1, \ldots, p), X = (\tilde{1}, \tilde{x_1}, \ldots, \tilde{x_p}), \tilde{\beta} = (\tilde{\beta_0}, \ldots, \tilde{\beta_p})^T$, and $\tilde{\epsilon} = (\epsilon_1, \ldots, \epsilon_n)^T$

* Model: $\tilde{y} = X \tilde{\beta} + \tilde{\epsilon}$
* Assumptions: $\epsilon_1, \ldots, \epsilon_n$ are independent with $E(\epsilon_i) = 0$ and $var(\epsilon_i) = \sigma^2$
* Least square estimate: $\tilde{\hat{\beta}} = \arg \min_{\tilde{\beta}} (\tilde{y} - X \tilde{\beta})^T (\tilde{y} - X \tilde{\beta})$

$(\tilde{y} - X \tilde{\beta})^T (\tilde{y} - X \tilde{\beta}) = (\tilde{y}^T - \tilde{\beta}^T X^T)(\tilde{y} - X \tilde{\beta}) = \tilde{y}^T \tilde{y} - \tilde{y}^T X \tilde{\beta} - \tilde{\beta}^T X^T \tilde{y} + \tilde{\beta}^T X^T X \tilde{\beta}$

$\frac{\partial (\tilde{\beta}^T X^T \tilde{y})}{\partial \tilde{\beta}} = X^T \tilde{y}$

Therefore, $\frac{\partial}{\partial \tilde{\beta}} (\tilde{y} - X \tilde{\beta})^T (\tilde{y} - X \tilde{\beta}) = 2X^T X \tilde{\beta} - X^T \tilde{y} - X^T \tilde{y} = \tilde{0}$

$\hat{\tilde{\beta}} = (X^T X)^{-1} X^T \tilde{y}$

$rank(X^T X) = rank(XX^T) = rank(X) = rank(X^T) = p + 1$
* $X: n \times (p + 1)$
* $(X^T X)^{-1}: (p + 1) \cdot n \times n \cdot (p + 1) = (p + 1)(p + 1)$

$\tilde{\hat{y}} = X \hat{\tilde{\beta}} = X(X^T X)^{-1} X^T \tilde{y}$

<!-- multiple regression의 기하학적 의미도 simple regression에서와 동일하다. (y hat도 y 점에서 Xbeta에 수직으로 내린 점이다!) -->

### In case of $p=1$
$\tilde{\hat{\beta}} = \begin{bmatrix} \hat{\beta_0} \\\ \hat{\beta_1} \end{bmatrix} = \begin{bmatrix} n & \sum x_j \\\ \sum x_j & \sum x_j^2 \end{bmatrix}^{-1} \begin{bmatrix} \sum y_j \\\ \sum x_j y_j \end{bmatrix}$
$= \begin{bmatrix} \frac{(\sum s_j^2)(\sum y_j) - (\sum x_j)(\sum x_j y_j)}{n \sum x_j^2 - (\sum x_j)^2} \\\ \frac{n \sum x_j y_j - (\sum x_j)(\sum y_j)}{n \sum x_j^2 - (\sum x_j)^2} \end{bmatrix}$

Note: 
$\begin{bmatrix} n & \sum x_j \\\ \sum x_j & \sum x_j^2 \end{bmatrix}^{-1} = \frac{1}{n \sum x_j^2 - (\sum x_j)^2} \begin{bmatrix} \sum x_j^2 & -\sum x_j \\\ -\sum x_j & n \end{bmatrix}$

This is coincident with the result of simple linear regression.

## Method of inference

### Properties of estimates
Recall that $E(\tilde{y}) = X \tilde{\beta}, var(\tilde{y}) = var(\tilde{\epsilon}) = I_n \sigma^2, \tilde{\hat{y}} = X \tilde{\hat{\beta}} = X(X^T X)^{-1} X^T \tilde{y} = P \tilde{y}$

* $E(\tilde{\hat{\beta}}) = \tilde{\beta}$
  * $E(\tilde{\hat{\beta}}) = E[(X^T X)^{-1} X^T \tilde{y}] = (X^T X)^{-1} X^T E(\tilde{y}) = \tilde{\beta}$
* $var(\tilde{\hat{\beta}}) = (X^T X)^{-1} \sigma^2$
  * $var(\tilde{\hat{\beta}}) = (X^T X)^{-1} X^T var(\tilde{y}) X(X^T X)^{-1} = (X^T X)^{-1} \sigma^2$
* $E(\hat{\sigma}^2) = \sigma^2$
  * $E(SSE) = e^T e = E(tr(e^T e)) = E(tr(ee^T)) = tr E(ee^T) = tr var(e) = tr((I - P) \sigma^2) = \sigma^2 (n - p - 1)$
* $E(\tilde{e}) = 0$
  * $\tilde(e) = tilde{y} - \tilde{\hat{y}} = (I - P) \tilde{y}$
  * $E(\tilde{e}) = (I - P)E(\tilde{y}) = (I - P)X \tilde{\beta} = X \tilde{\beta} - PX \tilde{\beta} = X \tilde{\beta} - X \tilde{\beta} = 0$
* $var(\tilde{e}) = (I - P) \sigma^2$
  * $var(\tilde{e}) = (I - P)var(\tilde{y})(I - P)^T = (I - P)I_n \sigma^2 (I - P) = (I - P)^2 \sigma^2 = (I - P) \sigma^2$
* SSE = $\frac{1}{n-p-1} y^T (I - P)y = e^T e$
  * $\hat{\sigma^2} = \frac{1}{n - p - 1}$ SSE = $\frac{1}{n - p - 1} (y - X \hat{\beta})^T (y - X \hat{\beta}) = \frac{1}{n - p - 1} y^T (I - P)^T (I - P) y = \frac{1}{n - p - 1} y^T (I - P) y$

<!-- 유도 잘 생각해보기.. 강의 다시 듣기.. 여기 하나하나 유도해보기.... -->

### Inference under additional normality assumption
Let $C = (X^T X)^{-1} = (c_{ij})_{0 \leq i, j \leq p}$

#### $\beta_i$
$\frac{\hat{\beta_i} - \beta_i}{s.e.(\hat{\beta_i})} \sim t(n - p - 1)$

$s.e.(\hat{\beta_i}) = \hat{\sigma} c_{ii}^{1/2} (i = 1, \ldots, p)$

$Pr\\{\beta_i \in \hat{\beta_i} \pm t(n - p - 1; \alpha/2) s.e. (\hat{\beta_i}) \\} = 1 - \alpha$

Reject $H_0: \beta_i = \beta_i^0$ v.s. $H_1: \beta_i \neq \beta_i^0$ iff $\mid \frac{\hat{\beta_i} - \beta_i}{s.e.(\hat{\beta_i})} \mid > t(n - p - 1, \alpha/2)$

#### $\beta_0$
$\frac{\hat{\beta_0} - \beta_0}{s.e.(\hat{\beta_0})} \sim t(n - p - 1)$

$s.e.(\hat{\beta_0}) = \hat{\sigma} c_{00}^{1/2}$

Similar to $\hat{\beta_i}$

#### $\mu_0$
$\mu_0 = E(Y \mid \tilde{x_0}) = \tilde{x^T_0} \tilde{\beta}$ where $\tilde{x_0} = (x_{00}, x_{01}, \ldots, x_{0p})^T$ with $x_{00} = 1$

$\frac{\hat{\mu_0} - \mu_0}{s.e.(\hat{\mu_0})} \sim t(n - p - 1)$

$var(hat{\mu_0}) = var(\tilde{x^T_0} \hat{\beta}) = \tilde{x^T_0} var(\hat{\beta}) \tilde{x_0} = \tilde{x^T_0} (X^T X)^{-1} \sigma^2 \tilde{x_0}$

$s.e.(\hat{\mu_0}) = \hat{\sigma} [\tilde{x^T_0} (X^T X)^{-1} tilde{x_0}]^{1/2}$

$\because \hat{\mu_0} = \tilde{x^T_0} \tilde{\hat{\beta}}$

$\therefore var(\hat{\mu_0}) = \tilde{x^T_0} var(\hat{\beta}) \tilde{x_0}$

C.I. and Test

#### Prediction for $y_0$
$y_0 = \tilde{x^T_0} \tilde{\beta} + \epsilon_0 (\epsilon_0 \perp \epsilon_1, \ldots, \epsilon_n)$

$\hat{y_0} = \tilde{x^T_0} \tilde{\hat{\beta}} (+ \hat{\epsilon_0})$

$\frac{y_0 - \hat{y_0}}{s.e.(y_0 - \hat{y_0})} \sim t(n - p - 1)$

$s.e.(y_0 - \hat{y_0}) = \hat{\sigma} (1 + \tilde{x^T_0} (X^T X)^{-1} \tilde{x_0})^{1/2}$

## Measuring the quality of fit
### Decomposition of sum of squares
$\sum_{i = 1}^n (y_i - \bar{y})^2$ (SST) $= \sum_{i=1}^n (y_i - \hat{y_i})^2$ (SSE) $+ \sum_{i=1}^n (\hat{y_i} - \bar{y})^2$ (SSR)

Recall, for $e_i = y_i - \hat{y_i}, \sum e_i = 0, \sum x_{i1} e_i = 0, \ldots, \sum x_{ip} e_i = 0$

($\sum (x_{i1} - \bar{x_1}) e_i = 0, \ldots, \sum (x_{ip} - \bar{x_p}) e_i = 0$)

SST = $\sum (y_i - \bar{y})^2 = \sum ((y_i - \hat{y_i}) + (\hat{y_i} - \bar{y}))^2 = \sum (y_i - \hat{y_i})^2 + \sum (\hat{y_i} - \bar{y})^2 + 2 \sum (y_i - \hat{y_i})(\hat{y_i} - \bar{y})$

$\sum (y_i - \hat{y_i})(\hat{y_i} - \bar{y}) = \sum e_i \\{\hat{\beta_1} (x_{i1} - \bar{x_1}) + \ldots + \hat{\beta_p} (x_{ip} - \bar{x_p}) \\} = 0$

Degree of freedom
* SST: n-1
  * $\sum y_i - \bar{y} = 0$
* SSE: n-p-1
  * $\sum e_i = 0, \sum x_{i1} e_i = 0, \ldots, \sum x_{ip} e_i = 0$
* SSR: p
  * $\hat{\beta_1} (x_{i1} - \bar{x_1}) + \ldots + \hat{\beta_p} (x_{ip} - \bar{x_p})$ <!-- beta만 자유롭게 움직일 수 있다 -->

### Multiple correlation coefficient (MCC) & Adjusted MCC
$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$

$0 \leq R^2 \leq 1$

High $R^2$ means that "determination of $y$ by linear combination of $x$ becomes larger" or "proportion of variation of $y$ explained by $x_1, \ldots, x_p$".

### Adjusted $R^2$
As the number of explanatory variables increases, $R^2$ increases and SSE decreases.

$\min_{\tilde{\beta} \in R^{p+1}} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_{i1} - \ldots - \beta_p x_{ip})^2 \leq \min_{\tilde{\beta} \in R^{p+1}, \beta_1 = 0} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_{i1} - \ldots - \beta_p x_{ip})^2$

In other words, SSE(reducted model) $\geq$ SSE(full model) and $R^2$(reduced model) $\leq R^2$(full model).

$R^2$ is inappropriate for comparing fitness between models with different numbers of explanatory variables.

Therefore, the following adjusted $R^2$ is considered.

$R^2_a = 1 - \frac{SSE/(n-p-1)}{SST/(n-1)}$  

<!-- 설명변수의 개수가 서로 다른 모형의 적합도 비교에 사용 -->

## Hypotheses testing in linear regression model

### Reduced model vs. Full model
<!-- Reduced model: 축소모형, full model: 완전모형 -->

* $H_0$: reduced model (RM)
  * $q+1$ regression parameters
* $H_1$: full model (FM)
  * $p+1$ regression parameters
where RM $\subset$ FM, $q = p-r$

#### Example
* FM: $y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_6 x_{i6} + \epsilon_i (i = 1, \ldots, 30)$
* RM
  * $H_0: y_i = \beta_0 + \epsilon_i$ v.s. $H_1$: full model $\Rightarrow H_0: \beta_1 = \ldots = \beta_6 = 0$ v.s. $H_1$: not $H_0$  <!-- ANOVA -->
  * $H_0: y_i = \beta_0 + \beta_1 x_{i1} + \beta_3 x_{i3} + \epsilon_i$ v.s. $H_1$: full model $\Rightarrow H_0: \beta_2 = \beta_4 = \beta_5 = \beta_6 = 0$ v.s. $H_1$: not $H_0$
  * $H_0: \beta_1 = \beta_2$ v.s. $H_1$: not $H_0$

### Sums-of-squares in RM & FM
<!-- SST는 RM과 FM에서 동일하다 -->

#### SSE
* SSE(FM) = $\sum_{i = 1}^n (y_i - \hat{y_i}^{FM})^2$
  * $\hat{y_i}^{FM}$: l.s. fit under FM
  * Number of parameters = $p + 1$
  * Degree of freedom = $n - p - 1$
* SSE(RM) = $\sum_{i = 1}^n (y_i - \hat{y_i}^{RM})^2$
  * $\hat{y_i}^{RM}$: l.s. fit under RM
  * Number of parameters = $q + 1$
  * Degree of freedom = $n - q - 1$

SSE(FM) = $\min_{\tilde{\beta} \in R^{p+1}} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_{i1} - \ldots - \beta_p x_{ip})^2 \leq \min_{restrictions w.r.t \tilde{\beta}} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_{i1} - \ldots - \beta_p x_{ip})^2 =$ SSE(RM)

#### SSR
* SSR(FM) = SST - SSE(FM)
  * Degree of freedom = p
* SSR(RM) = SST - SSE(RM)
  * Degree of freedom = q

#### SST 
SST = $\sum_{i=1}^n (y_i - \bar{y})^2$

#### SSE(RM) - SSE(FM)
reduction in "residual" s.s. by introducing $p-q$ more parameters (variables) to RM

#### SSR(FM) - SSR(RM)
added amount of explanation due to $p-q$ more parameters (variables) to RM

SSE(RM) - SSE(FM) = SSR(FM) - SSR(RM)

## F-test statistic for RM vs. FM
$F = \frac{(SSR(FM) - SSR(RM))/(df(FM)-df(RM))}{SSE(FM)/(n-df(FM))} = \frac{(R^2_p - R^2_q)/((p+1) - (q+1))}{(1-R^2_p)/(n-p-1)} = \frac{(SSE(RM)-SSE(FM))/(df(FM)-df(RM))}{(SSE(FM))/(n-df(FM))}$

$F \sim F(p-q, n-p-1)$ under $H_0$

Reject RM vs. FM if $F \geq F(p-q, n-p-1; \alpha)$

$\frac{1}{p+1} E[SSR(FM) - SSR(RM)] = \sigma^2 + \frac{1}{p+1} \sum_{i=1}^n (\mu_i - \bar{\mu})^2, \bar{\mu} = \frac{1}{n} \sum_{i=1}^n \mu_i$





