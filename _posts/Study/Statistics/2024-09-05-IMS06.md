---
title: Convergence
author: JSH
date: 2024-09-05 10:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, IMS]
use_math: true
---

# Convergence

## Expectation of Function

### Parameter
When the pdf(pmf) of $X$ is $f(x; \theta), \theta$ is called parameter.
The shape of pdf depends on the parameter.

#### Example
$X \sim N(\mu, \sigma^2) \Rightarrow \theta - (\mu, \sigma^2)$

$X \sim B(n, p) \Rightarrow \theta = p$

### Parameter space
The set of values of parameters.
Denoted as $\Omega$.

### Random sample
$X_1, X_2, \ldots, X_n$: random sample
$\Leftrightarrow X_1, X_2, \ldots, X_n$ are independent and identically distributed (iid)

### Statistic
$T$: statistic
$\Leftrightarrow T = g(X_1, \ldots, X_n)$ for any function, $g$

### Point estimator
$T$: point estimator
$\Leftrightarrow T$ is used to estimator $\theta$

#### Example
$X_1, \ldots, X_n \sim$ iid $(\mu, \sigma^2)$
* Sample mean: $\bar{X} = \frac{1}{n} (X_1 + \ldots + X_n) \Rightarrow$ statistic and point estimator for $\mu$
* Sample variance: $S^2 = \frac{1}{n-1} \sum_{i = 1}^n (X_i - \bar{X})^2 \Rightarrow$ statistic and point estimator for $\sigma^2$

$X_1, \ldots, X_n \sim$ iid Bernoulli $(p)$
* Sample proportion: $\bar{X} = \frac{1}{n} (X_1 + \ldots + X_n) \Rightarrow$ statistic and point estimator for $p$

#### Facts



