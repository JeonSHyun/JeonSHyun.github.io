---
title: Convergence
author: JSH
date: 2024-09-05 10:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, IMS]
use_math: true
---

# Convergence

## Expectation of Function

### Parameter
When the pdf(pmf) of $X$ is $f(x; \theta), \theta$ is called parameter.
The shape of pdf depends on the parameter.

#### Example
$X \sim N(\mu, \sigma^2) \Rightarrow \theta - (\mu, \sigma^2)$

$X \sim B(n, p) \Rightarrow \theta = p$

### Parameter space
The set of values of parameters.
Denoted as $\Omega$.

### Random sample
$X_1, X_2, \ldots, X_n$: random sample
$\Leftrightarrow X_1, X_2, \ldots, X_n$ are independent and identically distributed (iid)

### Statistic
$T$: statistic
$\Leftrightarrow T = g(X_1, \ldots, X_n)$ for any function, $g$

### Point estimator
$T$: point estimator
$\Leftrightarrow T$ is used to estimator $\theta$

#### Example
$X_1, \ldots, X_n \sim$ iid $(\mu, \sigma^2)$
* Sample mean: $\bar{X} = \frac{1}{n} (X_1 + \ldots + X_n) \Rightarrow$ statistic and point estimator for $\mu$
* Sample variance: $S^2 = \frac{1}{n-1} \sum_{i = 1}^n (X_i - \bar{X})^2 \Rightarrow$ statistic and point estimator for $\sigma^2$

$X_1, \ldots, X_n \sim$ iid Bernoulli $(p)$
* Sample proportion: $\bar{X} = \frac{1}{n} (X_1 + \ldots + X_n) \Rightarrow$ statistic and point estimator for $p$

#### Facts
* If $T = a_1 X_1 + \ldots + a_n X_n$ and $E \mid X_i \mid < \infty (i = 1, \ldots, n)$, then $E(T) = a_1 E(X_1) + \ldots + a_n E(X_n)$
* If $T = a_1 X_1 + \ldots + a_n X_n, W = b_1 Y_1 + \ldots + b_m Y_m$, and if $E(X_i^2) < \infty, E(Y_j^2) < \infty (i = 1, \ldots, n, j = 1, \ldots, m)$, then $Cov(T, W) = \sum_{i = 1}^n \sum_{j = 1}^m a_i b_j Cov(X_i, Y_j)$
* If $T = a_1 X_1 + \ldots + a_n X_n$ and $E(X_i^2) < \infty (i = 1, \ldots, n)$, then $Var(T) = Cov(T, T) = \sum_{i = 1}^n a_i^2 Var(X_i) + 2 \sum_{i < j} a_i a_j Cov(X_i, X_j)$

### Unbiased estimator
Let $X_1, \ldots, X_n$ be random samples from the distribution, $f(x; \theta)$, and $T$ denote a statistic.
Then $T$ is called unbiased estimator if and only if $E(T) = \theta$.
Otherwise, $T$ is a biased estimator of $\theta$.



