---
title: Linear Methods for Regression
author: JSH
date: 2024-07-02 10:03:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics]
use_math: true
---

# Linear Methods for Regression

## Introduction

### Regression
Regression is a statistical method used to study the relationship between a dependent variable and one or more independent varibles.
The goal of regression is to estimate or interpret the outcome variable based on the predictors.

### Linear Regression
Linear regression is a simple supervised learning approach, assuming that the dependence of $Y$ on $X$ is linear.

$y = \beta_0 + \sum_{j=1}^p \beta_j x_j + \epsilon$

### Assumptions of Linear Regressiong
Linearity, Independence, Normality, Equal Variance (LINE)

### Coefficient Estimate
Ordinary Least Squares (OLS), Maximum Likelihood Estimation (MLE)

## Ordinary Least Squares
### Minimize the residual sum of squares (RSS)
Residual = Observed value - Predicted value = $y_i - \hat{y}_i$

#### Simple Linear Regression
$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$
* $\beta_0 = \bar{y} - \beta_1 \bar{x}$
* $\beta_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{Cov(x, y)}{Var(x)}$

#### Multiple Linear Regression
$y = X \beta + \epsilon$
* $\beta = (X^T X)^{-1} X^T y$

### Expectation
$E[\hat{\beta}] = (X^T X)^{-1} X^T E[y]$

$y = X \beta + \epsilon$

$E[y] = X \beta (\because E[\epsilon] = 0)$

$E[\hat{\beta}] = (X^T X)^{-1} X^T (X \beta)$

$(X^T X)^{-1} X^T X = I$

Therefore, $E[\hat{\beta}] = \beta \rightarrow$ unbiased estimator

### Variance
$Var(\hat{\beta}) = Var((X^T X)^{-1} X^T y) = Var((X^T X)^{-1} X^T (X \beta + \epsilon)$

$\hat{\beta} = \beta + (X^T X)^{-1} X^T \epsilon$

$Var(\hat{\beta}) = Var((X^T X)^{-1} X^T \epsilon) = (X^T X)^{-1} X^T Var(\epsilon) X(X^T X)^{-1}$

$Var(\epsilon) = \sigma^2 I (\because Var(\epsilon) = \sigma^2 I_n)$

$Var(\hat{\beta}) = \sigma^2 (X^T X)^{-1}$

## Quality of fit

### $R^2$
Proportion of variance of y explained by x.

$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$

$0 \leq R^2 \leq 1$

* SST = $\sum_{i=1}^n (y_i - \bar{y})^2$
* SSR = $\sum_{i=1}^n (\hat{y_i} - \bar{y})^2$
* SSE = $\sum_{i=1}^n (\hat{y_i} - y_i)^2$

### MSE
MSE = $\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2$

### RMSE
RMSE = $\sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2}$

### Error vs. Residual
* Error = $\epsilon_i = y_i - E[y_i]$
* Residual = $e_i = y_i - \hat{y_i}$

## Gauss-Markov Theorem

### Best Linear Unbiased Estimator (BLUE)
The least squares estimator has the smallest variance of all unbiased linear estimators.

### Gauss-Markov Theorem
$Var(\tilde{\beta}) \geq Var(\hat{\beta})$
* $\tilde{\beta}$: another linear unbiased estimator of $\beta$



## Singular value decomposition
The SVD of the $N \times p$ matrix $X$ can be written as $X = UDV^T$.
$U$ and $V$ are orthogonal matrices.
Columns of $U$ span the column of space of $X$.
Columns of $V$ span the row space of $X$.
$D$ is a $p \times p$ diagnoal matrix with entries

Find out $S, U, V$ by eigen decomposition
* $AA^T = U \Sigma V^T \cdot V \Sigma^T U^T = U \Sigma \Sigma^T U^T$
* $A^T A = V \Sigma^T U^T \cdot U \Sigma V^T = V \Sigma^T \Sigma V^T$
