---
title: Linear Methods for Regression
author: JSH
date: 2024-07-02 10:03:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, ESL]
use_math: true
---

# Linear Methods for Regression

## Introduction

### Regression
Regression is a statistical method used to study the relationship between a dependent variable and one or more independent varibles.
The goal of regression is to estimate or interpret the outcome variable based on the predictors.

### Linear Regression
Linear regression is a simple supervised learning approach, assuming that the dependence of $Y$ on $X$ is linear.

$y = \beta_0 + \sum_{j=1}^p \beta_j x_j + \epsilon$

### Assumptions of Linear Regressiong
Linearity, Independence, Normality, Equal Variance (LINE)

### Coefficient Estimate
Ordinary Least Squares (OLS), Maximum Likelihood Estimation (MLE)

## Ordinary Least Squares
### Minimize the residual sum of squares (RSS)
Residual = Observed value - Predicted value = $y_i - \hat{y}_i$

#### Simple Linear Regression
$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$
* $\beta_0 = \bar{y} - \beta_1 \bar{x}$
* $\beta_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{Cov(x, y)}{Var(x)}$

#### Multiple Linear Regression
$y = X \beta + \epsilon$
* $\beta = (X^T X)^{-1} X^T y$

### Expectation
$E[\hat{\beta}] = (X^T X)^{-1} X^T E[y]$

$y = X \beta + \epsilon$

$E[y] = X \beta (\because E[\epsilon] = 0)$

$E[\hat{\beta}] = (X^T X)^{-1} X^T (X \beta)$

$(X^T X)^{-1} X^T X = I$

Therefore, $E[\hat{\beta}] = \beta \rightarrow$ unbiased estimator

### Variance
$Var(\hat{\beta}) = Var((X^T X)^{-1} X^T y) = Var((X^T X)^{-1} X^T (X \beta + \epsilon)$

$\hat{\beta} = \beta + (X^T X)^{-1} X^T \epsilon$

$Var(\hat{\beta}) = Var((X^T X)^{-1} X^T \epsilon) = (X^T X)^{-1} X^T Var(\epsilon) X(X^T X)^{-1}$

$Var(\epsilon) = \sigma^2 I (\because Var(\epsilon) = \sigma^2 I_n)$

$Var(\hat{\beta}) = \sigma^2 (X^T X)^{-1}$

## Quality of fit

### $R^2$
Proportion of variance of y explained by x.

$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$

$0 \leq R^2 \leq 1$

* SST = $\sum_{i=1}^n (y_i - \bar{y})^2$
* SSR = $\sum_{i=1}^n (\hat{y_i} - \bar{y})^2$
* SSE = $\sum_{i=1}^n (\hat{y_i} - y_i)^2$

### MSE
MSE = $\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2$

### RMSE
RMSE = $\sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2}$

### Error vs. Residual
* Error = $\epsilon_i = y_i - E[y_i]$
* Residual = $e_i = y_i - \hat{y_i}$

## Gauss-Markov Theorem

### Best Linear Unbiased Estimator (BLUE)
The least squares estimator has the smallest variance of all unbiased linear estimators.

### Gauss-Markov Theorem
$Var(\tilde{\beta}) \geq Var(\hat{\beta})$
* $\tilde{\beta}$: another linear unbiased estimator of $\beta$

## MLE for Linear Regression

### Maximum Likelihood Estimator (MLE)
MLE estimates the parameters by maximizing a likelihood function so that, under the assumed statistical model, the obaserved data is most possible.

OLS is also guaranteed to be statistically optimal through MLE.

$\arg \max_{\beta} l(\beta) = \arg \min_{\beta} \sum_{i=1}^n (y_i - X_i^T \beta)^2$

## Limitations of OLS
### Multicollinearity
Collinearity among independent variables makes estimation impossible or increase variance

$\beta = (X^T X)^{-1} X^T y$

$A^{-1} = \frac{1}{det(A)} \cdot adj(A)$

### Overfitting

## Bias-Variance Trade-off
MSE($\hat{\theta}$) = Bias($\hat{\theta})^2$ + Variance($\hat{\theta}$)

## Subset Selection

### Best-Subset Selection
Compute the least squares fit for all possible subsets and then choose between them based on some criterion that balances training error with model size

### Forward Stepwise Selection
Begins with a model containing no predictors, and then adds predictors to the model one-at-a-time, until all of the predictors are in the model

### Backward Stepwise Selection
Begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time

### Forward Stagewise Selection
Add a predictor one-at-a-time, without changing the fitted coefficients from the previous stages

## Ridge Regression
Ridge regression is a linear regression method that uses the L2 norm as a regularization term to penalize large coefficients, reducing overfitting and improving the model's generalization ability.
It does not shrink coefficients to exactly zero.

### L2 norm
$\mid x \mid_2 = \sqrt{\sum_{i=1}^n x_i^2}$

### Constraint Form
$\hat{\beta}^{ridge} = \arg \min_{\beta} \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2$ subject to $\sum_{j=1}^p \beta_j^2 \leq t$

### Lagrangian Form
$\hat{\beta}^{ridge} = \arg \min_{\beta} \\{\sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2$

### Matrix Form
$\hat{\beta}^{ridge} = (X^T X + \lambda I)^{-1} X^T y$

## Lasso Regression
Lasso regression uses the L1 norm to promote sparsity in coefficients, reducing overfitting, improving generalizatoin, and enabling dimensionality reduction through feature selection.

### L1 norm
$\mid x \mid_1 = \sum_{i=1}^n \mid x_i \mid$

### Constraint Form
$\hat{\beta}^{lasso} = \arg \min_{\beta} \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2$ subject to $\sum_{j=1}^p \mid \beta_j \mid \leq t$

### Lagrangian Form
$\hat{\beta}^{lasso} = \arg \min_{\beta} \\{\frac{1}{2} \sum_{j=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^p \mid \beta_j \mid \\}$

## Elastic Net
Elastic Net is a regularized regression method that combines the L1 norm (lasso) and L2 norm (ridge) penalties to encourage sparsity while addressing multicollinearity, providing a balance between feature selection and coefficient stability.

### Constratint Form
$\hat{\beta}^{elastic net} = \arg \min_{\beta} \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2$ subject to $\alpha \sum_{i=1}^p \mid \beta_j \mid + (1 - \alpha) \sum_{j=1}^p \beta_j^2 \leq t$

### Lagrangian Form
$\hat{\beta}^{elastic net} = \arg \min_{\beta} \\{\frac{1}{2} \sum_{i=1}^N (y_i - \beta_0 - \sum_{i=1}^N x_{ij} \beta_j)^2 + \lambda[\alpha \sum_{j=1}^p \mid \beta_j \mid + (1 - \alpha)\sum_{j=1}^p \beta^2_j] \\}$

## Principal Component Regression (PCR)
PCR works by performing PCA on the predictor variables to reduce dimensionality and then fitting a regression model using the selected principal components as predictors

### Algorithm
* Standardize
* Compute the Covariance Matrix
* Eigenvalue Decomposition
* Transform the Predictor Matrix
* Perform Regression in the Principal Component Space

### Singular value decomposition
The SVD of the $N \times p$ matrix $X$ can be written as $X = UDV^T$.
$U$ and $V$ are orthogonal matrices.
Columns of $U$ span the column of space of $X$.
Columns of $V$ span the row space of $X$.
$D$ is a $p \times p$ diagnoal matrix with entries

Find out $S, U, V$ by eigen decomposition
* $AA^T = U \Sigma V^T \cdot V \Sigma^T U^T = U \Sigma \Sigma^T U^T$
* $A^T A = V \Sigma^T U^T \cdot U \Sigma V^T = V \Sigma^T \Sigma V^T$

## Partial Least Squares (PLS)
PLS identifies latent variables that maximize the covariance between X and y while reducing the dimensionality of X by capturing key variations in both

$\hat{Y} = \sum_{i=1}^k t_i b_i + t_2 b_2 + \ldots + t_k b_k$

Notatoin
* $X$: Independent variables $(n \times p)$
* $Y$: Dependent variable $(n \times 1)$
* $w$: Weight vector $(p \times 1)$
* $t$: latent variable $(n \times 1)$

Expansion
* $t = Xw$
* $cov(t, Y) = corr(t, Y) \sqrt{var(t)} \sqrt{var(Y)}$
* Maximize $cov(t, Y) \propto$ Maximize $corr(t, Y) var(t)$
* Maximize $corr(t, Y) = cov(Xw, Y) = cov(Wx, Y) = E[(Xw-E[Xw]) \cdot (Y-E[Y])] = E[(Xw) \cdot (Y)] = \frac{1}{n} \sum_{i=1}^n (Xw)_i \cdot Y_i = \frac{1}{n} w^T (X^T Y)$

Therefore, $w^T (X^T Y) = \mid w \mid \cdot \mid X^T Y \mid \cdot \cos \theta$.
It is maximize when $\theta = 0$.
$\therefore w = X^T Y$

### Algorithm
* Step 1: Mean centering
* Step 2:
  * Set $X_1 = X, Y_1 = Y$
  * $w_1 = x_1^T Y_1 \rightarrow w_1 = \frac{x_1^T Y_1}{\mid x_1^T Y_1 \mid}$ (scaling)
  * $t_1 = X_1 w_1$
  * $Y_1 = t_1 b_1 + F_1, b_1 = (t_1^T t_1)^{-1} (t_1^T Y_1)$ by OLS
  * $X_1 = t_1 p_1^T + E_1, p_1^T = (t_1^T t_1)^{-1} (t_1^T X_1)$ by OLS
* Step 3:
  * $Y_2 = F_1 = Y_1 - t_1 b_1$
  * $X_2 = E_1 = X_1 - t_1 p_1^T$
  * $w_2 = x_2^T Y_2 \rightarrow w_2 = \frac{x_2^T Y_2}{\mid x_2^T Y_2 \mid}$ (scaling)
  * $t_2 = X_2 w_2$
  * $Y_2 = t_2 b_2 + F_2, b_2 = (t_2^T t_2)^{-1} (t_2^T Y_2)$ by OLS
  * $X_2 = t_2 p_2^T + E_2, p_2^T = (t_2^T t_2)^{-1} (t_2^T X_2)$ by OLS
* Step 4: Iterate


