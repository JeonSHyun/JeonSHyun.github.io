---
title: Weighted Least Squares (WLS)
author: JSH
date: 2025-05-05 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Weighted Least Squares
<!-- 회귀 진단에서 문제가 있을 때 중요한 공변량이 빠졌는지 먼저 확인해보고, 혹시 등분산성이 깨졌을 때 WLS 방법도 취해볼 수 있다 -->

<!-- 예시와 같은 이분산성이 있는 경우
var(y|x) = f(x). p-value for x & other variables correlated with x는 신뢰할 수 없다

RCT같은 경우 변수들의 독립성을 만족하기 위해 사용한다. 그럼 다른 변수들과의 연관성이 사라진다.

-->

## Strategies for treating heteroskedasticity
### Transformation of variables
* log transformation ($\ln y_i = \beta_0 + \beta_1 x_i + \epsilon_i$)
* $y_i = \beta_0 + \beta_1 x_i + \epsilon_i, var(\epsilon_i) = k^2 x_i^2$, then, $\frac{y_i}{x_i} = \beta_0 \frac{1}{x_i} + \beta_1 + \epsilon_i *, var(\epsilon_i *) = k^2$

### WLS
Minimize $\sum \frac{1}{x_i^2}(y_i - \beta_0 - \beta_1 x_i)^2 = \sum w_i (y_i - \beta_0 - \beta_1 x_i)^2$ where $w_1, \ldots, w_n$: weights

The second transformation gives the same result as WLS, but it is difficult to interpret the result.

## Weighted Least Squares (WLS)
Model: $y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i, \epsilon_i \sim$ indep. $N(0, c_i^2 \sigma^2)$ $(i = 1, \ldots, n)$

<!-- WLS를 쓰려면 c_i를 알아야 한다. 중회귀분석을 해서 어떤 설명변수와 관련이 있는지 파악함. 사전정보 필요. -->

Minimize $\sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}))^2 / var(y_i \mid x_i)$ w.r.t. $\beta_0, \ldots, \beta_p$

$\Leftrightarrow$ minimize $\sum_{i=1}^n c_i^{-2} (y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}))^2$ w.r.t. $\beta_0, \ldots, \beta_p$

$\Rightarrow \sum_{i=1}^n w_i e_i = \sum_{i=1}^n w_i x_{i1} e_i = \ldots = \sum_{i=1}^n w_i x_{ip} e_i = 0$

<!-- w_i는 c_i^-2이므로 c_i가 클수록 w_i는 작아진다. 오차항의 분산이 크다는 것은 신뢰도가 떨어진다는 의미. 
따라서 신뢰도가 떨어지는 데이터는 가중치를 적게 주는게 좋다. 분산이 큰 데이터는 beta 추정에 영향을 작게 미치도록 하는 방법 -->

### IDEA
Adjust the weight so that inaccurate observations have less impact on "min. of SSE"

Extremely, if $w_i = 0$, these observations are excluded from the estimate

If $w_1 = w_2 = \ldots = w_n$, OLS

## Sums of Squares in WLS
SST = SSR + SSE

* It should be noted that $\sum w_i (\hat{y_i}^w - \bar{y}^w)(y_i - \hat{y_i}^w) = 0$ because $\sum x_i e_i = 0$ and $\sum w_i x_i e_i = 0$
* $\hat{\sigma}^2 = \sum w_i (y_i - \hat{y_i}^w)^2 /(n - p - 1)$: Estimation of $\sigma^2$ in $\epsilon_i \sim$ indep. $N(0, c_i^2 \sigma^2)$ 

### SST
SST = $\min_a \sum w_i (y_i - a)^2= \sum w_i (y_i - \bar{y}^w)^2$ where $\bar{y}^w = \frac{\sum w_i y_i}{w_i}$
<!-- 가중평균 계산 -->
* Degree of freedom: n-1 <!-- SST에서 $\sum w_i (y_i - \bar{y}^w)^2 = 0$이어서 하나 뺌 -->

### SSE
SSE = $\min_{\beta} \sum w_i (y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}))^2 = \sum w_i (y_i - \hat{y_i}^w)^2$

* $\hat{y_i}^w = \hat{\beta_0}^w + \hat{\beta_1}^w x_{i1} + \ldots + \hat{\beta_p}^w x_{ip}$
* Degree of freedom: n-p-1

### SSR
SSR = $\sum w_i (\hat{y_i}^w - \bar{y}^w)^2$

* Degree of freedom: p

<!-- w_i=1이면 원래 SST, SSE, SSR의 공식이 된다 -->

## Matrix notations
$\tilde{\hat{\beta}} = \arg \min_{\beta} \sum w_i (y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}))^2 = \arg \min_{\beta} (Y - X \beta)^t W (Y - X \beta)$

$\tilde{\hat{\beta}}: X^t W(Y-X \beta) = 0 \Rightarrow \tilde{\hat{\beta}} = (X^t W X)^{-1} X^t WY$ where $W =$ diag($w_1, \ldots, w_n$)
<!-- 위의 식을 beta에 대해 미분해서 구함. 따로 과정을 적지는 않겠다. 필요 시 해볼 것 -->

When $Var(Y) = V$, $W = V^{-1}$

* $E(\tilde{\hat{\beta}}) = \beta$ even when $W$ is misspecified
  * $E(\tilde{\hat{\beta}}) = E[(X^t W X)^{-1} X^t WY] = E[(X^t W X)^{-1} X^t W X \beta] = \beta$
  * Satisfied even if the following formula is valid: $Var(Y) \neq W^{-1} \sigma^2$

Var(Y) = W^{-1} \sigma^2











