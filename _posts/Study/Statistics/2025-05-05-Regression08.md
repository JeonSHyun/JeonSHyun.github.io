---
title: Weighted Least Squares (WLS)
author: JSH
date: 2025-05-05 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Weighted Least Squares
<!-- 회귀 진단에서 문제가 있을 때 중요한 공변량이 빠졌는지 먼저 확인해보고, 혹시 등분산성이 깨졌을 때 WLS 방법도 취해볼 수 있다 -->

<!-- 예시와 같은 이분산성이 있는 경우
var(y|x) = f(x). p-value for x & other variables correlated with x는 신뢰할 수 없다

RCT같은 경우 변수들의 독립성을 만족하기 위해 사용한다. 그럼 다른 변수들과의 연관성이 사라진다.

-->

## Strategies for treating heteroskedasticity
### Transformation of variables
* log transformation ($\ln y_i = \beta_0 + \beta_1 x_i + \epsilon_i$)
* $y_i = \beta_0 + \beta_1 x_i + \epsilon_i, var(\epsilon_i) = k^2 x_i^2$, then, $\frac{y_i}{x_i} = \beta_0 \frac{1}{x_i} + \beta_1 + \epsilon_i *, var(\epsilon_i *) = k^2$

### WLS
Minimize $\sum \frac{1}{x_i^2}(y_i - \beta_0 - \beta_1 x_i)^2 = \sum w_i (y_i - \beta_0 - \beta_1 x_i)^2$ where $w_1, \ldots, w_n$: weights

The second transformation gives the same result as WLS, but it is difficult to interpret the result.

## Weighted Least Squares (WLS)
Model: $y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i, \epsilon_i \sim$ indep. $N(0, c_i^2 \sigma^2)$ $(i = 1, \ldots, n)$

<!-- WLS를 쓰려면 c_i를 알아야 한다. 중회귀분석을 해서 어떤 설명변수와 관련이 있는지 파악함. 사전정보 필요. -->

Minimize $\sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}))^2 / var(y_i \mid x_i)$ w.r.t. $\beta_0, \ldots, \beta_p$

$\Leftrightarrow$ minimize $\sum_{i=1}^n c_i^{-2} (y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}))^2$ w.r.t. $\beta_0, \ldots, \beta_p$

$\Rightarrow \sum_{i=1}^n w_i e_i = \sum_{i=1}^n w_i x_{i1} e_i = \ldots = \sum_{i=1}^n w_i x_{ip} e_i = 0$

<!-- w_i는 c_i^-2이므로 c_i가 클수록 w_i는 작아진다. 오차항의 분산이 크다는 것은 신뢰도가 떨어진다는 의미. 
따라서 신뢰도가 떨어지는 데이터는 가중치를 적게 주는게 좋다. 분산이 큰 데이터는 beta 추정에 영향을 작게 미치도록 하는 방법 -->

### IDEA
Adjust the weight so that inaccurate observations have less impact on "min. of SSE"

Extremely, if $w_i = 0$, these observations are excluded from the estimate

If $w_1 = w_2 = \ldots = w_n$, OLS

## Sums of Squares in WLS
SST = SSR + SSE

* It should be noted that $\sum w_i (\hat{y_i}^w - \bar{y}^w)(y_i - \hat{y_i}^w) = 0$ because $\sum x_i e_i = 0$ and $\sum w_i x_i e_i = 0$
* $\hat{\sigma}^2 = \sum w_i (y_i - \hat{y_i}^w)^2 /(n - p - 1)$: Estimation of $\sigma^2$ in $\epsilon_i \sim$ indep. $N(0, c_i^2 \sigma^2)$ 

### SST
SST = $\min_a \sum w_i (y_i - a)^2= \sum w_i (y_i - \bar{y}^w)^2$ where $\bar{y}^w = \frac{\sum w_i y_i}{w_i}$
<!-- 가중평균 계산 -->
* Degree of freedom: n-1 <!-- SST에서 $\sum w_i (y_i - \bar{y}^w)^2 = 0$이어서 하나 뺌 -->

### SSE
SSE = $\min_{\beta} \sum w_i (y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}))^2 = \sum w_i (y_i - \hat{y_i}^w)^2$

* $\hat{y_i}^w = \hat{\beta_0}^w + \hat{\beta_1}^w x_{i1} + \ldots + \hat{\beta_p}^w x_{ip}$
* Degree of freedom: n-p-1

### SSR
SSR = $\sum w_i (\hat{y_i}^w - \bar{y}^w)^2$

* Degree of freedom: p

<!-- w_i=1이면 원래 SST, SSE, SSR의 공식이 된다 -->

## Matrix notations
$\tilde{\hat{\beta}} = \arg \min_{\beta} \sum w_i (y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}))^2 = \arg \min_{\beta} (Y - X \beta)^t W (Y - X \beta)$

$\tilde{\hat{\beta}}: X^t W(Y-X \beta) = 0 \Rightarrow \tilde{\hat{\beta}} = (X^t W X)^{-1} X^t WY$ where $W =$ diag($w_1, \ldots, w_n$)
<!-- 위의 식을 beta에 대해 미분해서 구함. 따로 과정을 적지는 않겠다. 필요 시 해볼 것 -->

When $Var(Y) = V$, $W = V^{-1}$

* $E(\tilde{\hat{\beta}}) = \beta$ even when $W$ is misspecified
  * $E(\tilde{\hat{\beta}}) = E[(X^t W X)^{-1} X^t WY] = E[(X^t W X)^{-1} X^t W X \beta] = \beta$
  * Satisfied even if the following formula is valid: $Var(Y) \neq W^{-1} \sigma^2$

* $var(\tilde{\hat{\beta}}) = var((X^t W X)^{-1} X^t WY) = (X^t W X)^{-1} X^t W var(Y) WX(X^t WX)^{-1} = \sigma^2 (X^t W X)^{-1}$
  * If $var(Y) = \sigma^2 W^{-1}, var(\tilde{\hat{\beta}}) = \sigma^2 (X^t W X)^{-1}$
  * o.w. $(X^t W X)^{-1} X^t W var(Y) WX(X^t WX)^{-1}$
  * Note: $Var(Y) = W^{-1} \sigma^2$

* Estimating equation: $X^t W(Y- X \beta) = 0$

## Interpretation of WLS
Model: $y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i, \epsilon_i \sim$ indep. $N(0, c_i^2 \sigma^2)$. $(i = 1, \ldots, n)$

* WLS minimizes $\sum_{i=1}^n w_i (y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}))^2$ w.r.t. $\beta_0, \ldots, \beta_p$
* i.e. minimizes $\sum_{i=1}^n (\sqrt{w_i} \cdot y_i - (\beta_0 \sqrt{w_i} + \beta_1 \sqrt{w_i} \cdot x_{i1} + \ldots + \beta_p \sqrt{w_i} \cdot x_{ip}))^2$  w.r.t. $\beta_0, \ldots, \beta_p$

If we reduce this to homogeneous variance model,
$\sqrt{w_i} \cdot y_i = \beta_0 \sqrt{w_i} + \beta_1 \sqrt{w_i} \cdot x_{i1} + \ldots + \beta_p \sqrt{w_i} \cdot x_{ip} + \sqrt{w_i} \cdot \epsilon_i$ where $w_i = 1/c_i^2$

* $y_i^* = \beta_0 + \beta_1 x_{i1}^* + \ldots + \beta_p x_{ip}^* + \epsilon_i^*$
* $\epsilon_i^* \sim$ indep. $N(0, \sigma^2)$. $(i = 1, \ldots, n)$

Then, application of OLS without intercept to $(y_i^*, x_{i0}^*, \ldots, x_{ip}^*)$ yeilds WLS to $(y_i; 1, x_{i1}, \ldots, x_{ip})$
* Thus, MSE = SSE/(n-p-1) at $(y_^*, x_{i0}^*, \ldots, x_{ip}^*)$ is an estimate of $\sigma^2$
* Residual: $e_i = y_i^* - \hat{y_i^*} = \sqrt{w_i} (y_i - \hat{y_i^w})$

<!-- 예시의 경우 var(Y_i)와 x_i^2가 비례관계에 있으므로 w_i = 1/x_i^2로 설정 -->

## Case of Unknown Variance Ratio (Two-stage estimation)
When it is difficult to understand the structure of $c_i$ (dispersive structure) through a $(x, y)$-scatter plot or a residual plot.

Mainly useful cases include (1) replicated observations or (2) grouped data.

<!-- figure 7.2: 명목형 변수의 경우. 동일한 샘플을 반복측정. 각 그룹별 표준분산을 구해서 각각 c_i 대신 사용 -->

###  Nonconstant variance with replicated observations
Model: $y_{ij} = \beta_0 + \beta_1 x_j + \epsilon_{ij}; \epsilon_{ij} \sim N(0, \sigma_j^2), i = 1, 2, \ldots, n, \sigma_j^2 > 0$: unknown
* $j$: cluster or group index
* $\bar{y_j}$: the mean of the response variable in the $j$ th cluster
* $\hat{\sigma_j^2} = \sum_{i=1}^{n_j} (y_{ij} - \bar{y_j})^2 / (n_j - 1), w_{ij} = 1/\hat{\sigma_j^2}$
* WLS estimator = $\arg \min_{\beta = (\beta_0, \beta_1)} \sum_j \sum_{i=1}^{n_j} w_{ij} (y_{ij} - (\beta_0 + \beta_1 x_j))^2$

### Clustering observations according to meaningful associations
Model: $y_{ij} = \beta_0 + \beta_1 x_{ij1} + \ldots + \beta_p x_{ijp} + \epsilon_{ij}$ where $\epsilon_{ij} \sim N(0, c_j^2 \sigma^2), i = 1, 2, \ldots, n_j, c_j^2 > 0$: unknown & $j$: group index

Group may have been observed from scratch or found by need (re-collect data) during analysis

#### Step 1: Obtain a preliminary estimate of $c_j^2$
Perform OLS method for observations with variance $c_j^2 \sigma^2 (= \sigma_j^2)$

$\hat{\sigma_j^2} = \sum_{i=1}^{n_j} (y_{ij} - \hat{y_{ij}})^2 / (n_j - 1)$

$\hat{c_j^2} = \hat{\sigma_j^2} / \hat{\sigma^2}$

$\hat{\sigma^2} = \sum_j \sum_{i=1}^{n_j} (y_{ij} - \hat{y_{ij}})^2 / \sum_j n_j$

where $y_{1j}, \ldots, y_{nj}$: measurement of $j^{th}$ group, $n_j$: # of obeservations at $j^{th}$ group, and $\hat{y_{ij}}$: OLS estimates from the pooled data

#### Step 2: Apply the result in Step 1 with $\hat{w_j} = \hat{c_j^{-2}}$ or $\hat{w_j} = \hat{\sigma_j^{-2}}$
WLS estimator = $\arg \min_{\beta} \sum_j \sum_{i=1}^{n_j} \hat{w_j} (y_{ij} - (\beta_0 + \beta_1 x_{ij1} + \ldots + \beta_p x_{ijp}))^2$















