---
title: Continuous Random variables
author: JSH
date: 2024-07-12 10:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, IMS]
use_math: true
---

# Continuous Random Variables

## Continuous Distribution
If the measurements could come from an interval of possible outcomes, we call them data from a continuous-type population or continuous-type data

### Histogram
A histogram is a graphical display of tabular frequencies, shown as adjacent rectangles.
We assume that a simple of $n$ observations, $x_1, \ldots, x_n$, from a distribution are observed.
If we have cutpoints for class intervals: $(c_0, c_1), (c_1, c_2), (c_2, c_3), \ldots, (c_{k-1}, c_k)$, then each rectangle has an area equal to the relative frequency $\frac{f_i}{n}$ of the observations for the class.
That is, the function is defined by $h(x) = \frac{f_i}{n(c_i - c_{i-1})}$, for $c_i < x < c_i, i = 1, \ldots, k$.
<!-- i번째 구간에 들어가는 데이터 숫자를 f_i라고 했을 때. -->
<!-- n이 무한대로 발산할 때, h(x)는f(x)로 수렴한다. 히스토그램의 극한이 확률밀도함수 -->

With a sample size of $n \geq 30$, the histogram is an adequate estimate of the distribution $p(y)$ that produced your data.
Larger $n$ provides better accuracy.

<!-- 히스토그램은 n이 고정되어있어도 bin의 개수에 따라 형태가 달라진다 -->

### Empirical rule
We assume that a sample of $n$ observations, $x_1, \ldots, x_n$ from a distribution are observed.

If we let $\overline{x}$ and $s$ be a sample mean and a sample standard deviation respectively, and the histogram of these datais bell shaped, then
* Approximately 68% of the data are in the interval $(\overline{x} - s, \overline{x} + s)$
* Approximately 95% of the data are in the interval $(\overline{x} - 2s, \overline{x} + 2s)$
* Approximately 99.7% of the data are in the interval $(\overline{x} - 3s, \overline{x} + 3s)$
<!-- 표본평균, 표준편차 -->

### Stem-and-leaf display
If the leaves are carefully aligned vertically, the table has the same effect as a histogram, but the original numbers are not lost
<!-- 분포와 관찰값을 바로 알 수 있다는 장점이 있다 -->

### Order statistics
<!-- 순서통계량 -->
For a sample of $n$ observations $(x_1, \ldots, x_n)$, when the observations are ordered from small to large, the resulting ordered data are called the order statistics.
The smallest observation is called the first order statistics and the $i$th smallest observation is called the $i$th order statistics.

### (100$p$)th sample percentile (= sample quantile of order $p$)
<!-- p는 0~1 사이의 값. sample이란 말이 들어간 이유는 앞서 관찰한 데이터 x_1 ~ x_n로부터 계산했기 때문이다 -->
If $(n+1)p$ is not an integer but is equal to $r$ plus some proper fraction, $a/b$, the (100$p$)th sample percentile is $y_r + (a/b)(y_{r+1} - y_r) = (1 - a/b)y_r + (a/b)y_{r+1}$ where $n$ is the number of sample and $y_r$ is the $r$th order statistics.

With a sample size $n \geq 30$, the quantiles are adquate estimates, making qq plot reasonably trustworthy.
Qq plot compares population quantile and sample quantile.
Larger $n$ provides better accuracy.

#### Example
When there are 50 samples,

50th percentile: $51 \times 0.5 = 25.5$ and the sample median is $(1 - 0.5)y_{25} + 0.5 y_{26}$, $r = 25, a/b = 0.5$

25th percentile: $51 \times 0.25 = 12.75$ and the first quantile is $(1 - 0.75)y_{12} + 0.75 y_{13}$

#### Percentile vs quantile
25 percentile = 1st quantile

50 percentile = 2nd quantile

75 percentile = 3rd quantile


### If there are $n$ observations and $y_r$ indicates the $r$th order statistics,
* Midrange: $0.5(y_1 + y_n)$  <!-- 중심에 대한 measure -->
* Trimean: 0.25(1st quartile) + 0.5(2nd quartile) + 0.25(3rd quartile)  <!-- 중심에 대한 measure. 극단적인 값에 영향을 덜 받는다. -->
* Range: $y_n - y_1$  <!-- 산포(데이터의 변동)에 대한 measure -->
* Interquartile range (IQR): (3rd quartile) - (1st quartile)  <!-- 75% - 25%. 산포에 대한 measure -->


### (100$p$)th percentiles of a population (= a population quantile of order $p$)
<!-- 데이터의 분포에 대한 가정이 필요. 데이터가 정규분포를 따른다는 가정 하에 population quantile 계산 -->
(100$p$)th percentile is a number $\pi_p$ such that the area under $f(x)$ to the left of $\pi_p$ is $p$.
The $r$th order statistic, $y_r$, from $n$ measurements is the quantile of order $r/(n+1)$ as well as the $100r/(n+1)$th percentile.


### Quantile-quantile plot (QQ plot)
<!-- sample quantile은 데이터로부터, population quantile은 정규분포 가정이 들어가서 계산함. 
만약 데이터가 정규분포를 따른다면 sample quantile과 population quantile은 비슷해야한다.
따라서, 두 quantile이 비슷하면 population quantile의 분포를 sample quantile이 따른다고 볼 수 있다
데이터 분포 확인하는데 가장 좋은 방법 -->

We assume that there are $n$ measurements from some distribution.
If we let the $r$th order statistics by $y_r$ and $p = r/(n+1)$, the plot of $(\pi_p, y_r)$ for several values of $r$ is called the q-q plot

q-q plot can be used to confirm the assumed distribution.

#### Interpreting quantile-quantile plot
* Compare the data to the straight line. The closer they are to the line, the more they appear as if produced by the assumed model
* Data points higher than the line in the upper right suggest that the pdf that produced the data tends to give more extreme values in the upper tail than does the assumed distribution
* Data points lower than the line in the upper right suggest that the pdf that produced the data tends to give less extreme values in the upper tail than does the assumed distribution
* Pronounced horizontal lines are evidence of discreteness


## Properties of pdfs
* $0 \leq f_X(x), -\infty < x < \infty$
* $\int_{-\infty}^{\infty} f_X(x) dx = 1$
* $P(a < X \leq b) = \int_a^b f_X(x) dx$
* $P(a < X \leq b) = P(a < X < b) = P(a \leq X < b) = P(a \leq X \leq b)$


## Transformations
<!-- 복잡하니 공식처럼 외워두자 -->

### Support
$S_X$: support of $X \Leftrightarrow S_X = \\{x: f_X(x) > 0 \\}, f_X(x)$: pdf of $X$

$S_Y$: support of $Y \Leftrightarrow S_Y = \\{x: f_Y(y) > 0 \\}, f_Y(y)$: pdf of $Y$

<!-- support: 확률밀도함수가 0 이상일 때-->

### Transformation: method 1
Method 1 to obtain pdf of $Y = g(X)$: cdf technique  
<!-- 확률밀도함수의 정의 자체가 cdf를 미분하여 pdf를 얻는 데서 시작하므로 무조건 F_X(x) 누적확률부터 생각하고 이를 바탕으로 pdf를 찾자. 1. 확률 구하기 2.확률의 누적 구하기 3.미분해서 pdf 구하기 -->

$f_Y(y) = \frac{d}{dy} F_Y(y), F_Y(y) = P(Y \leq y) = P(g(X) \leq y)$


#### Example
Let $X$ have the distribution function $F(x)$ of the continuous type that is strictly increasing on the support $a < x < b$.
Then if $Y = F(X)$, find a distribution function of $Y$.

$F_Y(y) = P(Y \leq y) = P(F(X) \leq y) = P(X \leq F^{-1}(y)) = F_X(F^{-1} (y)) = y$

Support: $(a, b) \rightarrow (F(a), F(b))$

$f_Y(y) = 1$ if $F(a) < y < F(b)$


### Jacobian ($J$)
For $y = g(x), J = \frac{dx}{dy} = \frac{d}{dy}g^{-1}(y) = \frac{1}{\frac{dy}{dx}}$.
Since $x = g^{-1}(y)$.

### Transformation: method 2
Method 2 to obtain pdf of $Y = g(X)$: transformation

Suppose that $f_X(x)$: pdf of $X$ and $g(x)$: differentiable function

#### One-to-one
When $g(x)$ is a one-to-one differentiable function on $S_X$

$f_Y(y) = f_X(g^{-1}(y)) \cdot \mid \frac{dx}{dy} \mid$ for $y \subset S_Y$ and $0$ in o.w.

and $S_Y = \\{y: y = g(x), x \subset S_X \\}$

* If $y = g(x)$ is increasing
  * $F_Y(y) = P(Y \leq y) = P(g(X) \leq y) = P(X \leq g^{-1}(y)) = F_X(g^{-1}(y))$
  * Therefore, $f_Y(y) = \frac{d}{dy}F_X(g^{-1}(y)) = f_X(g^{-1}(y)) \cdot \frac{d}{dy} g^{-1}(y)$
* If $y = g(x)$ is decreasing
  * $F_Y(y) = P(Y \leq y) = P(g(X) \leq y) = P(X \geq g^{-1}(y))$
  * Therefore, $f_Y(y) = \frac{d}{dy}F_X(g^{-1}(y)) = \frac{d}{dy} (1 - P(X \leq g^{-1}(y))) = \frac{d}{dy} (1 - F_X(g^{-1}(y))) = -\frac{d}{dy}F_X(g^{-1}(y)) = -f_X(g^{-1}(y)) \cdot \frac{d}{dy}g^{-1}(y)$ <!-- 양수임 g^-1는 음수기 때문 -->
  
<!-- 이부분 잘 모르겠으면 해설 보기!! 그림 그려서 이해하기 -->

#### Transformation in general cases
$g(x)$ may be a $r$-to-1

Suppose:
* $\\{S_0, S_1, \ldots, S_r \\}$: partition of $S_X, S_Y = \cup_{i=0}^r T_i$ where $P(X \in S_0) = 0$ and $T_0 = \\{y: y = g(x), x \in S_0 \\}$
* $g_i(i = 1, \ldots, r)$ is a one-to-one function from $S_i$ to $T_i$
* For $x \in S_i, g_i(x) = g(x)$ and $g_i(x)$: differentiable on $S_i$, then, $f_Y(y) = \sum_{y \in T_i} f_X(g_i^{-1}(y)) \mid \frac{d}{dy}g_i^{-1}(y) \mid$ for $y \in S_Y-T_0$ and $0$ o.w.

<!-- S_i와 T_i에서 일대일 대응이 되게 S_X를 나눈 후 해당 영역에서 transformation 하고 전부 더한다 -->


#### Example
$y = x^2$

$g_1: \sqrt{y} \rightarrow y$

$g_2: -\sqrt{y} \rightarrow y$

$f_Y(y) = f_X(g_1^{-1}(y)) \cdot \mid J_1 \mid + f_X(g_2^{-1}(y)) \cdot \mid J_2 \mid$

$f_Y(y) = f_X(\sqrt{y}) \cdot \frac{1}{2 \sqrt{y}} + f_X(-\sqrt{y}) \cdot \frac{1}{2 \sqrt{y}}$

* $g_1(g_1^{-1}(y)) = y, g_2(g_2^{-1}(y)) = y$

<!-- 그래프를 잘 만들어서 진행해야하고, 3차함수, 4차함수에 대해서도 y축에서 수평으로 그려서 점이 몇개 나오는지 확인해서 g1, g2, g3, g4 등을 구역을 나누어 진행해야 할 것 -->


## Expectation of a Random Variable

### Definition of Expectation
$X$: continuous

If $X$ has a pdf $f(x)$ and $\int_{-\infty}^{\infty} \mid x \mid f(x) dx < \infty, E(X) = \int_{-\infty}^{\infty} xf(x)dx$

### Expectation of $Y = g(X)$
$X$: continuous with pdf $f(x)$

If $\int_{-\infty}^{\infty} \mid g(x) \mid f(x)dx < \infty, E(Y) = \int_{-\infty}^{\infty} g(x)f(x)dx$

* If $g(x) = y, E(Y) = \int_{-\infty}^{\infty} g(x)f_X(x)dx = \int_{-\infty}^{\infty} y f_Y(y)dy$

### Properties of Expectation
* $E(aX + b) = aE(X) + b$
* $E(k_1 g_1 (X) + k_2 g_2 (X)) = k_1 E(g_1 (X)) + k_2E(g_2 (X))$
* $g(x) \geq 0 \Rightarrow E(g(X)) \geq 0$


#### Example
Divide, at random, a horizontal line segment of length 5 into two parts.
If $X$ and $Y$ are the length of the left-hand and right-hand parts respectively, calculate $E(X), E(Y), E(XY)$ as well as reasonable pdfs of $X$ and $Y$.

* $Y = 5 - X$
* $F_X(x) = 0$ if $x < 0, x/5$ if $0 \leq x < 5, 1$ if $x \geq 5$
* $f_X(x) = 1/5$ if $0 < x < 5, 0$ o.w.
* $E(X) = \int_0^5 x f_X(x) dx$
* $E(Y) = \int_0^5 (5 - x) f_X(x) dx$
* $E(XY) = \int_0^5 x(5 - x) f_X(x) dx$


### Mean, Variance, Standard deviation

#### Definition
* Mean: $\mu = E(X)$
* Variance: $\sigma^2 = Var(X) = E[(X-\mu)^2]$
* Standard deviation: $\sigma = sd(X) = \sqrt{\sigma^2}$
* Skewness: $E[(X-\mu)^3 / \sigma^3]$
* Kurtosis: $E[(X-\mu)^4 / \sigma^4]$

#### Properties
* $Var(X) \geq 0$
* $Var(X) = E(X^2) - E(X)^2$

  $(\because) Var(X) = E(X - \mu)^2 = E(X^2 -2 \mu X + \mu^2) = E(X^2) - E(X)^2$
* $Var(aX+b) = a^2 Var(X)$

#### Example
When $X$ have the following pdf, find the mean and the variance,
$f(x) = \frac{1}{2} (x + 1) \cdot I_{(-1, 1)}(x)$.

* $I_A(x) = 1$ if $x \in A, 0$ o.w.
* $E(X) = \int_{-1}^1 x \cdot \frac{1}{2} (x + 1) dx$


#### Example
Example where the mean of $X$ does not exist:
$f(x) = \frac{1}{x^2} \cdot I_{(1, \infty)}(x)$

* $E(X) = \int_1^{\infty} x \cdot \frac{1}{x^2} dx = \int_1^{\infty} \frac{1}{x} dx = \lim_{b \rightarrow \infty} [\log x]^b_1 = \infty$

<!-- 기댓값이 존재하지 않는 경우가 이렇게 있다 -->

### Moment generating function (mgf)
#### $r$ th moment: $E(X^r)$
#### Definition of mgf of $X$
Let $X$ be a continuous r.v. such that $E(e^{tX}) < \infty$ for some $h > 0$ and $-h < t < h$.
Then, mgf of $X: M_X(t) = E(e^{tX}) = \int_{-\infty}^{\infty} e^{tx} f(x) dx$

### Relationship between mgf and moment $(E(X^m))$
$E(X^r) = M_X^{(r)}(0), M_X(t) = \sum_{r=0}^{\infty} \frac{E(X^r)}{r!} t^r, \mid t \mid < h$ for some $h > 0$

where $M_X(t)$ exists.

### Uniqueness of mgf
$M_X(t) = M_Y(t)$ for some $h > 0$ and $-h < t < h$

$\Leftrightarrow F_X(z) = F_Y(z) \forall z \in R$

<!-- F_X = F_Y라고 해서 f_X = f_Y가 성립하지는 않는다. 어떤 한 점에서 확률이 다를 경우 존재하기 때문에 -->


#### Example
$M(t) = e^{t^2/2}, -\infty < t < \infty$.

Standard normal distribution's mgf


#### Example
Let $X$ have the pdf $f_X(x) = e^{-x} I_{(0, \infty)}(x)$.
Then, find the mgf of $X$.

$M_X(t) = E(e^{tX}) = \int_0^{\infty} e^{tx} \cdot e^{-x} dx = \int_0^{\infty} e^{(t - 1)x} dx$
* If $t > 1: \int_0^{\infty} e^{(t - 1)x} dx = [\frac{1}{t - 1} e^{(t - 1)x}]^{\infty}_0 = \infty$
* If $t = 1: 1$
* If $t < 1: \int_0^{\infty} e^{-(t - 1)x} dx = [-\frac{e^{-x(1 - t)}}{1 - t}]_0^{\infty} = \frac{1}{1 - t}$

<!-- 이런 경우에 mgf는 1보다 작은 수에 대해서 1/(1-t) 존재하고, -h < t < h인 영역에서 정의하므로, h가 1보다 작거나 같아야 한 -->


### Remarks
For both the discrete and continuous cases, note that if the $r$th moment $E(X^r)$ exists and is finite, then the same is true of all lower order moments, $E(X^k), k = 1, 2, \ldots, r-1$.
However, the converse is not true.
Moreover, if $E(e^{tX})$ exists and is finite for $-h < t < h$, then all moments exist and are finite, but the converse is not necessarily true.

<!-- log normal distribution같은 경우에는 moment는 다 존재함. 그러나 mgf가 존재하지 않는다. solution 참고 -->


#### Example
Let $X$ have the pdf $f_X(x) = e^{-x-1} I_{(-1, \infty)}(x)$.
Then, find the mgf of $X, E(X), var(X)$.

$M_X(t) = E(e^{tX}) = \int_{-1}^{\infty} e^{tx} \cdot e^{-(x + 1)} dx = \int_{-1}^{\infty} e^{x(t - 1) - 1} dx$
* If $t < 1: M_X(t) = [\frac{-e^{x(t - 1) - 1}}{1 - t}]_1^{\infty} = \frac{e^t}{1 - t}$

$E(X) = M_X^{(1)}(0), Var(X) = M_X^{(2)}(0) - (M_X^{(1)}(0))^2$


## Several distribution

### Uniform distribution
Let the random variable $X$ denote the outcome when a point is selected at random from $[a, b], -\infty < a < b < \infty$.
The probability that the point is selected from the interval $[a, x]$ is

$F(x) = 0$ if $x < a$, $\frac{x-a}{b-a}$ if $a \leq x < b$, 1 if $b \leq x$

and its pdf is $f_X(x) = \frac{1}{b-a} I_{(a,b)} (x)$.
The random variable $X$ has a uniform distribution and denoted as $U(a, b)$.

#### Mean, Variance, mgf
Let $X$ follow $U(a, b)$.
* $E(X) = \frac{a+b}{2}$
* $Var(X) = \frac{(b-a)^2}{12}$
* $M_X(t) = \frac{e^{tb} - e^{ta}}{t(b-a)}$ if $t \neq 0, 1$ if $t = 0$


### Exponential distribution
We are interested in the waiting times between successive changes.  <!-- 포아송분포가 기간동안 event가 몇번 일어났는지에 대한 분포라면, 지수분포는 event가 일어날 때까지 걸린 시간에 대한 분포 -->
We assume that the number of changes follows the Poisson $(\lambda)$.
If we let $W$ be the waiting time until the first change, the pdf of $W$ is $f_W(x) = \lambda e^{-\lambda x}$.

If we let $\lambda = \frac{1}{\theta}, f_W(x) = \frac{1}{\theta} e^{-x/\theta} I_{(0, \infty)} (x)$ and it is called exponential distribution, 
$W \sim Exp(\theta), \theta > 0$
<!-- 지수분포의 평균 파라미터는 포아송 분포의 역수다! 기억하기 -->

#### Expansion
$X \sim Poisson(\lambda)$

$f_W(w) = \frac{d}{dw}F_W(w)$

$X_W \sim Poisson(\lambda w)$  <!-- w라는 시간단위에서 event 일어나는 수 -->

$F_W(w) = P(W \leq w) = P(X_w \geq 1) = 1 - P(X_w = 0) = 1 - e^{-\lambda w}$

$f_W(w) = \lambda e^{-lambda w}$


#### Mean, Variance, mgf
Let $X \sim Exp(\theta), \theta > 0$. Then,
* $E(X) = \theta$
  * $E(X) = \int_0^{\infty} x \frac{1}{\theta} e^{-\frac{x}{\theta}} dx = [-x e^{-\frac{x}{\theta}} - \theta e^{-\frac{x}{\theta}}]_0^{\infty} = \theta$
* $Var(X) = \theta^2$
* $M_X(t) = \frac{1}{1 - \theta t}, t < \frac{1}{\theta}$


#### Example
Customers arrive in a certain shop according to an approximate Possion process at a mean rate of 20 per hour.
What is the probability that the shopkeeper will have to wait more than 5 minutes for the arrival of the first customer?


#### Example (no memory property)
Suppose that a certain type of electronic component has an exponential distribution with a mean life of 500 hours.
If $X$ denotes the life of this comonent (or the time to failure of this component), then calculate $P(X > 900 \mid X > 300)$.




### Gamma distribution
We are interested in the waiting times until the $\alpha$ changes occurs.
We assume that the number of chnages follows the Possion ($\lambda$).
If we let $W$ be the waiting time, the pdf of $W$ is 
$F_W(x) = P(W \leq w) = P(\alpha$ or more than $\alpha$ changes occur in $[0, w])$
= $1 - P$(fewer than $\alpha$ changes occur in $[0, w])$
= $1 - \sum_{k=0}^{\alpha - 1} \frac{(\lambda w)^k e^{-\lambda w}}{k!}$

$f_X(x) = \lambda e^{-\lambda x} - e^{-\lambda x} \sum_{k=1}^{\alpha -1}[\frac{(\lambda x)^{k-1} \lambda}{(k-1)!} - \frac{(\lambda x)^k \lambda}{k!}]$ 
$= \frac{\lambda (\lambda x)^{\alpha -1} e^{-\lambda x}}{(\alpha -1)!}$
$= \frac{(x)^{\alpha -1} e^{-x/\beta}}{(\alpha -1)! \beta^{\alpha}}$

If $w < 0$, then $F_X(x) = 0$ and $f_X(x) = 0$.
The random variable $W$ is said to have a gamma distribution with parameters $\alpha, \beta (= 1/\lambda)$

#### Gamma function
$\Gamma (\alpha) = \int_0^{\infty} y^{\alpha -1} e^{-y} dy$ for $\alpha > 0$
* $\Gamma (\alpha +1) = \alpha \Gamma(\alpha)$
* $\Gamma (1) = 1$ and $\Gamma (1/2) = \sqrt{\pi}$
* $\Gamma (n) = (n-1)!$ for $n$: positive integer

#### Definition (gamma distribution)
$X \sim \Gamma (\alpha, \beta)$ for $\alpha, \beta > 0$

$\Leftrightarrow f(x) = \frac{1}{\Gamma (\alpha) \beta^{\alpha}} x^{\alpha -1} e^{-x/\beta}, 0 < x < \infty$

* mgf: $M(t) = (1 - \beta t)^{-\alpha}, t < \frac{1}{\beta}$
* mean: $E(X) = \alpha \beta$
* variance: $Var(X) = \alpha \beta^2$
* Exp($\theta) = \Gamma (1, \theta)$

### Chi-square distribution
A special case of the gamma distribution

#### Definition
$X \sim \chi^2 (r)$ for $r$: positive integer
$\Leftrightarrow X \sim \Gamma(r/2, 2)$
$\Leftrightarrow f(x) = \frac{1}{\Gamma(r/2) 2^{r/2}} x^{r/2 - 1} e^{-x/2}, 0 < x < \infty$

#### Mean, variance, mgf
* Mean: $E(X) = r$
* Variance: $Var(X) = 2r$
* mgf: $M(t) = (1 - 2t)^{-r/2}, 0 < x < \infty$

### Normal distribution
#### Definition
$X \sim N(\mu, \sigma^2), \sigma > 0$
$\Leftrightarrow f(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp{\\{-\frac{1}{2} (\frac{x-\mu}{\sigma})^2 \\}}, -\infty < x < \infty$

#### Mean, Variance, mgf
* mgf: $M_X(t) = \exp(\mu t + \frac{1}{2} \sigma^2 t^2)$
* Mean: $E(X) = \mu$
* Variance: $Var(X) = \sigma^2$
* skewness: $E((X - \mu)^3/\sigma^3) = 0$

  If the absolute of sample skewness is larger than or equal to 2, the distribution is markedly different from a normal distribution in its asymmetry.
  
* kurtosis: $E((X - \mu)^4/\sigma^4) = 0$

  If the absolute of sample kurtosis is larger than or equal to 3, the distribution is markedly different from a normal distribution in its peopensity to produce outliers.

#### Standard normal distribution: $Z \sim N(0, 1)$
$E(Z) = 0$

$Var(Z) = E(Z^2) = 1$

$E(Z^3) = 0$

$E(Z^4) = 3$

cdf: $\phi(z) = \int_{-\infty}^z \frac{1}{\sqrt(2 \pi)} \exp(-\frac{1}{2} x^2) dx$

### Mixed type of random variable
The distribution function that is a combination of those for the discrete and continuous types.

#### Definition of Expectation
$X$: Continuous on $[c_1, d_1], \ldots, [c_n, d_n]$ and pdf is $f_j(x)$ on $x \in (c_j, d_j)$ descrete on $\\{x_1, x_2, \ldots \\}$ and pmf for each point is $p(x_i)$ on $x_i$

Then, $E(X) = \sum_{i=1}^{\infty} x_i p(x_i) + \sum_{j=1}^n \int_{c_j}^{d_j} x f_j(x) dx$


## Important Inequalities
Let $X$ a r.v. and $m$ be a positive integer and suppose $E[X^m] < \infty$.

$k$: integer and $k \leq m \Rightarrow E[X^k] < \infty$

### Markov's inequality
Let $u(x) \geq 0$ as a function of r.v. $X$, $P[u(X) \geq c] \leq \frac{E[u(X)]}{c}$ for $\forall c > 0$

### Chebyshev's inequality
Let $\mu = E(X)$, and $\sigma^2 = Var(X) < \infty$, $P[|X - \mu| \geq k] \leq \frac{\sigma^2}{k^2}$ for $\forall k > 0$

#### Example
For any random variable $X$ with finite variance, $P[|X - \mu| \geq k \sigma] \leq 1 - \frac{1}{k^2}$.

* $k = 2$: At most 75% of the data will be within $\pm2$ standard deviations of the mean
* $k = 3$: At most 88.9% of the data will be within $\pm3$ standard deviations of the mean
* $k = 4$: At most 93.75% of the data will be within $\pm4$ standard deviations of the mean

### Covex function
A function $\phi$ on $(a, b), -\infty \leq a < b \leq \infty$ is said to be a convex function if $\phi [\gamma x + (1 - \gamma) y] \leq \gamma \phi (x) + (1 - \gamma) \phi (y)$ for all $x, y \in (a, b)$ and all $0 < \gamma < 1$

If the above inequality is strict, strictly convex.

(Converse: concave function)

### Jensen's inequality
If $\phi$ is convex on an open interval $I$, $\phi (E(X)) \leq E(\phi (X))$

### Liapounov's inequality
For $0 < r < s, E|X|^s < \infty \Rightarrow (E|X|^r)^{1/r} \leq (E|X|^s)^{1/s}$


