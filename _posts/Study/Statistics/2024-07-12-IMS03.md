---
title: Continuous Random variables
author: JSH
date: 2024-07-12 10:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, IMS]
use_math: true
---

# Continuous Random Variables

## Continuous Distribution
If the measurements could come from an interval of possible outcomes, we call them data from a continuous-type population or continuous-type data

### Histogram
A histogram is a graphical display of tabular frequencies, shown as adjacent rectangles.
We assume that a simple of $n$ observations, $x_1, \ldots, x_n$, from a distribution are observed.
If we have cutpoints for class intervals: $(c_0, c_1), (c_1, c_2), (c_2, c_3), \ldots, (c_{k-1}, c_k)$, then each rectangle has an area equal to the relative frequency $\frac{f_i}{n}$ of the observations for the class.
That is, the function is defined by $h(x) = \frac{f_i}{n(c_i - c_{i-1}}$, for $c_i < x < c_i, i = 1, \ldots, k$.

With a sample size of $n \geq 30$, the histogram is an adequate estimate of the distribution $p(y)$ that produced your data.
Larger $n$ provides better accuracy.

### Empirical rule
We assume that a sample of $n$ observations, $x_1, \ldots, x_n$ from a distribution are observed.

If we let $\overline{x}$ and $s$ be a sample mean and a sample standard deviation respectively, and the histogram of these datais bell shaped, then
* Approximately 68% of the data are in the interval $(\overline{x} - s, \overline{x} + s)$
* Approximately 95% of the data are in the interval $(\overline{x} - 2s, \overline{x} + 2s)$
* Approximately 99.7% of the data are in the interval $(\overline{x} - 3s, \overline{x} + 3s)$

### Stem-and-leaf display
If the leaves are carefully aligned vertically, the table has the same effect as a histogram, but the original numbers are not lost

### Order statistics
For a sample of $n$ observations $(x_1, \ldots, x_n)$, when the observations are ordered from small to large, the resulting ordered data are called the order statistics.
The smallest observation is called the first order statistics and the $i$th smallest observation is called the $i$th order statistics.

### (100$p$)th sample percentile (= sample quantile of order $p$)
If $(n+1)p$ is not an integer but is equal to $r$ plus some proper fraction, $a/b$, the (100$p$)th sample percentile is $y_r + (a/b)(y_{r+1} - y_r) = (1 - a/b)y_r + (a/b)y_{r+1}$ where $n$ is the number of sample and $y_r$ is the $r$th order statistics.

With a sample size $n \geq 30$, the quantiles are adquate estimates, making qq plot reasonably trustworthy.
Larger $n$ provides better accuracy.

### If there are $n$ observations and $y_r$ indicates the $r$th order statistics,
* Midrange: $0.5(y_1 + y_n)$
* Trimean: 0.25(1st quartile) + 0.5(2nd quartile) + 0.25(3rd quartile)
* Range: $y_n - y_1$
* Interquartile range (IQR): (3rd quartile) - (1st quartile)

### (100$p$)th percentiles of a population (= a population quantile  of order $p$)
(100$p$)th percentile is a number $\pi_p$ such that the area under $f(x)$ to the left of $\pi_p$ is $p$.
The $r$th order statistic, $y_r$, from $n$ measurements is the quantile of order $r/(n+1)$ as well as the $100r/(n+1)$th percentile.

### Quantile-quantile plot (QQ plot)
We assume that there are $n$ measurements from some distribution.
If we let the $r$th order statistics by $y_r$ and $p = r/(n+1)$, the plot of $(\pi_p, y_r)$ for several values of $r$ is called the q-q plot

#### Interpreting quantile-quantile plot
* Compare the data to the straight line. The closer they are to the line, the more they appear as if produced by the assumed model
* Data points higher than the line in the upper right suggest that the pdf that produced the data tends to give more extreme values in the upper tail than does the assumed distribution
* Data points lower than the line in the upper right suggest that the pdf that produced the data tends to give less extreme values in the upper tail than does the assumed distribution
* Pronounced horizontal lines are evidence of discreteness


## Properties of pdfs
* $0 \leq f_X(x), -\infty < x < \infty$
* $\int_{-\infty}^{\infty} f_X(x) dx = 1$
* $P(a < X \leq b) = \int_a^b f_X(x) dx$
* $P(a < X \leq b) = P(a < X < b) = P(a \leq X < b) = P(a \leq X \leq b)$


## Transformations

### Support
$S_X$: support of $X \Leftrightarrow S_X = \\{x: f_X(x) > 0 \\}, f_X(x)$: pdf of $X$

$S_Y$: support of $Y \Leftrightarrow S_Y = \\{x: f_Y(y) > 0 \\}, f_Y(y)$: pdf of $Y$

### Transformation: method 1
Method 1 to obtain pdf of $Y = g(X)$: cdf technique

$f_Y(y) = \frac{d}{dy} F_Y(y), F_Y(y) = P(Y \leq y) = P(g(X) \leq y)$

### Jacobian ($J$)
For $y = g(x), J = \frac{dx}{dy} = \frac{d}{dy}g^{-1}(y)$

### Transformation: method 2
Method 2 to obtain pdf of $Y = g(X)$: transformation

Suppose that $f_X(x)$: pdf of $X$ and $g(x)$: differentiable function

#### When $g(x)$ is a one-to-one differentiable function on $S_X$
$f_Y(y) = \begin{cases} 1, & 0 < x <1 \\ 0,  & \text{o.w.} \end{cases}$

and $S_Y = \\{y: y = g(x), x \in S_X \\}$

#### Transformation in general cases: $g(x)$ may be a $r$-to-1
Suppose:
* $\\{S_0, S_1, \ldots, S_r \\}$: partition of $S_X, S_Y = \cup_{i=0}^r T_i$ where $P(X \in S_0) = 0$ and $T_0 = \\{y: y = g(x), x \in S_0 \\}$
* $g_i(i = 1, \ldots, r)$ is a one-to-one function from $S_i$ to $T_i$
* For $x \in S_i, g_i(x) = g(x)$ and $g_i(x)$: differentiable on $S_i$, then, $f_Y(y) = \begin{cases} \sum_{y \in T_i} f_X(g_i^{-1}(y)) |\frac{d}{dy}g_i^{-1}(y)|, & \text{for } y \in S_Y-T_0 \\ 0,  & \text{o.w.} \end{cases}$


## Expectation of a Random Variable

### Definition of Expectation
$X$: continuous

If $X$ has a pdf $f(x)$ and $\int_{-\infty}^{\infty} |x| f(x) dx < \infty, E(X) = \int_{-\infty}^{\infty} xf(x)dx$

### Expectation of $Y = g(X)$
$X$: continuous with pdf $f(x)$

If $\int_{-\infty}^{\infty} |g(x)|f(x)dx < \infty, E(Y) = \int_{-\infty}^{\infty} g(x)f(x)dx$

### Properties of Expectation
* $E(aX + b) = aE(X) + b$
* $E(k_1 g_1 (X) + k_2 g_2 (X)) = k_1 E(g_1 (X)) + k_2E(g_2 (X))
* $g(x) \geq 0 \Rightarrow E(g(X)) \geq 0$

### Mean, Variance, Standard deviation

#### Definition
* Mean: $\mu = E(X)$
* Variance: $\sigma^2 = Var(X) = E[(X-\mu)^2]$
* Standard deviation: $\sigma = sd(X) = \sqrt{\sigma^2}$
* Skewness: $E[(X-\mu)^3 / \sigma^3]$
* Kurtosis: $E[(X-\mu)^4 / \sigma^4]$

#### Properties
* $Var(X) \geq 0$
* $Var(X) = E(X^2) - E(X)^2$

  $(\because) Var(X) = E(X - \mu)^2 = E(X^2 -2 \mu X + \mu^2) = E(X^2) - E(X)^2$
* $Var(aX+b) = a^2 Var(X)$


### Moment generating function (mgf)
#### $r$th moment: $E(X^r)$
#### Definition of mgf of $X$
Let $X$ be a continuous r.v. such that $E(e^{tX}) < \infty$ for some $h > 0$ and $-h < t < h$.
Then, mgf of $X: M_X(t) = E(e^{tX}) = \int_{-\infty}^{\infty} e^{tx} f(x) dx$

### Relationship between mgf and moment $(E(X^m))$
$E(X^r) = M_X^{(r)}(0), M_X(t) = \sum_{r=0}^{\infty} \frac{E(X^r)}{r!} t^r, |t| < h$ for some $h > 0$

where $M_X(t)$ exists.

### Uniqueness of mgf
$M_X(t) = M_Y(t)$ for some $h > 0$ and $-h < t < h$

$\Leftrightarrow F_X(z) = F_Y(z) \forall z \in R$

### Remarks
For both the discrete and continuous cases, note that if the $r$th moment $E(X^r)$ exists and is finite, then the same is true of all lower order moments, $E(X^k), k = 1, 2, \ldots, r-1$.
However, the converse is not true.
Moreover, if $E(e^{tX})$ exists and is finite for $-h < t < h$, then all moments exist and are finite, but the converse is not necessarily true.


## Several distribution

### Uniform distribution

#### Uniform distribution
Let the random variable $X$ denote the outcome when a point is selected at random from $[a, b], -\infty < a < b < \infty$.
The probability that the point is selected from the interval $[a, x]$ is

$F(x) = \begin{cases} 0, & x < a \\ \frac{x-a}{b-a},  & a \leq x < b \\ 1, & b \leq x \end{cases}$

and its pdf is $f_X(x) = \frac{1}{b-a} I_{(a,b)} (x)$.
The random variable $X$ has a uniform distribution and denoted as $U(a, b)$.

#### Mean, Variance, mgf
Let $X$ follow $U(a, b)$.
* $E(X) = \frac{a+b}{2}$
* $Var(X) = \frac{(b-a)^2}{12}$
* $M_X(t) = \begin{cases} \frac{e^{tb} - e^{ta}}{t(b-a)}, & t \neq 0 \\ 1,  & t = 0 \end{cases}$

### Exponential distribution

#### Exponential distribution
We are interested in the waiting times between successive changes.
We assume that the number of changes follows the Poisson $(\lambda)$.
If we let $W$ be the waiting time until the first change, the pdf of $W$ is $f_W(x) = \lambda e^{-\lambda x}$.

If we let $\lambda = \frac{1}{\theta}, f_W(x) = \frac{1}{\theta} e^{-x/\theta} I_{(0, \infty)} (x)$ and it is called exponential distribution, 
$W \sim Exp(\theta), \theta > 0$

#### Mean, Variance, mgf
Let $X \sim Exp(\theta), \theta > 0$. Then,
* $E(X) = \theta$
* $Var(X) = \theta^2$
* $M_X(t) = \frac{1}{1 - \theta t}, t < \frac{1}{\theta}$

### Gamma distribution
We are interested in the waiting times until the $\alpha$ changes occurs.
We assume that the number of chnages follows the Possion ($\lambda$).
If we let $W$ be the waiting time, the pdf of $W$ is 
$F_W(x) = P(W \leq w) = P(\alpha$ or more than $\alpha$ changes occur in $[0, w])$
= $1 - P$(fewer than $\alpha$ changes occur in $[0, w])$
= $1 - \sum_{k=0}^{\alpha - 1} \frac{(\lambda w)^k e^{-\lambda w}}{k!}$

$f_X(x) = \lambda e^{-\lambda x} - e^{-\lambda x} \sum_{k=1}^{\alpha -1}[\frac{(\lambda x)^{k-1} \lambda}{(k-1)!} - \frac{(\lambda x)^k \lambda}{k!}]$ 
$= \frac{\lambda (\lambda x)^{\alpha -1} e^{-\lambda x}}{(\alpha -1)!}$
$= \frac{(x)^{\alpha -1} e^{-x/\beta}}{(\alpha -1)! \beta^{\alpha}}$

If $w < 0$, then $F_X(x) = 0$ and $f_X(x) = 0$.
The random variable $W$ is said to have a gamma distribution with parameters $\alpha, \beta (= 1/\lambda)$

#### Gamma function
$\Gamma (\alpha) = \int_0^{\infty} y^{\alpha -1} e^{-y} dy$ for $\alpha > 0$
* $\Gamma (\alpha +1) = \alpha \Gamma(\alpha)$
* $\Gamma (1) = 1$ and $\Gamma (1/2) = \sqrt{\pi}$
* $\Gamma (n) = (n-1)!$ for $n$: positive integer

#### Definition (gamma distribution)
$X \sim \Gamma (\alpha, \beta)$ for $\alpha, \beta > 0$

$\Leftrightarrow f(x) = \frac{1}{\Gamma (\alpha) \beta^{\alpha}} x^{\alpha -1} e^{-x/\beta}, 0 < x < \infty$

* mgf: $M(t) = (1 - \beta t)^{-\alpha}, t < \frac{1}{\beta}$
* mean: $E(X) = \alpha \beta$
* variance: $Var(X) = \alpha \beta^2$
* Exp($\theta) = \Gamma (1, \theta)$

### Chi-square distribution
A special case of the gamma distribution

#### Definition
$X \sim \chi^2 (r)$ for $r$: positive integer
$\Leftrightarrow X \sim \Gamma(r/2, 2)$
$\Leftrightarrow f(x) = \frac{1}{\Gamma(r/2) 2^{r/2}} x^{r/2 - 1} e^{-x/2}, 0 < x < \infty$

#### Mean, variance, mgf
* Mean: $E(X) = r$
* Variance: $Var(X) = 2r$
* mgf: $M(t) = (1 - 2t)^{-r/2}, 0 < x < \infty$

### Normal distribution
#### Definition
$X \sim N(\mu, \sigma^2), \sigma > 0$
$\Leftrightarrow f(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp{\\{-\frac{1}{2} (\frac{x-\mu}{\sigma})^2 \\}}, -\infty < x < \infty$

#### Mean, Variance, mgf
* mgf: $M_X(t) = \exp(\mu t + \frac{1}{2} \sigma^2 t^2)$
* Mean: $E(X) = \mu$
* Variance: $Var(X) = \sigma^2$
* skewness: $E((X - \mu)^3/\sigma^3) = 0$

  If the absolute of sample skewness is larger than or equal to 2, the distribution is markedly different from a normal distribution in its asymmetry.
  
* kurtosis: $E((X - \mu)^4/\sigma^4) = 0$

  If the absolute of sample kurtosis is larger than or equal to 3, the distribution is markedly different from a normal distribution in its peopensity to produce outliers.

#### Standard normal distribution: $Z \sim N(0, 1)$
$E(Z) = 0$

$Var(Z) = E(Z^2) = 1$

$E(Z^3) = 0$

$E(Z^4) = 3$

cdf: $\phi(z) = \int_{-\infty}^z \frac{1}{\sqrt(2 \pi)} \exp(-\frac{1}{2} x^2) dx$

