---
title: Variable Selection
author: JSH
date: 2025-05-21 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Variable Selection

## Goal
To explain the response with the smallest number of explanatory variables.
<!-- 중요한 교란변수의 경우에는 variable selection에서 빼서 포함시키기. 이외에는 유의성이 떨어지는 변수는 빼서 모델 만들기 -->

The goal of regression analysis is to model the complex interactions between independent and dependent variables using a regression equation.
Therefore, to build a precise regression model, as many independent variables as possible should be included. (Goodness of fit)

However, the model should also be easy to understand and interpret. (principle of parsimony or simplicity of model perspective)

These two perspectives should be well balanced.
Balancing between "goodness of fit" and "simplicity"

### Full model
Model with all possible explanatory variables, (particularly when $q$ is large) adopted after checking the model assumptions:
$y_i = \beta_0 + \beta-1 x_{i1} + \ldots + \beta_p x_{ip} + \ldots + \beta_q x_{iq} + \epsilon_i, \epsilon_i \sim iid N(0, \sigma^2) \rightarrow \hat{\beta_j*}, \hat{y_i*}$

### Current model (or subset model)
$y_i = \beta_0 + \beta-1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i'$ where $p < q \rightarrow \hat{\beta_j}, \hat{y_i}$

### Consequences of Variables Deletion
$Var(\hat{\beta_j*}) \geq Var(\hat{\beta_j})$ and $Var(\hat{y_i*} \leq Var(\hat{y_j})$

The accuracy of estimates for regression coefficients and response variables can be increased through appropriate variable selection.

## Statistics used in Variable Selection
To decide that one subset is better than another, we need some criteria for subset selection.

### Adjusted multiple correlation coefficient
For fixed $p$, maximize $1-SSE_p/SST (= R^2)$ (or minimize $SSE_p$) among possible choices of $p$ variables.

For different $p$, maximize $1 - \frac{SSE_p / (n-p-1)}{SST/(n-1)} (=R^2_a)$ (or minimize $SSE_p/(n-p-1) = \hat{\sigma_p^2} (=RMS_p))$

* Note: $\hat{\sigma_p^2} = (n - p - 1)^{-1} SSE_p$ (Residual Mean Square: $RMS_p$)

### Mallow's $C_p$
Minimize $\mid C_p - (p+1) \mid$ where $C_p = \frac{SSE_p}{\hat{\sigma}^2} + 2(p+1) - n$ and $\hat{\sigma}^2 = SSE/(n-q-1)$

If $q = p, \mid C_p - (p+1) \mid = 0$

<!-- Mallow's C_p에서 sigma는 full model에서 나옴. -->

#### Expansion
$SSE_p = \sum (y_i - \hat{y_i})^2 = \sum (y_i - E(y_i) + E(y_i) - \hat{y_i})^2 = \sum [(y_i - E(y_i))^2 + (\hat{y_i} - E(y_i))^2 + 2(y_i - E(y_i))(E(y_i) - \hat{y_i})] = \sum [(y_i - E(y_i))^2 + (\hat{y_i} - E(y_i))^2$

$= (n-p) \frac{\sigma^2 + \sigma^2*}{\sigma^2} + 2 (p + 1) - n$

If $\sigma^2* \approx 0, then SSE_p \approx p+1$

<!-- 따라서 E(y_i)와 \hat{y_i}가 가깝게 하는게 목적 -->

### AIC (Akaike information criteria)
<!-- train/test 개념으로 i번째 데이터를 빼고 모형을 만든 후 i번째 데이터로 test -->

Minimize $AIC_p = n \ln (SSE_p / n) + 2(p+1)$

Calculating $\sum (y_i - \hat{y_{i(-i)}})^2$

### BIC (Schwarz Bayes information criteria)
Minimize $BIC_p = n \ln (SSE_p / n) + (p+1) \ln n$

BIC is a modified criterion for overfitting the tendency of choosing a $p$ with a larger AIC.

### Partial F-test statistics for testing
$H_0: \beta_p = 0 \mid \beta_0, \beta_1, \ldots, \beta_{p-1}$ vs $H_1: \beta_p \neq 0 \mid \beta_0, \beta_1, \ldots, \beta_{p-1}$

#### Test statistic
$F[p \mid 0, 1, \ldots, p-1] = \frac{SSE(\beta_0, \ldots, \beta_{p-1}) - SSE(\beta_0, \ldots, \beta_{p-1}, \beta_p)}{SSE(\beta_0, \ldots, \beta_{p-1}, \beta_p) / (n-p-1)}$

$F[p \mid 0, 1, \ldots, p-1] \sim F(1, n-p-1)$ if $\beta_p = 0$ under the current model: $y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i', \epsilon_i' \sim iid N(0, \sigma^2)$

#### Remark
$F[p \mid 0, 1, \ldots, p-1] = (t[p \mid 0, 1, \ldots, p-1])^2$ where $t[p \mid 0, 1, \ldots, p-1] = \frac{\hat{\beta_p}}{s.e.(\beta_p)} \sim t(n-p-1)$

$\hat{\beta_p}$: esimate of $\beta_p$ under the current model.

## Variable Selection

### Evaluating all possible equations
* $R^2_a$: evaluate all possible $R^2_a$'s, and choose a model with the smallest $R^2_a$
* $C_p$: evaluate all possible $C_p$'s, and choose a model with the smallest $\mid C_p - (p+1) \mid$ for $p < q$
* AIC or BIC: evaluate all possible AICs and BICs, and choose a model with the smallest value

### Variable selection procedures (Partial F-test)

#### Forward selection (FS)
Select (1) if $R^2_{(1)} = \max_{1 \leq i \leq q} R^2_i$ where $R^2_i = SST-SSE(0, i)$ and $F[(1) \mid 0] \geq F(1, n-2;\alpha)$.
Otherwise, stop at $x_0 = 1$.

Once (1), (2), ..., (p-1) have been selected add (p) if $F[(p) \mid 0, (1), \ldots, (p-1)] = \max_{1 \leq j \leq q, j \neq (1), \ldots, (p-1)} F[j \mid 0, (1), \ldots, (p-1)]$
and $F[j \mid 0, (1), \ldots, (p-1)] \geq F(0, n-p-1; \alpha)$

<!-- 즉, p-value가 작으면 선택 -->

Otherwise, stop at (1), ..., (p-1)

* Summary
  * Each step identifies important variables and checks the significance of their contribution
  * A maximum of $q + (q-1) + \ldots + 2 + 1 = q(q+1)/2$ regression equations are applied
  * Once a variable is included in a previous step, it cannot be removed in subsequent steps, regardless of which variables are selected later.
  * Therefore, the model selected through forward selection is not guaranteed to be optimal.
  * If the significance level $\alpha$ is too small, important variables may not be selected. (typically $\alpha > 0.05$)

#### Backward elimination
Start with all variables.

Eliminate $(j*)$ if $F[(j*) \mid 0, (1), \ldots, (j*-1), (j*+1), \ldots, (p)] = \min_{1 \leq j \leq p} F[(j) \mid 0, (1), \ldots, (j-1), (j+1), \ldots, (p)]$
and $F[(j*) \mid 0, (1), \ldots, (j*-1), (j*+1), \ldots, (p)] < F(1, n-p-1; \alpha)$

<!-- p-value가 크면 선택 -->

Otherwise, stop at (1), ..., (p)

* Summary
  * At each step, insignificant variables are identified and their significance is checked.
  * A maximum of $q + (q-1) + \ldots + 2 + 1 = q(q+1)/2$ regression equations are applied
  * Once a variable is excluded in a previous step, it cannot be added back in later steps, regardless of future selections.
  * Therefore, the model selected through backward elimination is not guaranteed to be optimal.
  * If the significance level $\alpha$ is too small, variables may be removed too easily (SAS default: $\alpha = 0.1$)

#### Stepwise selection
Same as Forward Selection except that after including (p) by FS, then eliminate (j) ($j = 1, \ldots, p-1)$ if $F[(j) \mid 0, (1), \ldots, (j-1), (j+1), \ldots, (p)] < F(1, n-p-1; \alpha)$

* Summary
  * An improved version of Forward Selection (FS); when a new variable is added, it checks whether previously added variables remain significant.
  * That is, at each stage, it verifies whether the previously selected variables still retain their significance after new variables are introduced.
  * Requires fewer regression equations compared to the “All possible equations” method, making it more efficient.
