---
title: Variable Selection
author: JSH
date: 2025-05-21 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Variable Selection

## Goal
To explain the response with the smallest number of explanatory variables.
<!-- 중요한 교란변수의 경우에는 variable selection에서 빼서 포함시키기. 이외에는 유의성이 떨어지는 변수는 빼서 모델 만들기 -->

The goal of regression analysis is to model the complex interactions between independent and dependent variables using a regression equation.
Therefore, to build a precise regression model, as many independent variables as possible should be included. (Goodness of fit)

However, the model should also be easy to understand and interpret. (principle of parsimony or simplicity of model perspective)

These two perspectives should be well balanced.
Balancing between "goodness of fit" and "simplicity"

### Full model
Model with all possible explanatory variables, (particularly when $q$ is large) adopted after checking the model assumptions:
$y_i = \beta_0 + \beta-1 x_{i1} + \ldots + \beta_p x_{ip} + \ldots + \beta_q x_{iq} + \epsilon_i, \epsilon_i \sim iid N(0, \sigma^2) \rightarrow \hat{\beta_j*}, \hat{y_i*}$

### Current model (or subset model)
$y_i = \beta_0 + \beta-1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i'$ where $p < q \rightarrow \hat{\beta_j}, \hat{y_i}$

### Consequences of Variables Deletion
$Var(\hat{\beta_j*}) \geq Var(\hat{\beta_j})$ and $Var(\hat{y_i*} \leq Var(\hat{y_j})$

The accuracy of estimates for regression coefficients and response variables can be increased through appropriate variable selection.

## Statistics used in Variable Selection
To decide that one subset is better than another, we need some criteria for subset selection.

### Adjusted multiple correlation coefficient
For fixed $p$, maximize $1-SSE_p/SST (= R^2)$ (or minimize $SSE_p$) among possible choices of $p$ variables.

For different $p$, maximize $1 - \frac{SSE_p / (n-p-1)}{SST/(n-1)} (=R^2_a)$ (or minimize $SSE_p/(n-p-1) = \hat{\sigma_p^2} (=RMS_p))$

* Note: $\hat{\sigma_p^2} = (n - p - 1)^{-1} SSE_p$ (Residual Mean Square: $RMS_p$)

### Mallow's $C_p$
Minimize $\mid C_p - (p+1) \mid$ where $C_p = \frac{SSE_p}{\hat{\sigma}^2} + 2(p+1) - n$ and $\hat{\sigma}^2 = SSE/(n-q-1)$

If $q = p, \mid C_p - (p+1) \mid = 0$

<!-- Mallow's C_p에서 sigma는 full model에서 나옴. -->

#### Expansion
$SSE_p = \sum (y_i - \hat{y_i})^2 = \sum (y_i - E(y_i) + E(y_i) - \hat{y_i})^2 = \sum [(y_i - E(y_i))^2 + (\hat{y_i} - E(y_i))^2 + 2(y_i - E(y_i))(E(y_i) - \hat{y_i})] = \sum [(y_i - E(y_i))^2 + (\hat{y_i} - E(y_i))^2$

$= (n-p) \frac{\sigma^2 + \sigma^2*}{\sigma^2} + 2 (p + 1) - n$

If $\sigma^2* \approx 0, then SSE_p \approx p+1$

<!-- 따라서 E(y_i)와 \hat{y_i}가 가깝게 하는게 목적 -->

### AIC (Akaike information criteria)
<!-- train/test 개념으로 i번째 데이터를 빼고 모형을 만든 후 i번째 데이터로 test -->

Minimize $AIC_p = n \ln (SSE_p / n) + 2(p+1)$

Calculating $\sum (y_i - \hat{y_{i(-i)}})^2$

### BIC (Schwarz Bayes information criteria)
Minimize $BIC_p = n \ln (SSE_p / n) + (p+1) \ln n$

BIC is a modified criterion for overfitting the tendency of choosing a $p$ with a larger AIC.

### Partial F-test statistics for testing
$H_0: \beta_p = 0 \mid \beta_0, \beta_1, \ldots, \beta_{p-1}$ vs $H_1: \beta_p \neq 0 \mid \beta_0, \beta_1, \ldots, \beta_{p-1}$

#### Test statistic
$F[p \mid 0, 1, \ldots, p-1] = \frac{SSE(\beta_0, \ldots, \beta_{p-1}) - SSE(\beta_0, \ldots, \beta_{p-1}, \beta_p)}{SSE(\beta_0, \ldots, \beta_{p-1}, \beta_p) / (n-p-1)}$

$F[p \mid 0, 1, \ldots, p-1] \sim F(1, n-p-1)$ if $\beta_p = 0$ under the current model: $\y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i', \epsilon_i' \sim iid N(0, \sigma^2)$

#### Remark
F[p \mid 0, 1, \ldots, p-1] = (t[p \mid 0, 1, \ldots, p-1])^2$ where t[p \mid 0, 1, \ldots, p-1] = \frac{\hat{\beta_p}}{s.e.(\beta_p)} \sim t(n-p-1)$

$\hat{\beta_p}$: esimate of $\beta_p$ under the current model.

## Variable Selection



