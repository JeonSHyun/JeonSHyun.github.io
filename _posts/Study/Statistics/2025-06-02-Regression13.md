---
title: Logistic Regression
author: JSH
date: 2025-06-02 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Logistic Regression

## Regression Analysis and Categorical Data Analysis

Earlier chapters
* Dependent variable: quantitative
* Independent variable: quantitative or qualitative

This chapter
* Dependent variatle: qualitative
* Independent variable: quantitative or qualitative

## Modeling Qualitative Data
Rather than predicting these two values of the binary response variable, we try to model the probabilities that the response takes one of these two values.

Let $\pi$ denote the probability that $Y=1$ when $X=x$.

If we use the standard linear model, we cannot model probability; $\pi = Pr(Y=1 \mid X=x) = \beta_0 + \beta_1 x + \epsilon$.
* LHS lies between 0 and 1 while RHS is unbounded

### Logistic model
Therefore, in logistic model, $\log \frac{\pi}{1-\pi} = \beta_0 + \beta_1 x$.
Then, $\pi = Pr(Y=1 \mid X=x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}$

$Y \mid X = x \sim B(1, \pi)$.

### Logistic regression function (logistic model for multiple regression)
$\pi = Pr(Y=1 \mid X_1 = x_1, \ldots, X_p = x_p) = \frac{e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}$

Nonlinear in the parameters $\beta_0, \beta_1, \ldots, \beta_p$ but it can be linearized by the logit transformation
* $1 - \pi = Pr(Y=0 \mid X_1 = x_1, \ldots, X_p = x_p) = \frac{1}{1+e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}$
* $\frac{\pi}{1 - \pi} = e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}$
* $\log (\frac{\pi}{1 - \pi}) = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$
  * $\frac{\pi}{1 - \pi}$: odds
  * $\log (\frac{\pi}{1 - \pi})$: logit
  * Relative risk: $\pi_1/\pi_0$
  * Odds Ratio: $\frac{\pi_1}{1-\pi_1} / \frac{\pi_0}{1-\pi_0} = \frac{\pi_1(1-\pi_0)}{\pi_0 (1-\pi_1)}$

<!-- log(x/(1-x))는 logit function이라고 부른다. 확률에다가 logit function을 이용한게 logistic regression -->
<!-- GLM의 special case이다 -->

### Modeling and estimating the logistic regression model
* Maximum likelihood estimation (using an iterative procedure)
* Unlike least squares fitting, no closed-form expression exists for the estimates of the parameters. To fit a logistic regression in practice a computer program is essential.
* Tools, used for the suitability of the model, are not the usual $R^2, t, F$ tests, the ones employed in least squares regression.
* Information criteria such as AIC and BIC can be used for model selection.
* Instead of SSE, the logarithm of the likelihood (log-likelihood) for the fitted model is used. <!-- deviance -->


<!--
각 샘플에 대해서 \pi_x는 서로 다르다. 왜냐하면 \pi_x = E(Y|X=x). 
샘플 별 분포는 Bernoulli(\pi_x)를 따르기 때문에 분산은 각 데이터마다 \pi_x(1-\pi_x)
그러므로 이분산 구조를 가진다
-->

### Interpretation of regression coefficient
$\log \frac{\pi_x}{1 - \pi_x} = \beta_0 + \beta_1 x_1 + \beta_2 x_2$

$\log \frac{\pi_{x+1}}{1 - \pi_{x+1}} = \beta_0 + \beta_1 (x_1 + 1) + \beta_2 x_2$

$\log (\frac{\pi_{x+1}}{1 - \pi_{x+1}})/(\frac{\pi_x}{1 - \pi_x}) = \beta_1$  <!-- x_1이 한 단위 증가했을 때 기준값과의 odds ratio -->

In logistic regression, the odds ratio estimate is obtained by exponentiating the estimated beta coefficient.

For unit increase in $X_2$ with $X_1$ and $X_3$ keeping fixed, the odds ratios of $\frac{Pr(y=1)}{Pr(y=0)}$ is multiplied by $e^{\beta_2}$.

Note that $\frac{\pi}{1-\pi} = e^{\beta_0 + \beta_1 x_1 \ldots + \beta_p x_p}$

#### Interpretation of odds ratio
* $0 \leq OR \leq \infty$
* $OR \approx 1$: x has no significant effect
* $OR \rightarrow 0$: x significantly decreases the relative odds
* $OR \rightarrow \infty$: x significantly increases the relative odds

### Model significance
Global null hypothesis: $H_0: \beta_1 = \beta_2 = \ldots = 0$ vs $H_1$: not $H_0$  <!-- 자유도는 beta 개수! -->

Likelihood Ratio test: $\chi^2(df)$

## Diagnostic in logistic regression
### Diagnostic measures
* $\hat{\pi_i}, i = 1, \ldots, n$
* Residual
  * Pearson's residual (RESCHI in SAS): $PR_i, i = 1, \ldots, n$
  * Standardized deviance residual (RESDEV in SAS): $DR_i$
* Leverage and influential observation
  * Weighted leverage: $p_{ii} *$
  * Cook's distance, $DBETA_i, DFG_i$

### How to use the measures
Same way as the corresponding ones from a linear regression
* scatter plot of $DR_i$ vs. $\hat{\pi_i}$
* scatter plot of $PR_i$ vs. $\hat{\pi_i}$
* index plot of $DR_i, DBETA_i, DFG_i$ and $p_{ii} *$

## Determination of Variables to Retain
Model is significant but none of individual predictors are significant.
Do we need all three variables?
(Also, you can check multicollinearity)

Instead of looking at the reduction in the error sum of squares (SSE), we look at the change in the (log) likelihood for the two fitted models in logistic regression.

With a large number of explanatory variables the side-by-side boxplots provide a quick screening procedure.

### Change in the log likelihood
To see whether the $q$ additional variables are significant, we look
$\triangle G = -2(L(p) - L(p+q))$
* $L(p)$: log likelihood for a model with $p$ variables and constant
* $L(p+q)$: log likelihood for a model with $p+q$ variables and constant
* $\triangle G \sim \chi^2(q)$ under the null $H_0: \beta_{p+1} = \beta_{p+2} = \ldots = \beta_{p+q} = 0$ <!-- 오른쪽 검정 -->
* A large value of the test statistic would call for the retention of the $q$ variables in the model
* The test is valid when $n$ is large

Likelihood: $\pi_1^{y_1} (1-\pi_1)^{1-y_1} \times \ldots \times \pi_p^{y_p} (1 - \pi_p)^{1-y_p}$

### AIC/BIC
The AIC and BIC criteria can be used to judge the suitability of various logistic models
* AIC = -2(log-likelihood of the fitted model) + $2(p+1)$
* BIC = -2(log-likelihood of the fitted model) + $(p+1) \log n$

## Judging the fit of a logistic regression

