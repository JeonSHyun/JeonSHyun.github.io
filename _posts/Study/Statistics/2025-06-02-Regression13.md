---
title: Logistic Regression
author: JSH
date: 2025-06-02 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Logistic Regression

## Regression Analysis and Categorical Data Analysis

Earlier chapters
* Dependent variable: quantitative
* Independent variable: quantitative or qualitative

This chapter
* Dependent variatle: qualitative
* Independent variable: quantitative or qualitative

## Modeling Qualitative Data
Rather than predicting these two values of the binary response variable, we try to model the probabilities that the response takes one of these two values.

Let $\pi$ denote the probability that $Y=1$ when $X=x$.

If we use the standard linear model, we cannot model probability; $\pi = Pr(Y=1 \mid X=x) = \beta_0 + \beta_1 x + \epsilon$.
* LHS lies between 0 and 1 while RHS is unbounded

### Logistic model
Therefore, in logistic model, $\log \frac{\pi}{1-\pi} = \beta_0 + \beta_1 x$.
Then, $\pi = Pr(Y=1 \mid X=x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}$

$Y \mid X = x \sim B(1, \pi)$.

### Logistic regression function (logistic model for multiple regression)
$\pi = Pr(Y=1 \mid X_1 = x_1, \ldots, X_p = x_p) = \frac{e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}$

Nonlinear in the parameters $\beta_0, \beta_1, \ldots, \beta_p$ but it can be linearized by the logit transformation
* $1 - \pi = Pr(Y=0 \mid X_1 = x_1, \ldots, X_p = x_p) = \frac{1}{1+e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}$
* $\frac{\pi}{1 - \pi} = e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}$
* $\log (\frac{\pi}{1 - \pi}) = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$
  * $\frac{\pi}{1 - \pi}$: odds
  * $\log (\frac{\pi}{1 - \pi})$: logit
  * Relative risk: $\pi_1/\pi_0$
  * Odds Ratio: $\frac{\pi_1}{1-\pi_1} / \frac{\pi_0}{1-\pi_0} = \frac{\pi_1(1-\pi_0)}{\pi_0 (1-\pi_1)}$

<!-- log(x/(1-x))는 logit function이라고 부른다. 확률에다가 logit function을 이용한게 logistic regression -->
<!-- GLM의 special case이다 -->

### Modeling and estimating the logistic regression model
* Maximum likelihood estimation (using an iterative procedure)
* Unlike least squares fitting, no closed-form expression exists for the estimates of the parameters. To fit a logistic regression in practice a computer program is essential.
* Tools, used for the suitability of the model, are not the usual $R^2, t, F$ tests, the ones employed in least squares regression.
* Information criteria such as AIC and BIC can be used for model selection.
* Instead of SSE, the logarithm of the likelihood (log-likelihood) for the fitted model is used. <!-- deviance -->


<!--
각 샘플에 대해서 \pi_x는 서로 다르다. 왜냐하면 \pi_x = E(Y|X=x). 
샘플 별 분포는 Bernoulli(\pi_x)를 따르기 때문에 분산은 각 데이터마다 \pi_x(1-\pi_x)
그러므로 이분산 구조를 가진다
-->

### Interpretation of regression coefficient
$\log \frac{\pi_x}{1 - \pi_x} = \beta_0 + \beta_1 x_1 + \beta_2 x_2$

$\log \frac{\pi_{x+1}}{1 - \pi_{x+1}} = \beta_0 + \beta_1 (x_1 + 1) + \beta_2 x_2$

$\log (\frac{\pi_{x+1}}{1 - \pi_{x+1}})/(\frac{\pi_x}{1 - \pi_x}) = \beta_1$  <!-- x_1이 한 단위 증가했을 때 기준값과의 odds ratio -->

In logistic regression, the odds ratio estimate is obtained by exponentiating the estimated beta coefficient.

For unit increase in $X_2$ with $X_1$ and $X_3$ keeping fixed, the odds ratios of $\frac{Pr(y=1)}{Pr(y=0)}$ is multiplied by $e^{\beta_2}$.

Note that $\frac{\pi}{1-\pi} = e^{\beta_0 + \beta_1 x_1 \ldots + \beta_p x_p}$

#### Interpretation of odds ratio
* $0 \leq OR \leq \infty$
* $OR \approx 1$: x has no significant effect
* $OR \rightarrow 0$: x significantly decreases the relative odds
* $OR \rightarrow \infty$: x significantly increases the relative odds

### Model significance
Global null hypothesis: $H_0: \beta_1 = \beta_2 = \ldots = 0$ vs $H_1$: not $H_0$  <!-- 자유도는 beta 개수! -->

Likelihood Ratio test: $\chi^2(df)$





