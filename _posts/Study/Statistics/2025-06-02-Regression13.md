---
title: Logistic Regression
author: JSH
date: 2025-06-02 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Logistic Regression

## Regression Analysis and Categorical Data Analysis

Earlier chapters
* Dependent variable: quantitative
* Independent variable: quantitative or qualitative

This chapter
* Dependent variatle: qualitative
* Independent variable: quantitative or qualitative

## Modeling Qualitative Data
Rather than predicting these two values of the binary response variable, we try to model the probabilities that the response takes one of these two values.

Let $\pi$ denote the probability that $Y=1$ when $X=x$.

If we use the standard linear model, we cannot model probability; $\pi = Pr(Y=1 \mid X=x) = \beta_0 + \beta_1 x + \epsilon$.
* LHS lies between 0 and 1 while RHS is unbounded

### Logistic model
Therefore, in logistic model, $\log \frac{\pi}{1-\pi} = \beta_0 + \beta_1 x$.
Then, $\pi = Pr(Y=1 \mid X=x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}$

$Y \mid X = x \sim B(1, \pi)$.

### Logistic regression function (logistic model for multiple regression)
$\pi = Pr(Y=1 \mid X_1 = x_1, \ldots, X_p = x_p) = \frac{e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}$

Nonlinear in the parameters $\beta_0, \beta_1, \ldots, \beta_p$ but it can be linearized by the logit transformation
* $1 - \pi = Pr(Y=0 \mid X_1 = x_1, \ldots, X_p = x_p) = \frac{1}{1+e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}$
* $\frac{\pi}{1 - \pi} = e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}$
* $\log (\frac{\pi}{1 - \pi}) = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$
  * $\frac{\pi}{1 - \pi}$: odds
  * $\log (\frac{\pi}{1 - \pi})$: logit
  * Relative risk: $\pi_1/\pi_0$
  * Odds Ratio: $\frac{\pi_1}{1-\pi_1} / \frac{\pi_0}{1-\pi_0} = \frac{\pi_1(1-\pi_0)}{\pi_0 (1-\pi_1)}$

<!-- log(x/(1-x))는 logit function이라고 부른다. 확률에다가 logit function을 이용한게 logistic regression -->
<!-- GLM의 special case이다 -->

### Modeling and estimating the logistic regression model
* Maximum likelihood estimation (using an iterative procedure)
* Unlike least squares fitting, no closed-form expression exists for the estimates of the parameters. To fit a logistic regression in practice a computer program is essential.
* Tools, used for the suitability of the model, are not the usual $R^2, t, F$ tests, the ones employed in least squares regression.
* Information criteria such as AIC and BIC can be used for model selection.
* Instead of SSE, the logarithm of the likelihood (log-likelihood) for the fitted model is used. <!-- deviance -->


<!--
각 샘플에 대해서 \pi_x는 서로 다르다. 왜냐하면 \pi_x = E(Y|X=x). 
샘플 별 분포는 Bernoulli(\pi_x)를 따르기 때문에 분산은 각 데이터마다 \pi_x(1-\pi_x)
그러므로 이분산 구조를 가진다
-->

### Interpretation of regression coefficient
$\log \frac{\pi_x}{1 - \pi_x} = \beta_0 + \beta_1 x_1 + \beta_2 x_2$

$\log \frac{\pi_{x+1}}{1 - \pi_{x+1}} = \beta_0 + \beta_1 (x_1 + 1) + \beta_2 x_2$

$\log (\frac{\pi_{x+1}}{1 - \pi_{x+1}})/(\frac{\pi_x}{1 - \pi_x}) = \beta_1$  <!-- x_1이 한 단위 증가했을 때 기준값과의 odds ratio -->

In logistic regression, the odds ratio estimate is obtained by exponentiating the estimated beta coefficient.

For unit increase in $X_2$ with $X_1$ and $X_3$ keeping fixed, the odds ratios of $\frac{Pr(y=1)}{Pr(y=0)}$ is multiplied by $e^{\beta_2}$.

Note that $\frac{\pi}{1-\pi} = e^{\beta_0 + \beta_1 x_1 \ldots + \beta_p x_p}$

#### Interpretation of odds ratio
* $0 \leq OR \leq \infty$
* $OR \approx 1$: x has no significant effect
* $OR \rightarrow 0$: x significantly decreases the relative odds
* $OR \rightarrow \infty$: x significantly increases the relative odds

### Model significance
Global null hypothesis: $H_0: \beta_1 = \beta_2 = \ldots = 0$ vs $H_1$: not $H_0$  <!-- 자유도는 beta 개수! -->

Likelihood Ratio test: $\chi^2(df)$

## Diagnostic in logistic regression
### Diagnostic measures
* $\hat{\pi_i}, i = 1, \ldots, n$
* Residual
  * Pearson's residual (RESCHI in SAS): $PR_i, i = 1, \ldots, n$
  * Standardized deviance residual (RESDEV in SAS): $DR_i$
* Leverage and influential observation
  * Weighted leverage: $p_{ii} *$
  * Cook's distance, $DBETA_i, DFG_i$

### How to use the measures
Same way as the corresponding ones from a linear regression
* scatter plot of $DR_i$ vs. $\hat{\pi_i}$
* scatter plot of $PR_i$ vs. $\hat{\pi_i}$
* index plot of $DR_i, DBETA_i, DFG_i$ and $p_{ii} *$

## Determination of Variables to Retain
Model is significant but none of individual predictors are significant.
Do we need all three variables?
(Also, you can check multicollinearity)

Instead of looking at the reduction in the error sum of squares (SSE), we look at the change in the (log) likelihood for the two fitted models in logistic regression.

With a large number of explanatory variables the side-by-side boxplots provide a quick screening procedure.

### Change in the log likelihood
To see whether the $q$ additional variables are significant, we look
$\triangle G = -2(L(p) - L(p+q))$
* $L(p)$: log likelihood for a model with $p$ variables and constant
* $L(p+q)$: log likelihood for a model with $p+q$ variables and constant
* $\triangle G \sim \chi^2(q)$ under the null $H_0: \beta_{p+1} = \beta_{p+2} = \ldots = \beta_{p+q} = 0$ <!-- 오른쪽 검정 -->
* A large value of the test statistic would call for the retention of the $q$ variables in the model
* The test is valid when $n$ is large

Likelihood: $\pi_1^{y_1} (1-\pi_1)^{1-y_1} \times \ldots \times \pi_p^{y_p} (1 - \pi_p)^{1-y_p}$

### AIC/BIC
The AIC and BIC criteria can be used to judge the suitability of various logistic models
* AIC = -2(log-likelihood of the fitted model) + $2(p+1)$
* BIC = -2(log-likelihood of the fitted model) + $(p+1) \log n$

## Judging the fit of a logistic regression
Alternative to approaches based on log-likelihood:
* Calculate proportion of correct classification by using cutoff value 0.5 (or others)
* Base level of correct classification = max($\frac{n_1}{n}, \frac{n_2}{n}$) where $n_1$: group 1, $n_2$: group 2, $n = n_1 + n_2$

### Concordance index
C = (correct classification)/n

<!-- base level에 비해서 TP + FN의 비율이 얼마나 증가했는지가 중요 -->
<!-- 또한, 한 class에 너무 많은 비율이 몰려있는지 아닌지 여부가 중요 -->

Caution: Concordance index is upward biased because the same data that were used to fit the model, was used to judge the performance of the model.
<!-- overfitting -->

## Multinomial Logit Model
Logistic regression model extended to situations where the response variable assumes more than two values

### Case 1: multinomial (polytomous) logistic regression
Response categories are not ordered.

e.g., choice of mode of transportation to work: private automobile, car pool, public transport, bicycle, or walking

### Case 2: proportional odds model
Response categories are ordered.

e.g., an opinion survey (strongly agree, agree, no opinion, disagree, and strongly disagree) and a clinical trial with responses to a treatment (improved, no change, worse)

## Multinomial Logistic regression
Extension of Binary Logistic Regression (for cases with more than three categories).

In cases where the dependent variable has $K(\geq 3)$ categories:
$\ln (\frac{\pi_j (x)}{\pi_K (x)}) = \beta_{0j} + \beta_{1j} x_1 + \ldots + \beta_{pj} x_p, j = 1, \ldots, K-1$

* Generalized logit function = g logit
* The categories of the dependent variable are assumed to be unordered
* Using the K-th category as the reference (base level); that is, modeling the relative probabilities of each category compared to this reference category.
* Note that the regression coefficients vary depending on the category of the dependent variable.

### Example
If $K=3$,
$\ln (\frac{\pi_1 (x)}{\pi_3 (x)}) = \beta_{01} + \beta_{11} x_1 + \ldots + \beta_{p1} x_p$ and
$\ln (\frac{\pi_2 (x)}{\pi_3 (x)}) = \beta_{02} + \beta_{12} x_1 + \ldots + \beta_{p2} x_p$.

$\pi_j = P(Y=j \mid x) = \frac{\exp (\beta_{0j} + \beta_{1j} x_1 + \ldots + \beta_{pj} x_p)}{1 + \sum_{i=1}^{K-1} \exp (\beta_{0j} + \beta_{1j} x_1 + \ldots + \beta_{pj} x_p)}$

Changing the reference category: $\ln (\pi_1 / \pi_2) = \ln (\pi_1 / \pi_3) - \ln (\pi_2 / \pi_3)$

<!-- 예시결과 풀어서 볼 줄 알아야 한다. 결과 해석 및 df 등등 자세히. df는 constant 빼고 계수 6개이므로 6 -->

## Ordered Response Category: ordinal logistic regression
When the outcome categories in a multinomial logistic regression have an inherent order, the analysis is conducted using an ordinal logistic regression model.

Proportional odds model:
$\ln (\frac{P(Y \leq j \mid x)}{1 - P(Y \leq j \mid x)}) = \beta_{j0} + \beta_1 x_1 + \ldots + \beta_p x_p, j = 1, \ldots, K-1$

* Cumulative logit function = c logit
* The regression coefficients, except for the intercepts, do not vary across the categories of the dependent variable.
* Interpretation: If the regression coefficient $\beta > 0$, a one-unit increase in the independent variable $x$ increases the probability that the dependent variable $Y$ falls into a lower category (i.e., $1 \leq Y \leq j)$
