---
title: Logistic Regression
author: JSH
date: 2025-06-02 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Logistic Regression

## Regression Analysis and Categorical Data Analysis

Earlier chapters
* Dependent variable: quantitative
* Independent variable: quantitative or qualitative

This chapter
* Dependent variatle: qualitative
* Independent variable: quantitative or qualitative

## Modeling Qualitative Data
Rather than predicting these two values of the binary response variable, we try to model the probabilities that the response takes one of these two values.

Let $\pi$ denote the probability that $Y=1$ when $X=x$.

If we use the standard linear model, we cannot model probability; $\pi = Pr(Y=1 \mid X=x) = \beta_0 + \beta_1 x + \epsilon$.
* LHS lies between 0 and 1 while RHS is unbounded

### Logistic model
Therefore, in logistic model, $\log \frac{\pi}{1-\pi} = \beta_0 + \beta_1 x$.
Then, $\pi = Pr(Y=1 \mid X=x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}$

$Y \mid X = x \sim B(1, \pi)$.

### Logistic regression function (logistic model for multiple regression)
$\pi = Pr(Y=1 \mid X_1 = x_1, \ldots, X_p = x_p) = \frac{e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}$

Nonlinear in the parameters $\beta_0, \beta_1, \ldots, \beta_p$ but it can be linearized by the logit transformation
* $1 - \pi = Pr(Y=0 \mid X_1 = x_1, \ldots, X_p = x_p) = \frac{1}{1+e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}}$
* $\frac{\pi}{1 - \pi} = e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p}$
* $\log (\frac{\pi}{1 - \pi}) = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$
  * $\frac{\pi}{1 - \pi}$: odds
  * $\log (\frac{\pi}{1 - \pi})$: logit
  * Relative risk: $\pi_1/\pi_0$
  * Odds Ratio: $\frac{\pi_1}{1-\pi_1} / \frac{\pi_0}{1-\pi_0} = \frac{\pi_1(1-\pi_0)}{\pi_0 (1-\pi_1)}$

<!-- log(x/(1-x))는 logit function이라고 부른다. 확률에다가 logit function을 이용한게 logistic regression -->
<!-- GLM의 special case이다 -->

### Modeling and estimating the logistic regression model
* Maximum likelihood estimation (using an iterative procedure)
* Unlike least squares fitting, no closed-form expression exists for the estimates of the parameters. To fit a logistic regression in practice a computer program is essential.
* Tools, used for the suitability of the model, are not the usual $R^2, t, F$ tests, the ones employed in least squares regression.
* Information criteria such as AIC and BIC can be used for model selection.
* Instead of SSE, the logarithm of the likelihood (log-likelihood) for the fitted model is used. <!-- deviance -->




