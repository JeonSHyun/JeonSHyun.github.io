---
title: Qualitative Variables as Predictors
author: JSH
date: 2025-04-14 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Qualitative Variables as Predictors

## Variable types
Quantitative variable has a well-defined scale of measurement.

Qualitative variable (or categorical variables)
- Sometimes, it is necessary to use qualitative variables in a regression through “indicator(dummy) variables”

<!-- 명목형은 가변수 처리함. 순서형은 linear한 패턴이 나타나지 않으면 가변수 (indicator = dummy) 처리 -->

<!-- 순서형 자료 예를들어서 흡연을 10대, 20대, 30대로 구분할 때 
15, 25, 35로 양적변수처럼 코딩해서 회귀분석을 할 수도 있지만 사실 20대에서 피해가 제일 클 수도 있다.
이런 경우에는 이때도 가변수 사용해야 함 -->

## Interaction
<!-- 교호작용은 scale에 영향을 받는다. x1이 원래는 interaction이 있었는데 log scale에서는 없을 수도 있고 그런 특징이 있음 -->

<!-- 외삽을 하지말자 (주의해야 함! 교수님께서 여러번 설명하심)
회귀분석은 적합한 모형의 설명변수의 범위 내에서 이루어졌을 때만 유효하다. 
예를들어 x가 10~20 범위에서 분석되었는데 9인 새 데이터를 예측하는 데 활용하면 안됨 
교호작용이 평가한 조건에서는 최적이더라도 모든 조건에서 최적해가 아니라는 것도 같은 의미로 볼 수 있음
e.g.) 주식 예측. 현재까지의 데이터를 기반으로 미래 예측은 명백한 외삽. 얼마든지 패턴이 바뀔 수 있기 때문에 심각한 bias가 있을 수 있다.
-->
Effects that occur in combinations of two or more variables.
In other words, it means that the effect of each variable is different depending on the relative level (set value) between variables.

If there is an interaction, even if it is optimal in the evaluated conditions, it is not optimal in all conditions, and there is a risk of problems if it is used under conditions not evaluated.

### Model with interaction
Linear regression model: $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i$ where $\epsilon_i \sim N(0, \sigma^2)$ and $x_{2i}=0$ if male, $x_{2i}=1$ if female.

Then, $E(y) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 = \left\{\begin{matrix} \beta_0 + \beta_1 x_1 \text{if male} \\ (\beta_0 + \beta_2) + \beta_1 x_1 \text{if female} \end{matrix}\right$

$\beta_2 = E(y \mid F, x_1 = x_1') - E(y \mid M, x_1 = x_1')$

However, we cannot be sure that the slope of the $E(y)$ straight line along $x_1$ is always the same.

If the difference between men and women ($2x$) varies depending on the score of $x_1$, then the slope is different.
A loss means that there is an interaction between $x_1$ and $x_2$.

Linear regression model: $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{1i} x_{2i} \epsilon_i$.

$\therefore E(y) = \left\{\begin{matrix} \beta_0 + \beta_1 x_1 \text{if male} \\ (\beta_0 + \beta_2) + (\beta_1 + \beta_3) x_1 \text{if female} \end{matrix}\right$
<!-- beta3가 slope의 차이가 된다 -->

## Qualitative variables with more than three level
When using indicator variables to represent a set of categories, the # of these variables required is one less than the # of categories.

<!-- 더미 변수를 카테고리 개수만큼 만들면 X'X가 singular가 되는 문제가 발생하므로, 하나 적게 만들어야 한다. 3개 타입의 카테고리가 존재하는 경우에는 2개의 더미 변수를 만드는 식.
그렇게 해도 가능한 모든 경우의 조합을 표현할 수 있다.
-->


### Model without interaction
$y = f(x_1, x_2) + \epsilon, x_2 = c1, c2, c3$.

$\therefore y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \epsilon_i$ 

$E(y) =$
* $(\beta_0 + \beta_2) + \beta_1 x_1$ for c1
* $(\beta_0 + \beta_3) + \beta_1 x_1$ for c2
* $\beta_0 + \beta_1 x_1$ for c3
The slope is the same, but only the intercept is different

$\beta_3 = E(y \mid c2) - E(y \mid c2)$

$\beta_3 - \beta_2 = E(y \mid c2) - E(y \mid c1)$

### Model with interaction
If you want to do both intercept and slope differently depending on c1, c2, c3.
That is, assuming that there may be an interaction between $x_1$ and $x_2$,

$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{1i} x_{2i} + \beta5 x_{1i} x_{3i} + \epsilon_i$

$E(y \mid c1) = (\beta_0 + \beta_2) + (\beta_1 + \beta_4) x_1$

$E(y \mid c2) = (\beta_0 + \beta_3) + (\beta_1 + \beta_5) x_1$

$E(y \mid c3) = \beta_0 + \beta_1 x_1$

<!-- 이걸 기반으로 회귀 결과에서 어떤 beta값의 p-value가 유의할 때 무엇과 무엇의 차이가 유의하게 나타나는건지 해석할 수 있다. 잘 모르겠으면 12페이지 부분 강의 다시 확인 -->

<!-- 아웃라이어 제거 후 제거하지 않은 모델과 R^2 등 비교하려고 하면 샘플수가 달라지기 때문에 비교가 어렵다. adjusted R^2로 비교 x. -->

## Result fitting the model
Confidence interval
* $\hat{y_0} = \tilde{x_0^T} \tilde{\hat{\beta}}, s.e.(\hat{y_0}) = \hat{\sigma} \sqrt{\tilde{x_0^T} (X^T X)^{-1} \tilde{x_0}}$

Prediction interval
* $E(\hat{y_0}) = \tilde{x_0^T} \tilde{\hat{\beta}} + e, s.e.(\hat{y_0}) = \hat{\sigma} \sqrt{1 + \tilde{x_0^T} (X^T X)^{-1} \tilde{x_0}}$

<!-- 교재 20페이지에 있는 수식은 오타임 -->

## Example: Preemployment Testing Data

Objective: wheter the pre-employment test in an attempt to screen job applicants discriminates on the race or not.
* Response (y): job performance
* Explanatory variables
  * race: minority, white
  * pre-employment test: x
 
### Models
* Model 1 (Pooled): $y_{ij} = \beta_0 + \beta_1 x_{ij} + \epsilon_{ij}, j = 1, 2; i = 1, 2, \ldots, n_j$
* Model 2
  * Minority: $y_{i1} = \beta_{01} + \beta_{11} x_{i1} + \epsilon_{i1}$
  * White: $y_{i2} = \beta_{02} + \beta_{12} x_{i2} + \epsilon_{i2}$
* Model 1 is reduced model and Model 2 and 3 is full model.

Hypothesis: Whether the relationship is the same for both group
* $H_0: \beta_{11} = \beta_{12}, \beta_{01} = \beta_{02}$

### Additional model
Model 3: $JPERF_{ij} = \beta_0 + \beta_1 x_{ij} + \gamma R_{ij} + \delta (R_{ij} \cdot x_{ij}) + \epsilon_{ij}$ where $R_{ij} = 1$ if $(i, j)^{th} \in$ minority.

$(j = 1, 2; i = 1, \ldots, n_j)$

For the minority: $\beta_0 + \gamma + (\beta_1 + \delta) x$

For the white: $\beta_0 + \beta_1 x$

* Note that Model 3 is equivalent to Model 2.
* $H_0: \beta_{11} = \beta_{12}, \beta_{01} = \beta_{02} \Rightarrow H_0: \gamma = \delta = 0$

### Additional model 2
Model 4:
* $y_{i1} = \beta_{01} + \beta_{11} x_{i1} + \epsilon_{i1}, \epsilon_{i1} \sim N(0, \sigma_1^2)$
* $y_{i2} = \beta_{02} + \beta_{12} x_{i2} + \epsilon_{i2}, \epsilon_{i2} \sim N(0, \sigma_2^2)$

<!-- 나머지는 오차항의 분산이 같으므로 추정해야하는 모수가 5개고 이 경우에는 6개. 실질적으로 데이터를 두개로 나누어서 따로 분석하는 경우에 해당한다.
이게 subgroup 분석에 해당. Model 3와 같은 형태인데 오차항의 분산이 다르다고 가정함
많이 활용되는 방법이니 잘 숙지하고 모델 비교할줄 알아야 한다.
-->

In this case, Model 4 is full model and Model 3 is reduced model.
$H_0: \sigma_1^2 = \sigma_2^2$

### Hypotheses of interest
#### Model 3 vs. Model 1
$H_0: \gamma = \delta = 0$ v.s. $H_1$: not $H_0$

* Full model: Model 3
  * Parameters: $\beta_0, \beta_1, \gamma, \delta$ ($p+1 = 4$)
* Reduced model: Model 1
  * Parameters: $\beta_0, \beta_1$ ($q+1 = 2$)

Reject $H_0$ if $F \geq F(p-q, n-p-1; \alpha)$

* $F = \frac{SSE(RM) - SSE(FM)/(p-q)}{SSE(FM)/(n-p-1)}$
* F-test: $F(p-q, n-p-1; \alpha)$

#### Interaction ($\delta$)
$H_0: \delta = 0$ v.s. $H_1$: not $H_0$
<!-- 귀무가설을 기각한다는 말은 interaction에 해당하는 delta가 0이 아니다 -->

$H_0$
* Minority: $\beta_0 + \gamma + \beta_1 x$
* White: $\beta_0 + \beta_1 x$
* Reduced model
* Same effect of "Test" regardless of "Race"
* Parameters: $\beta_0, \beta_1, \gamma$ + variance

$H_1$
* Full model
* Parameters: $\beta_0, \beta_1, \delta, \gamma$ + variance

<!-- intercept는 다르지만 slope는 같은 모델 -->

#### $\gamma$
$H_0: \gamma = 0$ v.s. $H_1$: not $H_0$

$H_0$
* Minority: $\beta_0 + (\beta_1 + \delta) x$
* White: $\beta_0 + \beta_1 x$
* Reduced model
* Parameters: $\beta_0, \beta_1, \gamma$ + variance

$H_1$
* Full model
* Parameters: $\beta_0, \beta_1, \delta, \gamma$ + variance

#### Final model
$y_{ij} = \beta_0 + \beta_1 x_{ij} + \delta (R_{ij} x_{ij}) + \epsilon_{ij}$
<!-- gamma=0 test를 했을 때 귀무가설 채택했고, 제일 p-value가 높으므로 full model과 reduced model이 차이가 없다 -->

Then, test the $\delta$

$H_0: \delta = 0$ v.s. $H_1: \delta \neq 0$

<!-- 정리하자면, full model을 고정하고 여러 reduced model을 테스트했을 때 gamma가 0인 reduced model과 가장 유사했다.
이로부터 새롭게 full model을 정의하고 gamma가 유의한지 여부 테스트 새롭게 함 -->

#### Subgroup analysis
However, it is necessary to look at the data for the individual group carefully.

It should be a prerequisite for comparing regressions in two samples that the relatoinship be valid in each of the samples when taken aloone.

Based on these findings, it is reasonable that the test is of no value for screening white applcants.
<!-- beta1에 대해 p-value가 너무 낮고 전체 Rsq도 낮다 -->
<!-- 앞의 final model에서 각 그룹별 계수가 나왔지만 이들이 유의한지를 검증할 수 없었는데 subgroup 분석에서는 이것이 가능하다 -->

$X_m = \frac{Y_m - \hat{\beta_0}}{\hat{\beta_1}}$: an estimate of the minimum acceptable test score required to attain $Y_m$


