---
title: Eigen-value
author: JSH
date: 2025-05-19 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Eigen-value

## Eigenvalue Decomposition
### Definition
For the matrix $A$, $Ax = \lambda x$ where $\lambda$ is eigenvalue and $x$ is eigenvector.
<!-- dimensions: A (n, n), x (n, 1). lambda는 scalar -->

#### Calculation
$Ax = \lambda x \Rightarrow (A - \lambda I)x = 0 \Rightarrow \mid A - \lambda I \mid = 0$
$\Rightarrow a_n \lambda^n a_{n-1} \lambda^{n-1} + \ldots + a_1 \lambda + a_0 = 0$

### Summary
<!-- 
대칭: A^t = A
직교행렬: A^-1 = A^t. x_i^t x_j = 0 (i!=j)

이부분 시간이 되면 다시 공부하기.. 헷갈림
-->

* If an $n \times n$ square matrix $A$ has one or more eigenvalues equal to 0, then rank($A$) < $n$
* If all the eigenvalues of an $n \times n$ square matrix $A$ are real, then the corresponding eigenvectors have all real components
* If an $n \times n$ square matrix $A$ is symmetric, then all its eigenvalues are real
* Let $A$ be an $n \times n$ symmetric matrix with distinct eigenvalues $\lambda_1$ and $\lambda_2$, and let $x_1$ and $x_2$ be the corresponding eigenvectors. Then, if $\lambda_1 \neq \lambda_2, x_1$ and $x_2$ are orthogonal, i.e., $x_1^T x_2 = 0$
* For an $n \times n$ symmetric matrix $A$, if its eigenvectors form an orthonormal basis and diagonalize $A$ as $D$, then there exists an orthogonal matrix $X$ such that $X^T A X = D$, where $X^T X = I$

<!-- X^tAX = D <-> A = XDX^t -->

### Eigenvalue decomposition
If $A$ is symmetric, $A$ has eigendecomposition: $A = XDX^T$

Eigenvectors that correspond to big eigenvalues are the directions in which the data has strong components (= large variance).

If the eigenvalues are more or less the same – there is no preferable direction.

#### Remark
If $\Sigma$ is variance matrix ($\Sigma = var(X)$), $\Sigma = XDX^T$ <!-- D는 대각값이 lambda -->

$tr(\Sigma) = \sum_{j=1}^p var(X_j) = tr(\sum_{j=1}^p \lambda_j x_j x_j^T) = \sum_{j=1}^p \lambda_j tr(x_j x_j^T) = \sum_{j=1}^p \lambda_j tr(x_j^T x_j^) = \sum_{j=1}^p \lambda_j$

<!-- 그러므로 lambda는 각 변수들의 분산의 합. 그래서 lambda가 고르면 분산이 고르다. 
그런데 한 eigenvalue가 엄청 크고 다른곳이 작으면 한쪽 방향으로 엄청 분산이 퍼져있음 -->
<!-- 약간 헷갈려서 다시 듣기 -->

$e_1 = \arg \max_{\mid w \mid =1} var(w^T X)$

$e_2 = \arg \max_{\mid w \mid =1} E[(x^T \hat{X}_{k-1})^2]$, where $\hat{X}_{k-1} = X - w_1 w_1^T X$

$e_1$ and $e_2$ are eigenvectors; principal component score.
* $e_i$: $i$ th principal component score
* $e_i \cdot X$: $i$ th principal component
