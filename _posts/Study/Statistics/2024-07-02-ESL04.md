---
title: Linear Methods for Classification
author: JSH
date: 2024-07-02 10:04:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, ESL]
use_math: true
---

# Linear Methods for Classification

## Introduction

### Classification
The activity of assigning objects to some pre-existing classes or categories

### Decisiou Boundary
Decision boundary divides the input space into a collection of regions labeled according to the classification.
For an important class of procedures, these decision boundaries are linear, this is what we will mean by linear methods for classification.
Two very popular but different methods: linear discriminant analysis and linear logistic regression.

## Linear Regression of an Indicator Matrix

### Indicator Matrix
Each of the response categories are coded via an indicator variable.
If there are $K$ classes, there will be $K$ such indicators $ð‘Œ_ð‘˜, ð‘˜=1, â€¦, ð¾$.

$Y_k = 1$ if $G=k$ and $0$ if $G \neq k$.

These are collected together in a vector $ð‘Œ=(ð‘Œ_1,â€¦, ð‘Œ_ð¾)$, and the $N$ training instances of these form an $ð‘Ã—ð¾$ indicator response matrix $ð‘Œ$.

### Linear regression
We fit a linear regression model to each of the columns of Y simultaneously by least squares:

$\min_B \sum_{i=1}^N \mid y_i - [(1, x_i^T)B]^T \mid^2$

and the fit is given by $\hat{Y}_k = XB_k$
* $X$: $N \times P$ input data matrix
* $B_k$: coefficient vector of the size of $p \times 1$ for class $k$
* $\hat{Y}_k$: prediction score for class $k$

Doing this for all $K$ classes yields a score matrix $Y$ of the size of $ð‘ \times ð‘ƒ$ : $\hat{Y} = X \hat{B}$

* $(ð‘+1) \times ð¾$ coefficient vector $\hat{B} = (X^T X)^{-1} X^T Y$
* $X$ is the model matrix with $ð‘+1$ columns corresponding to the $ð‘$ inputs, and a leading column of 1â€™s for the intercept

A new observation with input x is classified as follows:
* Compute the fitted output $\hat{f}(x)^T = (1, x^T) \hat{B}$
* Identify the largest component and classify accordingly: $\hat{G} = \arg\max_k \hat{f}(x)$

### Masking problem
There is a serious problem with the regression approach when the number of classes K>=3.
Because of the rigid nature of the regression model, classes can be masked by others.

## Linear Discriminant Analysis

### Linear Discriminant Analysis (LDA)
LDA is a method to find a linear combination of features that characterizes or separates two or more classes of objects or events.

Class posterior $Pr(Y \mid X)$: $Pr(Y=k \mid X=x) = \frac{f_k (x) \pi_k}{\sum_{l = 1}^K f_l(x) \pi_l}$

* $\hat{f}(x) = Pr(X=x \mid Y=y)$: class-conditional density of $X$ in class $y=k$
* $\pi_k$: prior probability of class $k$
* $\sum_{k=1}^K \pi_k = 1$

Suppose that we model each class density as multivariate Gaussian:

$f_k (x) = \frac{1}{(2 \pi)^{p/2} \mid \Sigma_k \mid^{1/2}} e^{-\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)}$

We assume that the classes have a common covariance matrix $\Sigma_k = \Sigma$  $\forall k$

Decision boundary between class $k$ and $l$: $Pr(Y=k \mid X=x) = Pr(Y = l \mid X = x)$

It is sufficient to look at log-ratio:
$\log \frac{Pr(Y=k \mid X=x)}{Pr(Y=l \mid X=x)} = \log \frac{f_k(x)}{f_l(x)} + \log \frac{\pi_k}{\pi_l} = \log \frac{\pi_k}{\pi_l} -\frac{1}{2} (\mu_k + \mu_l)^T \Sigma^{-1} (\mu_k - \mu_l) + x^T \Sigma^{-1} (\mu_k = \mu_l)$

We see that the linear discriminant function: $\delta_k(x) = x^T \Sigma^{-1} \mu_k -\frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log \pi_k$

In practice, we do not know the parameters of the Gaussian distribution.
Rather, we can estimate them using training data:
* $\hat{pi}_k$: $N_k/N$, where $N_k$ is the number of class-k observations
* $\hat{\mu_k}$: $\sum_{y_i = k} x_i/N_k$
* $\hat{\Sigma}$: $\sum_{k=1}^K \sum_{y_k = k} (x_i - \hat{\mu_k})(x_i - \hat{\mu_k})^T/(N-K)$

### Quadratic discriminat function (QDA)
If the $\Sigma_k$ are not assume to be equal, we get QDA

$\delta_k(x) = -\frac{1}{2} \log \mid \Sigma_k \mid -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) + \log \pi_k$ 

The decision boundary between each pair of classes $k$ and $l$ is described by a quadratic equation: $\\{x: \delta_k(x) = \delta_l(x)\\}$

### Regularized Discriminat Analysis
Regularized discriminant analysis proposed regularized covariance matrices which is a compromise between LDA and QDA

$\hat{\Sigma}_k(\alpha) = \alpha \hat{\Sigma})_k + (1 - \alpha) \hat{\Sigma}$
* $\hat{\Sigma}$: pooled covariance matrix as used in LDA
* $\alpha \in [0, 1]$

In practice, $\alpha$ can be chosen based on the performance of the model on validation data, or by cross-validation.

## Logistic Regression
### Logistic Regression


