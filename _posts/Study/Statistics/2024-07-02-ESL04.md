---
title: Linear Methods for Classification
author: JSH
date: 2024-07-02 10:04:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, ESL]
use_math: true
---

# Linear Methods for Classification

## Introduction

### Classification
The activity of assigning objects to some pre-existing classes or categories

### Decisiou Boundary
Decision boundary divides the input space into a collection of regions labeled according to the classification.
For an important class of procedures, these decision boundaries are linear, this is what we will mean by linear methods for classification.
Two very popular but different methods: linear discriminant analysis and linear logistic regression.

## Linear Regression of an Indicator Matrix

### Indicator Matrix
Each of the response categories are coded via an indicator variable.
If there are $K$ classes, there will be $K$ such indicators $ð‘Œ_ð‘˜, ð‘˜=1, â€¦, ð¾$.

$Y_k = 1$ if $G=k$ and $0$ if $G \neq k$.

These are collected together in a vector $ð‘Œ=(ð‘Œ_1,â€¦, ð‘Œ_ð¾)$, and the $N$ training instances of these form an $ð‘Ã—ð¾$ indicator response matrix $ð‘Œ$.

### Linear regression
We fit a linear regression model to each of the columns of Y simultaneously by least squares:

$\min_B \sum_{i=1}^N \mid y_i - [(1, x_i^T)B]^T \mid^2$

and the fit is given by $\hat{Y}_k = XB_k$
* $X$: $N \times P$ input data matrix
* $B_k$: coefficient vector of the size of $p \times 1$ for class $k$
* $\hat{Y}_k$: prediction score for class $k$

Doing this for all $K$ classes yields a score matrix $Y$ of the size of $ð‘ \times ð‘ƒ$ : $\hat{Y} = X \hat{B}$

* $(ð‘+1) \times ð¾$ coefficient vector $\hat{B} = (X^T X)^{-1} X^T Y$
* $X$ is the model matrix with $ð‘+1$ columns corresponding to the $ð‘$ inputs, and a leading column of 1â€™s for the intercept

A new observation with input x is classified as follows:
* Compute the fitted output $\hat{f}(x)^T = (1, x^T) \hat{B}$
* Identify the largest component and classify accordingly: $\hat{G} = \arg\max_k \hat{f}(x)$

### Masking problem
There is a serious problem with the regression approach when the number of classes K>=3.
Because of the rigid nature of the regression model, classes can be masked by others.

## Linear Discriminant Analysis

### Linear Discriminant Analysis (LDA)
LDA is a method to find a linear combination of features that characterizes or separates two or more classes of objects or events.

Class posterior $Pr(Y \mid X)$: $Pr(Y=k \mid X=x) = \frac{f_k (x) \pi_k}{\sum_{l = 1}^K f_l(x) \pi_l}$

* $\hat{f}(x) = Pr(X=x \mid Y=y)$: class-conditional density of $X$ in class $y=k$
* $\pi_k$: prior probability of class $k$
* $\sum_{k=1}^K \pi_k = 1$

Suppose that we model each class density as multivariate Gaussian:

$f_k (x) = \frac{1}{(2 \pi)^{p/2} \mid \Sigma_k \mid^{1/2}} e^{-\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)}$

We assume that the classes have a common covariance matrix $\Sigma_k = \Sigma$  $\forall k$

Decision boundary between class $k$ and $l$: $Pr(Y=k \mid X=x) = Pr(Y = l \mid X = x)$

It is sufficient to look at log-ratio:
$\log \frac{Pr(Y=k \mid X=x)}{Pr(Y=l \mid X=x)} = \log \frac{f_k(x)}{f_l(x)} + \log \frac{\pi_k}{\pi_l} = \log \frac{\pi_k}{\pi_l} -\frac{1}{2} (\mu_k + \mu_l)^T \Sigma^{-1} (\mu_k - \mu_l) + x^T \Sigma^{-1} (\mu_k = \mu_l)$

We see that the linear discriminant function: $\delta_k(x) = x^T \Sigma^{-1} \mu_k -\frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log \pi_k$

In practice, we do not know the parameters of the Gaussian distribution.
Rather, we can estimate them using training data:
* $\hat{pi}_k$: $N_k/N$, where $N_k$ is the number of class-k observations
* $\hat{\mu_k}$: $\sum_{y_i = k} x_i/N_k$
* $\hat{\Sigma}$: $\sum_{k=1}^K \sum_{y_k = k} (x_i - \hat{\mu_k})(x_i - \hat{\mu_k})^T/(N-K)$

### Quadratic discriminat function (QDA)
If the $\Sigma_k$ are not assume to be equal, we get QDA

$\delta_k(x) = -\frac{1}{2} \log \mid \Sigma_k \mid -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) + \log \pi_k$ 

The decision boundary between each pair of classes $k$ and $l$ is described by a quadratic equation: $\\{x: \delta_k(x) = \delta_l(x)\\}$

### Regularized Discriminat Analysis
Regularized discriminant analysis proposed regularized covariance matrices which is a compromise between LDA and QDA

$\hat{\Sigma}_k(\alpha) = \alpha \hat{\Sigma}_k + (1 - \alpha) \hat{\Sigma}$
* $\hat{\Sigma}$: pooled covariance matrix as used in LDA
* $\alpha \in [0, 1]$

In practice, $\alpha$ can be chosen based on the performance of the model on validation data, or by cross-validation.

## Logistic Regression

### Logistic Regression
Odds of class $k$: a probability of $k$ occurring $P(y=k \mid x)$ and the opposite probability $P(y \neq k \mid x)$ ratio

Odds of class $k$ = $\frac{P(y = k \mid x)}{P(y \neq k \mid x)} = \frac{P(y = k \mid x)}{1 - P(y = k \mid x)}$

Log odds of class $k$ = $\log (\frac{P(y = k \mid x)}{1 - P(y = k \mid x)})$

Logistic model models the log-odds of an event as a linear combination of independent variables

$\log (\frac{P(y = k \mid x)}{1 - P(y = k \mid x)}) = \beta^T x$

Here $\beta = \\{\beta_0, \beta_1 \\}$, and we assume that the vector of input $x_i$ includes the constant term 1 to accomodate the intercept.

In two-calss case,

Log odds = $\log (\frac{P(y = 1 \mid x)}{1 - P(y = 1 \mid x)}) = \beta^T x$

Therefore, Odds = $\frac{P(y = 1 \mid x)}{1 - P(y = 1 \mid x)}$

$P(y=1 \mid x) = \frac{e^{\beta^T x}}{1 + e^{\beta^T x}} = \frac{1}{1 + e^{-\beta^T x}}$

* When $\beta^T x \rightarrow \infty$, it converges to 1
* When $\beta^T x \rightarrow -\infty$, it converges to 0

Therefore, the function limits the probability to between 0 and 1, which translates to the predictive probability for the class

### Fitting Logistic Regression Models
Logistic regression models are usually fit by maximum likelihood: $L(\beta) = \prod_{i = 1}^N P(y_i \mid x_i; \beta)$

In the two-class case, $P(y_i \mid x_i; \beta) = p(x_i; \beta)$ if $y_i = 1$ and $1 - p(x_i\beta)$ if $y_i = 0$,
where $p_k(x_i;\beta) = P(y_i=k \mid X = x_i;\beta) = \frac{1}{1 + e^{-\beta^T x}}$

Therefore, the likelihood for $N$ observations can be written as:

$L(\beta) = \prod_{i = 1}^N [p(x_i;\beta)]^{y_i} [1 - p(x_i;\beta)]^{1 - y_i}$

The log-likelihood can be written as: $l(\beta) = \sum_{i=1}^N \\{y_i \log p(x_i;\beta) + (1 - y_i) \log (1 - p(x_i;\beta)) \\}$

Since $p_k(x_i;\beta) = \frac{1}{1 + e^{-\beta^T x}}$, the log-likelihood can be written as:

$\sum_{i=1}^N \\{y_i \log p(x_i;\beta) + (1 - y_i) \log (1 - p(x_i;\beta)) \\} = \sum_{i = 1}^N \\{y_i \beta^T x_i -\log (1 + e^{\beta^T x_i}) \\}$

To maximize the log-likelihood, we set its derivatives to zero.
These score equations are:

$\frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^N x_i (y_i - p(x_i;\beta)) = 0$

To solve the score equations, we use the Newton-Raphson algorithm.
The Newton-Raphson method finds solutions by iteratively approximating nonlinear equations.

Starting with $\beta^{old}$, a single Newton update is $\beta^{new} = \beta^{old} - (\frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T})^{-1} \frac{\partial l(\beta)}{\partial \beta}$

Where $\frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T} = -\sum_{i=1}^N x_i x_i^T p(x_i;\beta)(1 - p(x_i; \beta))$

These equations get solved repeatedly.
