---
title: Analysis of Collinear Data
author: JSH
date: 2025-05-20 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Analysis of Collinear Data

## Introduction
Interpretation of the multiple regression equation depends implicitly on the assumption that the predictor variables are not strongly interrelated.
* e.g., interpretation of a regression coefficient
* a complete absence of linear relationship among the predictors - orthogonal

If the predictors are so strongly interrelated, the regression results are ambiguous.
* Condition of severe nonorthogonality = problem of collinear data or multicollinearity
* In regression model: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \epsilon$
  * Linear dependent: $a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + a_4 x_4 = 0$
  * Multicollinearity: $a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + a_4 x_4 \cong 0$
* The problem can be extremely difficult to detect
* It is not a specification error that may be uncovered by exploring regression residual.

Example: linear-dependent data. $x_1 + x_2 + x_3 = x_4$

<!-- 각 변수에 적당한 상수를 곱해서 0에 근사하도록 할 수 있다: multicollinearity -->

## Multicollinearity
<!-- 다중공선성 -->
$a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + a_4 x_4 \cong 0$ for some $a_0, a_1, \ldots, a_p: a \neq 0$

* Regression assumption: $rank(X) = p + 1 \rightarrow$ existence of an inverse matrix of $X^T X$
* It is difficult to interpret regression coefficients in multiple regression analysis
* Multicollinearity is not found through residual analysis
* The cause of multicollinearity may be lack of observation or the inherent nature of independent variables to be analyzed.
* Multicollinearity problems are usually considered after regression diagnosis including residual analysis.

### Confounding
<!-- 교란변수. z는 X와 Y에 영향을 주는 변수이면서 모형에 z가 빠져있는 경우. correlation이 있음
다중공선성은 설명변수들 간 correlation이 너무 높은 경우.
교란변수와 다중공선성이 같이 일어날 수도 있다

교란변수와 다중공선성의 의미를 정확히 이해하기!

범주형 자료에서 하나의 수준이 너무 많을 때 intercept와도 있을 수도 있고, interaction term을 넣어도 거의 값이 동일할 수 있다..
-->
A confounder is a variable that influences both the dependent variable and independent variable, causing a spurious association.

## Symptom of multicollinearity
<!-- 다중공선성이 있으면 전체 ANOVA table에서의 F-value는 유의하지만 각각 변수들의 p-value는 유의하지 않는 경우가 발생한다 -->

Model with $x_1, \ldots, x_p$: significant (through F-value or ANOVA table).
But, some (or many) of $x_i$: not significant (from t-value).

Because, estimation of $\hat{\beta_i}$ is unstable (i.e., s.e.($\hat{\beta_i}$) is large).
Drastic change of $\hat{\beta_i}$ by adding or deleting a variable.

* $var(\hat{\beta}) = (X^T X)^{-1} \sigma^2$
* $\sigma^2$ is increasing, then $t = \frac{\hat{\beta}}{\sqrt{var(\hat{\beta})}}$ is decreasing. p-value is increasing

Also, estimation result contrary to the common sense.
* The algebraic signs of the estimated coefficients do not conform to prior expectations
* Coefficients of variables that are expected to be important have large standard errors

<!-- 다중공선성이 크다고 해도 sample size가 커지면 효과가 줄어든다. 샘플이 커지면 SE가 줄어드니까 완화되는 경향 -->

## Numerical measure of multicollinearity

### Correlation coefficients of $x_i$ and $x_j$ (i \neq j)$
Pairwise linear relation.

Can't detect linear relation among 3 or more variables.

### Variance Inflation Factor (VIF)
<!-- 분산팽창계수, 분산확장인수 -->
Use multiple correlation coefficient between $x_i$ and $(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_p)$

$VIF_i = VIF(x_i) = \frac{1}{1 - R^2_{i \cdot 1, 2, \ldots, (i-1), (i+1), \ldots, p}}$

$\bar{VIF} = \frac{1}{p} \sum_{i=1}^p VIF_i$: overall measure of multicollinearity

Generally, $VIF_i > 10$ is the evidence of multicollinearity

### Principal components
Use eigenvalues $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_p > 0$ and eigenvectors $v_1, \ldots, v_p$ of the correlation matrix $R$ (positive definite matrix) of $x_1, \ldots, x_p$ (not robust to outliers)
* Eigenvalues: roots of $\mid \lambda I_p - R \mid = 0$
* Eigenvectors: $Rv = \lambda v$ for an eigenvalue $\lambda$

Overall measure of multicollinearity
* Condition number
  * $\sqrt{\lambda_1 / \lambda_p} = (1/\sqrt{\lambda_p})/(1/\sqrt{\lambda_1}) (>1)$
  * In practice, $\sqrt{\lambda_1 / \lambda_p} > 15$: multicollinearity
* Average $VIF^S$
  * $\frac{1}{p} \sum_{j=1}^p \frac{1}{\lambda_j}$
  * In practice, $VIF^S > 5$: multicollinearity

## Regression Analysis using Principal Component
### Idea
A method to explain the response variable using new explanatory variables (principal components) that are orthogonal to each other, so as to eliminate multicollinearity among the original explanatory variables.
In particular, the new variables are constructed so that they retain most of the variation (or variance) of the original explanatory variables.

When explanatory variables $x_1$ and $x_2$ show severe multicollinearity, a new explanatory variable $p_1 = v_1 x_1 + v_2 x_2$ can be created, which can explain the response variable. (Dimension reduction)

$cov(p_1, p_2) = 0$

### How to find the principal components
#### Step 1: Find eigenvalues $(\lambda_1 \geq \lambda_2 \geq \ldots \lambda_p > 0)$ and eigenvectors $(v_1, \ldots, v_p)$ of correlation matrix
$R = (\frac{S_{ij}}{\sqrt{S_{ii}} \sqrt{S_{jj}}})_{1 \leq i \leq p, 1 \leq j \leq p}$

where $S_{ij} = \frac{1}{n-1} \sum_{k=1}^n (x_{ki} - \bar{x_i})(x_{kj} - \bar{x_j})$

* Property: The eigenvectors of a symmetric matrix are mutually orthogonal; that is, $v_i^T v_j = 0$ for $i \neq j$
* Standardized eigenvectors are often used; that is, $v_i^T v_i = 1$

#### Step 2: Define the standardized independent variables
$x_1^S = \frac{x_1 - \bar{x_1}}{s_1}$ where $s_j = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_{ij} - \bar{x})^2}$

#### Step 3: Compute the $j$ th principal component
$C_j = x_1^S v_{1j} + \ldots + x_p^S v_{pj}$

$C_{ij} = \sum_{k=1}^p (\frac{x_{ik} - \bar{x_k}}{s_k}) v_{kj}$

<!-- 주성분 = 표준화변수 x 고유벡터 -->

#### Properties of principal component
* Variance of $C_j = (C_{1j}, \ldots, C_{nj})^T = \lambda_j$
* $Cov(C_i, C_j) = 0$ for $i \neq j$
* $\lambda_j = 0 \Rightarrow C_j = 0$

Expansion:

$R = \sum_{j=1}^p \lambda_j e_j e_j^T$

$C = e_j^T x_s$

$var(C) = var(e_j^T x_s) = e_j^T var(x_s) e_j = e_j^T (\sum_{i=1}^p \lambda_i e_i e_i^T) e_j = \sum_{i=1}^p \lambda_i e_j^T e_i e_i^T e_j = \lambda_j e_j^T e_j e_j^T e_j = \lambda_j$

<!-- Cov(C_i, C_j) = 0도 똑같이 증명 가능. 단 i != j 이므로 0이 되는것이다 -->

Expansion 2:

$R = VDV^T$

$C = x^s V$

$Var(C) = Var(x^s V) = V^T R V = V^T VDV^T V = D$

### Model Representation in terms of Principal Components
$y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i, \epsilon_i \sim iid N(0, \sigma^2) (i = 1, \ldots, n)$

$= \gamma_0 + \gamma_1 x_{i1}^S + \ldots + \gamma_p x_{ip}^S + \epsilon_i$, where $\gamma_j = s_j \beta_j, \gamma_0 = \beta_0 + \beta_1 \bar{x_1} + \ldots + \beta_p \bar{x_p}$ for $j = 1, \ldots, p$

$= \gamma_0 + \alpha_1 C_{i1} + \ldots + \alpha_p C_{ip} + \epsilon_i$

where $a_i = v_{1i} \gamma_1 + \ldots + v_{pi} \gamma_p; \gamma_i = v_{i1} \alpha_1 + \ldots + v_{ip} \alpha_p$

#### Summary
$Y = X \beta + \epsilon$

$= \gamma_0 1 + X^S \gamma + \epsilon$

$= \gamma_0 1 + X^S VV^T \gamma + \epsilon$

$= \gamma_0 1 + C \alpha + \epsilon$

$= \gamma_0 1 \alpha_1 C_1 + \ldots + \alpha_p C_p + \epsilon$

### Identification of the source of multicollinearity
* When $\lambda_{q+1} \approx	0, \ldots, \lambda_p \approx 0$, then test $H_0: \alpha_{q+1} = \ldots = \alpha_p = 0$ vs. $H_1$: not $H_0$
* If the result is not significant, then retain the reduced model
* If the result is significant, test $H_0: \alpha_{q+2} = \ldots = \alpha_p = 0$ vs. $H_1$: not $H_0$ (continue)

### Interpretation of the final fitting
$\hat{y_i} = \bar{y} + \hat{\beta_1} (x_{i1} - \bar{x_1}) + \ldots + \hat{\beta_p} (x_{ip} - \bar{x_p})$

$= \bar{y} + \hat{\gamma_1} x_{i1}^S + \ldots + \hat{\gamma_p} x_{ip}^S$

$= \bar{y} + \hat{\alpha_1} C_{i1} + \ldots + \hat{\alpha_q} C_{iq} + \hat{\alpha_{q+1}} C_{iq + 1} + \ldots + \hat{\alpha_p} C_{ip}$

$\approx \bar{y} + \hat{\alpha_1} C_{i1} + \ldots + \hat{\alpha_q} C_{iq}$ for $q < p$

* Note: $\hat{\gamma_0} = \hat{\beta_0} + \hat{\beta_1} \bar{x_1} + \ldots + \hat{\beta_p} \bar{x_p} = \bar{y}$

## What to do with multicollinearity data?
### Experimental situation
Design an experiment so that multicollinearity does not occur

### Observational situation
Reduce the model (essentially reduce the variables) using the information from the principal components (PCR should be done after examining "high leverage points", "influential observation" and "outliers")

Ridge regression.

<!-- 예시 보면서 무엇을 의미하는지 정확히 알아야 한다. 예시가 아주 좋으므로 숙지하고 이해하기.. 프린트하 -->

<!-- x1와 x2의 collinearity가 있으면 이 둘의 beta 추정치에 문제가 생기지 x3의 추정치 beta3에 영향을 주지 않는다고 이해하기 -->
