---
title: Analysis of Collinear Data
author: JSH
date: 2025-05-20 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Analysis of Collinear Data

## Introduction
Interpretation of the multiple regression equation depends implicitly on the assumption that the predictor variables are not strongly interrelated.
* e.g., interpretation of a regression coefficient
* a complete absence of linear relationship among the predictors - orthogonal

If the predictors are so strongly interrelated, the regression results are ambiguous.
* Condition of severe nonorthogonality = problem of collinear data or multicollinearity
* In regression model: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \epsilon$
  * Linear dependent: $a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + a_4 x_4 = 0$
  * Multicollinearity: $a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + a_4 x_4 \cong 0$
* The problem can be extremely difficult to detect
* It is not a specification error that may be uncovered by exploring regression residual.

Example: linear-dependent data. $x_1 + x_2 + x_3 = x_4$

<!-- 각 변수에 적당한 상수를 곱해서 0에 근사하도록 할 수 있다: multicollinearity -->

## Multicollinearity
<!-- 다중공선성 -->
$a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + a_4 x_4 \cong 0$ for some $a_0, a_1, \ldots, a_p: a \neq 0$

* Regression assumption: $rank(X) = p + 1 \rightarrow$ existence of an inverse matrix of $X^T X$
* It is difficult to interpret regression coefficients in multiple regression analysis
* Multicollinearity is not found through residual analysis
* The cause of multicollinearity may be lack of observation or the inherent nature of independent variables to be analyzed.
* Multicollinearity problems are usually considered after regression diagnosis including residual analysis.

### Confounding
<!-- 교란변수. z는 X와 Y에 영향을 주는 변수이면서 모형에 z가 빠져있는 경우. correlation이 있음
다중공선성은 설명변수들 간 correlation이 너무 높은 경우.
교란변수와 다중공선성이 같이 일어날 수도 있다

교란변수와 다중공선성의 의미를 정확히 이해하기!

범주형 자료에서 하나의 수준이 너무 많을 때 intercept와도 있을 수도 있고, interaction term을 넣어도 거의 값이 동일할 수 있다..
-->
A confounder is a variable that influences both the dependent variable and independent variable, causing a spurious association.

## Symptom of multicollinearity
<!-- 다중공선성이 있으면 전체 ANOVA table에서의 F-value는 유의하지만 각각 변수들의 p-value는 유의하지 않는 경우가 발생한다 -->

Model with $x_1, \ldots, x_p$: significant (through F-value or ANOVA table).
But, some (or many) of $x_i$: not significant (from t-value).

Because, estimation of $\hat{\beta_i}$ is unstable (i.e., s.e.($\hat{\beta_i}$) is large).
Drastic change of $\hat{\beta_i}$ by adding or deleting a variable.

* $var(\hat{\beta}) = (X^T X)^{-1} \sigma^2$
* $\sigma^2$ is increasing, then $t = \frac{\hat{\beta}}{\sqrt{var(\hat{\beta})}}$ is decreasing. p-value is increasing

Also, estimation result contrary to the common sense.
* The algebraic signs of the estimated coefficients do not conform to prior expectations
* Coefficients of variables that are expected to be important have large standard errors

<!-- 다중공선성이 크다고 해도 sample size가 커지면 효과가 줄어든다. 샘플이 커지면 SE가 줄어드니까 완화되는 경향 -->

## Numerical measure of multicollinearity

### Correlation coefficients of $x_i$ and $x_j$ (i \neq j)$
Pairwise linear relation.

Can't detect linear relation among 3 or more variables.

### Variance Inflation Factor (VIF)
<!-- 분산팽창계수, 분산확장인수 -->
Use multiple correlation coefficient between $x_i$ and $(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_p)$

$VIF_i = VIF(x_i) = \frac{1}{1 - R^2_{i \cdot 1, 2, \ldots, (i-1), (i+1), \ldots, p}}$

$\bar{VIF} = \frac{1}{p} \sum_{i=1}^p VIF_i$: overall measure of multicollinearity

Generally, $VIF_i > 10$ is the evidence of multicollinearity

### Principal components
Use eigenvalues $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_p > 0$ and eigenvectors $v_1, \ldots, v_p$ of the correlation matrix $R$ (positive definite matrix) of $x_1, \ldots, x_p$ (not robust to outliers)
* Eigenvalues: roots of $\mid \lambda I_p - R \mid = 0$
* Eigenvectors: $Rv = \lambda v$ for an eigenvalue $\lambda$

Overall measure of multicollinearity
* Condition number
  * $\sqrt{\lambda_1 / \lambda_p} = (1/\sqrt{\lambda_p})/(1/\sqrt{\lambda_1}) (>1)$
  * In practice, $\sqrt{\lambda_1 / \lambda_p} > 15$: multicollinearity
* Average $VIF^S$
  * $\frac{1}{p} \sum_{j=1}^p \frac{1}{\lambda_j}$
  * In practice, $VIF^S > 5$: multicollinearity




