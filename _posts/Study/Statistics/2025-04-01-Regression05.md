---
title: Regression Diagnostics - Detection of Model Violations
author: JSH
date: 2025-04-01 10:30:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, Regression]
use_math: true
---

# Regression Diagnostics: Detection of Model Violations

<!--
LINE 가정이 깨지면 p-value를 신뢰할 수 없다. 
beta값은 신뢰할 수 있지만 1종 오류가 증가하여 p-value는 신뢰 불가. 
따라서 LINE 가정은 가설검정을 위해 필요하다고 할 수 있다

오차는 우리가 볼 수 없으므로 잔차를 가지고 가정을 확인한다.
-->

## What happens if model is misspecified?
Let us presume that the data $(x1, y1), \ldots , (xn, yn)$ are generated from the model:

$y_i = \beta_0^* + \beta_1 (x_i - \bar{x}) + \beta_2 x_i^2 + \epsilon_i, \epsilon_i \sim iid N(0, \sigma^2)$.

However assumes the following simple linear regression model: 

$y_i = \beta_0^* + \beta_1 (x_i - \bar{x}) + \epsilon_i^*$.

Here, $\epsilon_i^* = \epsilon_i + \beta_2 x_i^2$ and the error for the analyst’s model is not truly random but has a systematic component:

$E(\epsilon_i^*) = \beta_2 x_i^2$

Then esimators are $\hat{y_i} = \hat{\beta_0^*} + \hat{\beta_1} (x_i - \bar{x})$

and $\hat{\beta_0^*} = \bar{y} = \frac{1}{n} \\{\sum_{i=1}^n [\beta_0^* + \beta_1 (x_i - \bar{x}) + \beta_2 x_i^2 + \epsilon_i] \\} = \beta_0^* + \beta_2 \bar{x^2} + \bar{\epsilon}$

$\hat{\beta_1^*} = \frac{1}{S_{xx}} \\{\sum_{i=1}^n (x_i - \bar{x})[\beta_0^* + \beta_1 (x_i - \bar{x}) + \beta_2 x_i^2 + \epsilon_i] \\} = \beta_1 + \frac{n \beta_2}{S_{xx}} (\bar{x^3} - \bar{x} \bar{x^2}) + \frac{1}{S_{xx}} \sum_{i=1}^n \epsilon_i (x_i - \bar{x})$

Would the residuals of the incorrect simple linear model still look random around a mean of zero? 

### Estimators are biased
$E(\hat{\beta_0^*}) = \beta_0^* + \beta_2 \bar{x}$

$E(\hat{\beta_1^*}) = \beta_1 + \frac{n \beta_2}{S_{xx}} (\bar{x^3} - \bar{x} \bar{x^2})$

<!-- LINE 가정의 L이 빠지면 E(\hat{\beta}) \neq \beta인 bias가 발생할 수 있다. 나머지 INE는 깨져도 불편성이 만족되는 경우가 많다 -->

### Estimated residuals have some patterns
$E(y_i - \hat{y_i}) = \beta_0^* + \beta_1 (x_i - \bar{x}) + \beta_2 x_i^2 - [\beta_0^* + \beta_2 \bar{x^2} + \beta_1 (x_i - \bar{x}) + \frac{n \beta_2}{S_{xx}} (x_i - \bar{x}) (\bar{x^3} - \bar{x} \bar{x^2})] = \beta_2[(x_i^2 - \bar{x^2}) + \frac{n}{S_{xx}} (x_i - \bar{x})(\bar{x^3} - \bar{x} \bar{x^2})] = \beta_2 (x_i^2 + r x_i + s)$

where $r = -\frac{n}{S_{xx}} (\bar{x^3} - \bar{x} \bar{x^2})$, $s = -[\bar{x^2} + \frac{n \bar{x}}{S_{xx}} (\bar{x^3} - \bar{x} \bar{x^2})]$.

If $\beta_2 = 0$, the fitted model is correct and results have mean zero.

## Validity of model assumption
$y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i, \epsilon \sim iid N(0, \sigma^2)$

### Linearity assumption
$E(Y \mid x_1, \ldots, x_p) = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$

Using graphical methods (e.g., scatter plot for simple linear regression)

### Error distribution assumption
* $E(\epsilon_i) = 0$
* $var(\epsilon_i) = \ldots = var(\epsilon_n) = \sigma^2$ (homogeneous variance)
* $\epsilon_i \sim N(0, \sigma^2)$
* $\epsilon_1, \ldots, \epsilon_n$: independent

Using graphical methods based on residuals $e_i = y_i - \hat{y_i}$

### Assumptions about explanatory (predictor) variables
* $x_{i1}, \ldots, x_{ip} (i = 1, \ldots, n)$ know exactly (non-random)
* $\tilde{1}, \tilde{x_1}, \ldots, \tilde{x_p}$: linearly independent

Using graphical methods or correlation matrices

## Residuals
<!-- normality를 따르는지 확인하기 -->

$r_i = \frac{e_i}{\sqrt{1 - p_{ii}} \hat{\sigma}}$ (internally standardized) where $\hat{\sigma}^2 = SSE/(n-p-1)$

<!-- 
r_i를 구할 때 sigma를 모르기 때문에 추정치를 쓴다. 이때 sigma에 e_i가 포함이 되므로 사실 엄밀하게 말하면 가우시안을 따르지 않는다. 잔차의 절댓값이 실제보다 작다. 다만 샘플이 충분히 크면 괜찮음.

예를 들어 (\bar{x} - \mu)/(\sigma/\sqrt{n}) 꼴로 바꾸면 normal dist를 따르지만, sigma를 구하지 못해서 표본표준편차 s를 사용하면 N(0, 1) 대신 t 분포를 따른다.
표본이 충분이 크면 근사적으로 N(0, 1)을 따른다
-->

$r_i^* = \frac{e_i}{\sqrt{1 - p_{ii}} \hat{\sigma_{(i)}}}$ (externally standardized) where $P = X(X^T X)^{-1} X^T, \hat{\sigma_{(i)}} = SSE_{(i)}/((n-1)-p-1)$

* $e = Y - \hat{Y} = (I - P)Y$
* $var(e_i) = (1 - P_{ii}) \sigma^2$

<!-- 이 의미는 i번째 데이터를 빼고 계산했다는 의미. 위 internally standardized의 문제를 개선하기 위해서임. e_i와 sigma가 독립적이라 낫다. 그래서 전체 나눠주는 개수가 (n-1) - p - 1이 된다. -->

<!-- r_i와 r_i* 모두 샘플이 충분히 크면 표준정규분포를 따른다 -->

Recall that $E(e_i) = 0, var(e_i) = (1-p_{ii}) \sigma^2, cov(e_i, e_j) = -p_{ij} \cdot \sigma^2$ $(E(\tilde{e}) = \tilde{0}, var(\tilde{e}) = (I-P) \sigma^2)$.

For a simple regression, $var(e_i) = \sigma^2 \sqrt{1 - [\frac{1}{n} + \frac{(x_i - \bar{x})^2}{S_{xx}}]}, p_{ii} = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{S_{xx}}$

$var(\hat{y}) = var(PY) = P var(Y) P = P^2 \sigma^2 = P \sigma^2 =  \sigma^2 X(X^T X)^{-1} X^T$: Residuals at $x$ locations close to the data center have smaller variances than those located at more extreme locations because

$X(X^T X)^{-1} X^T = 1 (1^T 1)^{-1} 1^T + \chi (\chi^T \chi)^{-1} \chi^T, \chi = (I - 1 (1^T 1)^{-1} 1^T) X_1$ where $X = (1, X_1)$.

<!-- 데이터 중심으로부터 떨어져있는 정도를 나타낸다. (떨어져있는 정도: chi) -->

Furthermore the standardized residuals are uncorrelated with each predictor variables and the fitted values because

$\sum e_i \hat{y_i} = \sum e_i (\hat{\beta_0} + \hat{\beta_1} + x_{1i} + \ldots + \hat{\beta_p} x_{pi}) = 0$

## Residual plot

### $(x_1, r), \ldots, (x_p, r)$ plot
* If the assumptions hold, this should be a random scatter plot
* Tools for checking non-linearity / non-homogeneous variance

<!-- 예를 들어 x2가 LINE 가정의 INE를 만족하지 못하고 다른 변수들은 만족했을 때 x2의 p-value가 특히 더 많이 영향을 받는 경향이 있다. -->

<!-- a는 제곱항 변수가 하나 빠졌다. independence 가정 불만족? b는 equal variance가 깨짐. x와 correlation이 있는 어떤 변수가 빠졌을 가능성이 크다고 생각해볼 수 있다. 둘다 p-value는 신뢰할 수 없 -->

### $(\hat{y}, r)$ plot
It should be a random scatter plot

<!-- r과 yhat을 축으로 해서 산점도 그려도 INE 가정을 만족해야 한다.
만약에 y가 포아송 분포를 따르는데 linear model을 가정했다고 하자. 포아송분포는 평균과 분산이 같다. 그러면 평균이 커질수록 분산이 커지므로 이런 경우를 찾아낼 수 있다. -->

### $(i, r_i)$ plot (index plot)
When the order in which the observations were taken is important.

<!-- (i, r) 시계열 데이터에 대한 index plot. 패턴이 있으면 independence가 깨짐. 이전 정보가 다음에 영향을 줌. -->

### Normal prob. plot (Q-Q plot)
($\Phi^{-1} (i/n), r_{(i)}$) resembles $"y=x"$ if the residuals are normally distributed.

<!-- 표준화 -> 순서통계량을 만든다. 표준정규분포에 해당하는 quantile 값과 비슷해야한다. 그래서 데이터가 정규분포를 따르는지 확인. 모든 잔차를 다 이용해서 따르는지 보기 때문에 매우 좋은 방법이다. -->

## Scatter plot
$(x_{i1}, y_i), \ldots, (x_{ip}, y_i)$ for linearity assumption.

$X(x_{il}, x_{im}) (l \neq m)$ for linearity independence (multicollinearity).
<!-- 다중공선성이 있을 때 각 변수를 하나씩 모델에 넣은 것과 두개 다 넣은 것이 결과가 차이가 아주 많이 난다. -->

## Added-Variable (A-V) Plot or Residual-Plus-Component (R+C) plot
A-V and R+C both check “linearity assumption” along with “outliers and influential observation”
Howver, A-V is more useful to detect influential points and R+C is more useful to detect nonlinearity.

<!-- AV랑 RC plot 모두 독립변수들 간 correlation이 있을 때 효과적인 방법이다 -->

### A-V plot (Partial Regression Plot)
$e_y$: residuals from $y$ and $(x_1, \ldots, x_{p-1})$.

$e_p$: residuals from $x_p$ and $(x_1, \ldots, x_{p-1})$.

Plot $((e_p)_i, (e_y)_i) (i = 1, 2, \ldots, n)$: partialling out the linear effects of $x_1, \ldots, x_{p-1}$.
<!-- x_1부터 x_p-1의 선형효과를 x_p와 y에서 제거한 후, linearity assumption check -->

### R+C plot (Partial Residual Plot)
Plot $(x_{ip}, e_i + \hat{\beta_p} x_{ip}) (i = 1, 2, \ldots, n)$.

Horizontal scale: $x_p$

<!-- RC plot에서  e_i는 모든 공변량의 효과를 제거한 것. 거기다가 x_p의 효과만 더해서 봄 -->

<!-- 예시에서 위 두개가 AV, 아래가 RC
왼쪽 위: 11번이 influential point, 7과 18이 outlier, time과 distance가 상관성이 매우 크다
오른쪽 위: 11번과 7번이 influential point, 18번이 outlier

아래: distance는 확실히 선형성이 있다. -->

## Leverage, Influence and Outliers
We would like to ensure that the fit is not overly determined by one or few observations.

<!-- Influential point: 영향점. 이 점이 조금 바뀌면 회귀분석 결과가 크게 달라진다. -->

### Leverage <!-- 지렛값 -->
Outliers in explanatory variables.

<!-- leverege: y가 한 단위 증가할 때 y_hat의 변화량.
leverage가 큰 점들은 y에 영향을 많이 주는 것. -->

$p_{ii} = i^{th}$ diagnoal element of $P = X(X^T X)^{-1} X^T$.
* Meaning: $p_{ii} = \frac{\partial \hat{y_i}}{\partial y_i}$
* $\frac{1}{n} \leq p_{ii} \leq 1, \sum_{i=1}^n p_{ii} = p+1$
  * $P = PY = tr((X^T X)^{-1} X^T X) = tr(I) = p + 1$  <!-- I가 (p+1) x (p+1) matrix -->
* Common practice: $p_{ii} > \frac{2(p+1)}{n}$
  * Twice the average
* Check if the high leverege point are also influential

### Measures of influence
<!-- i번째 데이터가 추가되었을때와 빠졌을 때 차이 확인 
Cooks: overall amount of change, DFITS: 추정치의 차이 

둘 다 계산을 n+1 해야하므로 너무 계산량이 많아진다.
이때, 간편계산법을 이용하면 아주 간편하게 할 수 있음
-->

#### Cook's distance
$C_i = \frac{\sum_{j=1}^n (\hat{y_j} - \hat{y_{j(i)}})^2}{(p+1) \hat{\sigma}^2}$

$= \frac{p_{ii}}{1 - p_{ii}} \frac{r_i^2}{p + 1}$ <!-- 간편계산법 -->

$\hat{y_{j(i)}}$: l.s. fit deleting the $i^{th}$ data point ($y_i; x_{i1}, \ldots, x_{ip}$)
= Amount of changes by deleting the $i^{th}$ obs.

#### Difference in Fits (DFITS)
DFITS $= \frac{\hat{y_i} - \hat{y_{j(i)}}}{\hat{\sigma_{(i)}} \sqrt{p_{ii}}}$

$= \sqrt{\frac{p_{ii}}{1 - p_{ii}}} \cdot r_i^*$ <!-- 간편계산 -->

$r_i^* = \frac{e_i}{\sqrt{1 - p_{ii}} \hat{\sigma_{(i)}}}$

#### Hadi's measure & Potential-Residual Plot
Based on the fact that influential obs. are outlying in y’s or in x’s, or both.
<!-- Hadi: 이상점과 영향점 같이 고려 -->

$H_i = \frac{p_{ii}}{1 - p_{ii}} + \frac{p + 1}{1 - p_{ii}} \cdot \frac{d_i^2}{1 - d_i^2}$

$d_i^2 = \frac{e_i}{\sqrt{SSE}} = \frac{e_i}{\frac{n-p-1} \hat{\sigma}}$
<!-- d_i는 표준화된 잔차 -->

Potential-residual plot: plot of $(\frac{p + 1}{1 - p_{ii}} \cdot \frac{d_i^2}{1 - d_i^2}, \frac{p_{ii}}{1 - p_{ii}})$

: residual ft v.s. potential ft

<!-- x축: potential residuel, y: 영향 변화. -->

### Outlier
* Outliers in the predictors - leverage
* Outliers in the response variable - standardized (studentized) residual
