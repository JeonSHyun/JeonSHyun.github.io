---
title: Support Vector Machines and Flexible Discriminants
author: JSH
date: 2024-08-27 10:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, ESL]
use_math: true
---
# Support Vector Machines and Flexible Discriminants

## Introduction
### Hyperplane
A hyperplane is a $p$-dimensional space is a flat affine subspace of dimension $p-1$.

In general, the equation for a hyperplane has the form $\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p = 0$.
$\beta = (\beta_1, \beta_2, \ldots, \beta_p)$ is called normal vector, which is pointing in a direction orthogonal to the surface of hyperplane.
The hyperplane separates the space into two: $\beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p > 0, \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p < 0$.

### Maximal Margin Classifier
Among all separating hyperplanes, find the one that makes the biggest gap or margin between the two classes.

A constraint optimization problem:

$Maximize_{\beta_0, \ldots, \beta_p} M$ subject to $y_i(\beta_0, \beta_1 x_{i1} + \ldots + \beta_p x_{ip} \geq M$, for all $i = 1, \ldots, n$

## Support Vector Machines for Linearly Separable Cases
### Binary Classification
We setup a binary classification, where $y_i \in \\{+1, -1 \\}$

#### Margin-based classifier
$\hat{y} = sign(\beta^T x) = sign(\sum_{j=1}^p \beta_j x_j)$
* If the angle between $\beta$ and $x$ is $<90 \degree, \beta^T x > 0 \rightarrow \hat{y} = +1$
* If the angle between $\beta$ and $x$ is $>90 \degree, \beta^T x < 0 \rightarrow \hat{y} = -1$
* If the angle between $\beta$ and $x$ is $=90 \degree$, we can't decide

#### Proof
$x = x_{\perp} + x_{\parallel}$

$\beta^T x = \beta^T x_{\perp} + \beta^T x_{\parallel} = \beta^T(\frac{r}{\mid \beta \mid} \cdot \beta) = \mid \beta \mid (\frac{r}{\mid \beta \mid} \cdot \beta) \cos 0 = r \mid \beta \mid$

$r = \frac{\beta^T x}{\mid \beta \mid}$

### Maximal Margin Classifier
Margin is the shortest distance between a data point and the decision boundary: $min_{i = 1, \ldots, n} y_i \frac{\beta^T x_i}{\mid \beta \mid}$.

We aim to find the maximum likelihood estimator of this: $\arg \max_{\beta} (\min_{i = 1, \ldots, n} \frac{1}{\mid \beta \mid} y_i \beta^T x_i)$

### Primal Problem


