---
title: Support Vector Machines and Flexible Discriminants
author: JSH
date: 2024-08-27 10:01:00 +0800
categories: [Study, Statistics]
tags: [Study, Statistics, ESL]
use_math: true
---
# Support Vector Machines and Flexible Discriminants

## Introduction
### Hyperplane
A hyperplane is a $p$-dimensional space is a flat affine subspace of dimension $p-1$.

In general, the equation for a hyperplane has the form $\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p = 0$.
$\beta = (\beta_1, \beta_2, \ldots, \beta_p)$ is called normal vector, which is pointing in a direction orthogonal to the surface of hyperplane.
The hyperplane separates the space into two: $\beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p > 0, \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p < 0$.

### Maximal Margin Classifier
Among all separating hyperplanes, find the one that makes the biggest gap or margin between the two classes.

A constraint optimization problem:

$Maximize_{\beta_0, \ldots, \beta_p} M$ subject to $y_i(\beta_0, \beta_1 x_{i1} + \ldots + \beta_p x_{ip} \geq M$, for all $i = 1, \ldots, n$

## Support Vector Machines for Linearly Separable Cases
### Binary Classification
We setup a binary classification, where $y_i \in \\{+1, -1 \\}$

#### Margin-based classifier
$\hat{y} = sign(\beta^T x) = sign(\sum_{j=1}^p \beta_j x_j)$
* If the angle between $\beta$ and $x$ is $<90 \degree, \beta^T x > 0 \rightarrow \hat{y} = +1$
* If the angle between $\beta$ and $x$ is $>90 \degree, \beta^T x < 0 \rightarrow \hat{y} = -1$
* If the angle between $\beta$ and $x$ is $=90 \degree$, we can't decide

#### Proof
$x = x_{\perp} + x_{\parallel}$

$\beta^T x = \beta^T x_{\perp} + \beta^T x_{\parallel} = \beta^T(\frac{r}{\mid \beta \mid} \cdot \beta) = \mid \beta \mid (\frac{r}{\mid \beta \mid} \cdot \beta) \cos 0 = r \mid \beta \mid$

$r = \frac{\beta^T x}{\mid \beta \mid}$

### Maximal Margin Classifier
Margin is the shortest distance between a data point and the decision boundary: $min_{i = 1, \ldots, n} y_i \frac{\beta^T x_i}{\mid \beta \mid}$.

We aim to find the maximum likelihood estimator of this: $\arg \max_{\beta} (\min_{i = 1, \ldots, n} \frac{1}{\mid \beta \mid} y_i \beta^T x_i)$

### Primal Problem
An inner product of two vectors does not change its sign even if we multiply a positive constant $\alpha$ to one of the input vectors.
So, let's take advantage of the fact that $sign(\beta^T x) = sign(\beta^T (\alpha x))$.
Using this, we rescale $\beta$ such that $y_i \beta^T x_i \geq 1$ for all data points $(x_i, y_i)$.

Then, our potimization problem changes: $\arg \max_{\beta} (\min_{i = 1, \ldots, n} \frac{1}{\mid \beta \mid} y_i \beta^T x_i) \Rightarrow \arg \max_{\beta} \frac{1}{\mid \beta \mid} \Rightarrow \arg \min_{\beta} \mid \beta \mid \Rightarrow \arg \min_{\beta} \frac{1}{2} \mid \beta \mid^2$

### Lagrange Multiplier
We can find the optimum of a function $f(x)$ subjected to the equality constraint $g(x) = 0$ by finding the stationary points of $\mathcal{L}$ considered as a function of $x$ and the Lagrange multiplier $\lambda \geq 0$.

$\mathcal{L}(x, \lambda) = f(x) + \sum_{i = 1} \lambda_i \cdot g_i(x)$.

That is, we aim to find $x$ and $\lambda$ that make the partial derivatives to zero: $\frac{\partial \mathcal{L}}{\partial x} = \frac{\partial \mathcal{L}}{\partial \lambda} = 0$

#### Expansion
$\arg \min_{\beta} \frac{1}{2} \mid \beta \mid^2$ s.t $y_i \beta^T x_i \geq 1$

$\mathcal{L} (\beta, \lambda) = \frac{1}{2} \mid \beta \mid^2 - \sum_{i = 1}^n \lambda_i (y_i \beta^T x_i - 1)$ s.t $\lambda_i \geq 0$

$\frac{\partial \mathcal{L}}{\partial \beta_j} = 0 \rightarrow \beta_j - \sum_{i = 1}^n \lambda_i y_i x_{i, j} = 0 \rightarrow \beta_j = \sum_{i = 1}^n \lambda_i y_i x_{i, j}$

$\mathcal{L} (\beta, \lambda) = \frac{1}{2} (\sum_{i = 1}^n \lambda_i y_i x_{i, j}, \sum_{i = 1}^n \lambda_i y_i x_{i, j}) - \sum_{i = 1}^n \lambda_i (y_i (\sum_{i = 1}^n \lambda_k y_k x_k, x_i) - 1)$
$= -\frac{1}{2}(\sum_{i = 1}^n \lambda_i y_i x_{i, j}, \sum_{i = 1}^n \lambda_i y_i x_{i, j}) + \sum_{i = 1}^n \lambda_i$

### Dual Problem
$\mathcal{L} = -\frac{1}{2}(\sum_{i = 1}^n \lambda_i y_i x_{i, j}, \sum_{i = 1}^n \lambda_i y_i x_{i, j}) + \sum_{i = 1}^n \lambda_i$

From this Lagrangian, we can rewrite our optimization problem with regards to $\lambda$.

$\hat{\lambda} = \arg \max_{\lambda}(-\frac{1}{2}(\sum_{i = 1}^n \lambda_i y_i x_{i, j}, \sum_{i = 1}^n \lambda_i y_i x_{i, j}) + \sum_{i = 1}^n \lambda_i)$ s.t $\lambda_i \geq 0$

$= \arg \max_{\lambda} (-\frac{1}{2} \sum_{i = 1}^n \sum_{k = 1}^n \lambda_i \lambda_k y_i y_k x_i^T x_k + \sum_{i = 1}^n \lambda_i)$ s.t $\lambda_i \geq 0$

* $y_i y_k$: If they actually belong to the same class, this becomes positive, and negative otherwise
* $x_i^T x_k = \mid x_i \mid \mid x_k \mid \cos \theta$: This is positive if the angle between them is within $90 \degree$, and negative otherwise
* If both terms agree (either both are positive or both are negatives), $\lambda_i$ will becomes 0 to maximize the formula
* If they do not agree, $\lambda_i$ would be > 0

Thus, $\lambda_i$ somhow indicates hardness of the example to be classified.
To be mor precise, examples on the margin have non-zero $\lambda_i$, and they are called support vector.

#### Final classifier
Our final classifier is given by:

$\hat{y} = sign(\beta^T x) = sign(\sum_{i = 1}^n \lambda_i y_i x_i, x) = sign(\sum_{i = 1}^n \lambda_i y_i x_i^T x) = sign(\sum_{i \in S^+} \lambda_i x_i^T x - \sum_{i \in S^-} \lambda_j x_j^T x)$

To classify a test point $x$, support vectors from positive class $(S^+)$ and those from negative class $(S^-)$ competes with each other.

## Support Vector Machines for Linearly non-Separable Cases

### SVM with Slack Variables
A slack variable $\xi_i$ measured how much the constraint $y_i \beta^T x_i \geq 1$ is violated.

* For non-support vectors: $\xi_i = 0$, since $y_i \beta^T x_i \geq 1$ (no violation)
* For support vectors:
  * $0 < \xi_i < 1$ if it is still within the decision boundary
  * $\xi_i \geq 1$ if it is beyond the decision boundary

#### Soft margin
$\xi_i = \max (0, 1 - y_i \beta^T x_i)$

### Primal Problem with Slack Variables
With slack variable, primal problem is:

$\arg \min_{\beta} (\frac{1}{2} \mid \beta \mid^2 + C \sum_{i=1}^n \xi_i)$ subject to $y_i \beta^T x_i \geq 1 - \xi_i$

$C$ controls the relative importance between minimizing margin and reducing misclassification rate
* With higher $C$, reducing missclassification rate is more emphasized
* With lower $C$, we aim to minimize margin more

### Dual Problem with Slack variables
Applying the Largrange multiplier to the modified primal problem, we got: $\mathcal{L} (\beta, \lambda, \mu) = \frac{1}{2} \mid \beta \mid^2 + C \sum_{i = 1}^n \xi_i - \lambda_i (y_i \beta_i^T x_i - (1 - \xi_i)) - \sum_{i = 1}^n \mu_i \xi_i$ subject to $\lambda_i, \mu_i \geq 0$

Solving for the optimal $\beta$, we get the same solution as before: $\frac{\partial \mathcal{L}}{\partial \beta_j} = 0 \rightarrow \beta_j - \sum_{i = 1}^n \lambda_i y_i x_{i, j} = 0 \rightarrow \beta_j = \sum_{i = 1}^n \lambda_i y_j x_{i, j}$

Solving for $\lambda$, we get $\hat{\lambda} = \arg \max_{\lambda} (-\frac{1}{2} \sum_{i = 1}^n \sum_{k = 1}^n \lambda_i \lambda_k y_i y_k x_i^T x_k + \sum_{i = 1}^n \lambda_i - \sum_{i = 1}^n \xi_i (\lambda_i - C) - \sum_{i = 1}^n \mu_i \xi_i)$ subject to $\lambda_i, \mu_i \geq 0$

## Nonlinear SVMs and Kernels

### Linearly Non-separable data
Even if we allow misclassification, some data cannot be classified with linear decision boundary, no matter what value of $C$.
We convert the input data to a linearly separable space (often in a higher dimensional space), using kernels.
Theoretically, we can make any dataset linearly separable, if we send it to enarly $n$-dimensional space. ($n$: # of training points)

### Kernels and Support Vector Machines
Kernel is a generalization of the inner product of the form $K(x_i, x_j')$ for vectors $x_i, x_j' \in R^p$.

Instead of directly mapping the examples $x$ to a higher dimensional space, denoted by $\Phi(x)$, we simply apply the kernel function:

$sign(\sum_{i \in S^+} \lambda_i x_i^T x - \sum_{j \in S^-} \lambda_j x_j^T x) \rightarrow sign(\sum_{i \in S^+} \lambda_i \Phi (x_i)^T \Phi (x) - \sum_{j \in S^-} \lambda_j \Phi (x_j)^T \Phi (x) \rightarrow sign(\sum_{i \in S^+} \lambda_i K(s_i, x) - \sum_{j \in S^-} \lambda_j K(x_j, x))$


### Radial Kernel
Another popular chice is the raidal kernel: $K(x_i, x_j ') = \exp (-\gamma \sum_{j = 1}^p (x_{ij} - x_{i'j})^2)$

If a test observation $x$ is far from a training observation $x_i$, $K(x, x_j)$ will be tiny.

The radial kernel has very local behavior. 
Only nearby training observations have an effect on the class label of test observation.

### Limitation
SVM is in nature designed only for binary classification
For $K > 2$ classes: One vs. All, One vs. One.
If $K$ is not too large, One vs. One is preferred.
