---
title: Fitting Models
author: JSH
date: 2025-12-06 9:03:00 +0800
categories: [Study, Artificial Intelligence]
tags: [Study, Artificial Intelligence, UDL]
use_math: true
---

# Fitting Models

## Loss function
Loss function (cost function) measures how bad model is: $L[\phi, f[x_i, \phi], \\{x_i, y_i\\}^I_{i=1}]$

or for short: $L[\phi]$.
It returns a scalar that is smaller when model maps inputs to outputs better.

In a training stage, find the parameters that minimize the loss: $\hat{\phi} = \arg \min_{\phi} L[\phi]$


## Gradient descent algorithm

Step 1. Compute the derivates of the loss with respect to the parameters:
$\frac{\partial L}{\partial \phi}$

Step 2. Update the parameters according to the rule:
$\phi \leftarrow \phi - \alpha \frac{\partial L}{\partial \phi}$,
where the positive scalar $\alpha$ determines the magnitude of the change.

## Linear regression example
### Step 1: Compute derivatives (slopes of function) with respect to the parameters
$L[\phi] = \sum_{i=1}^I l_i = \sum_{i=1}^I (f[x_i, \phi] - y_i)^2 = \sum_{i=1}^I (\phi_0 + \phi_1 x_i - y_i)^2$

$\frac{\partial L}{\partial \phi} = \frac{\partial}{\partial \phi} \sum_{i=1}^I l_i = \sum_{i=1}^I \frac{\partial l_i}{\partial \phi}$

$\frac{\partial l_i}{\partial \phi_0} = 2(\phi_0 + \phi_1 x_i - y_i)$

$\frac{\partial l_i}{\partial \phi_1} = 2x_i(\phi_0 + \phi_1 x_i - y_i)$

### Step 2: Update parameters according to rule
$\phi \leftarrow \phi - \alpha \frac{\partial L}{\partial \phi}$

$\alpha$: step size or learning rate


## Stochastic gradient descent (SGD)

Stochastic gradient descent (SGD) compute gradient based on only a single datapoint.

Minibatch-SGD compute gradient based on subset of datapoints.

One pass through the data is called an epoch.

### Full batch GD vs. Minibatch-SGD
Full batch GD: $\phi_{t+1} \leftarrow \phi_t - \alpha \sum_{i=1}^I \frac{\partial l_i[\phi_t]}{\partial \phi}$

Minibatch-SGD: $\phi_{t+1} \leftarrow \phi_t - \alpha \sum_{i \in B_t} \frac{\partial l_i[\phi_t]}{\partial \phi}$

### Properties of SGD
* Can escape from local minima
* Adds noise, but still sensible updates as based on part of data
* Less computationally expensive (per iteration)
* Doesn’t converge in traditional sense
* Learning rate schedule – decrease learning rate over time


## Momentum
Weighted sum of this gradient and previous gradient.

$m_{t+1} \leftarrow \beta \cdot m_t + (1-\beta) \sum_{i \in B_t} \frac{\partial l_i [\phi_t]}{\partial \phi}$

$\phi_{t+1} \leftarrow \phi_t - \alpha \cdot m_{t+1}$

* $m_t$ is the momentum
* $\beta \in [0, 1)$ controls degree to which the gradient is smoothed over time

With momentum, the change at the current step is a weighted combination of the previous change and the gradient computed from the batch

### Nesterov accelerated momentum

Momentum is kind of like a prediction of where we are going:
* $m_{t+1} \leftarrow \beta \cdot m_t + (1-\beta) \sum_{i \in B_t} \frac{\partial l_i[\phi_t]}{\partial \phi}$
* $\phi_{t+1} \leftarrow \phi_t - \alpha \cdot m_{t+1}$

Nesterov accelerated momentum move in the predicted direction, then measure the gradient:
* $m_{t+1} \leftarrow \beta \cdot m_t + (1-\beta) \sum_{i \in B_t} \frac{\partial l_i[\phi_t - \alpha \cdot m_t]}{\partial \phi}$
* $\phi_{t+1} \leftarrow \phi_t - \alpha \cdot m_{t+1}$


## Adaptive moment estimation (Adam)

### Normalized gradients
Measure mean and pointwise squared gradient:
* $m_{t+1} \leftarrow \frac{\partial L[\phi_t]}{\partial \phi}$
* $v_{t+1} \leftarrow (\frac{\partial L[\phi_t]}{\partial \phi})^2$

Normalize:
* $\phi_{t+1} \leftarrow \phi_t - \alpha \cdot \frac{m_{t+1}}{\sqrt{v_{t+1}} + \epsilon}$

Normalized gradient only may not converge.

### Adam
Measure mean and pointwise squared gradient:
* $m_{t+1} \leftarrow \beta \cdot m_t + (1-\beta) \frac{\partial L[\phi_t]}{\partial \phi}$
* $v_{t+1} \leftarrow \gamma \cdot v_t + (1-\gamma) (\frac{\partial L[\phi_t]}{\partial \phi})^2$

Moderate near start of the sequence:
* $\tilde{m_{t+1}} \leftarrow \frac{m_{t+1}}{1 - \beta^{t+1}}$
* $\tilde{v_{t+1}} \leftarrow \frac{v_{t+1}}{1 - \gamma^{t+1}}$

Update the parameters:
* $\phi_{t+1} \leftarrow \phi_t - \alpha \cdot \frac{\tilde{m_{t+1}}}{\sqrt{\tilde{v_{t+1}}} + \epsilon}$
