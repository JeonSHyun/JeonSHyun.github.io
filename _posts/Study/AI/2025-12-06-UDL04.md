---
title: Computing Gradients
author: JSH
date: 2025-12-06 10:03:00 +0800
categories: [Study, Artificial Intelligence]
tags: [Study, Artificial Intelligence, UDL]
use_math: true
---

# Computing Gradients

## Loss function 
Loss function (cost function) measures how bad model is: $L[\phi, f[x_i, \phi], \\{x_i, y_i\\}^I_{i=1}]$

or for short: $L[\phi]$.
It returns a scalar that is smaller when model maps inputs to outputs better.

In a training stage, find the parameters that minimize the loss: $\hat{\phi} = \arg \min_{\phi} L[\phi]$

### Problem 1: Computing gradients
* Loss: sum of individual terms - $L[\phi] = \sum_{i=1}^I l_i = \sum_{i=1}^I l[f[x_i, \phi], y_i]$
* Mini-batch SGD Algorithm: $\phi_{t+1} \leftarrow \phi_t - \alpha \sum_{i \in B_t} \frac{\partial l_i [\phi_t]}{\partial \phi}$
* Parameters: $\phi = \\{\beta_0, \Omega_0, \beta_1, \Omega_1, \beta_2, \Omega_2, \beta_3, \Omega_3 \\}$
* Need to compute gradients: $\frac{\partia l_i}{\partial \beta_k}$ and $\frac{\partia l_i}{\partial \Omega_k}$

Why is this such a big deal?
Neural network is a huge equation, and we need to compute derivative for every parameters, every point in the batch, and for every iteration of SGD.

### Problem 2: initialization
Where should we start the parameters before we commence SGD?

### Backpropagation intuition
Backpropagation algorithm: algorithm to compute gradient efficiently


## Toy function




