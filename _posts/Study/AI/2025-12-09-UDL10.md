---
title: Multi-armed Bandits
author: JSH
date: 2025-12-09 9:03:00 +0800
categories: [Study, Artificial Intelligence]
tags: [Study, Artificial Intelligence, UDL]
use_math: true
---

# Multi-armed Bandits

## Multi-armed Bandits
Multiple slot machines to choose from.

Simplification from RL: actions do not have delayed effect nor affect the state transition.

## k-armed Bandit Problem
Each of $k$ actions has an expected reward: value of action.
* $A_t$: action selected at time stpe $t$
* $R_t$: reward at time step $t$

Value of action $a$: $q_* (a) := E[R_t \mid A_t = a]$

## Action-value Methods
Natural way to estimate expected reward $q_* (a)$ is sample-average.

$Q_t(a):=$ (sum of rewards when $a$ taken prior to $t$)/(number of times $a$ taken prior to $t$ = $\frac{\sum_{i=1}^{t-1} R_i \cdot 1_{A_i = a}}{\sum_{i=1}^{t=1} 1_{A_i=a}}$

* $Q_t(a)$: estimated value of action $a$ at time stpe $t$
* May instead define $Q_t(a)$ as some default value

## Exploitation vs. Exploration
* Exploit: maximize reward given what is known.
  * Take $A_t = \arg \max_a Q_t(a)$
* Explore: collect more information for (potentially) higher reward

## Greedy vs. $\epsilon$-Greedy
Greedy action selection (exploitation):
$A_t = \arg \max_a Q_t(a)$

$\epsilon$-greedy action selection
* With probability $1-\epsilon$, take greedy action $A_t = \arg \max_a Q_t(a)$
* With probability $\epsilon$, select randomly among all actions
* Advantage: (in the limit) as $t$ increases, every action is sampled an infinite number of times

## 10-armed Testbed
Recall: $q_* (a)$ is a true expected reward of arm $a$ and $Q_t(a)$ is its estimate at time $t$

The distributions and $q_* (a)$ for each arm are unknown to the decision maker.

## Performances of $\epsilon$-Greey and Greedy Policies
$\epsilon = 0.01$ method improves more slowly, but eventually would perform better than $\epsilon = 0.1$.
It is possible to reduce $\epsilon$ over time.

Noisier rewards (high variance) takes more exploration to find the optimal action.

Question: would $\epsilon$-greedy perform better compared to a greedy policy with noisier rewards?

What about deterministic rewards (no noise)?

## Incremental Implementation
$Q_n$: estimate of action value after selected $n-1$ times.
* $Q_n := \frac{R_1 + R_2 + \ldots + R_{n-1}}{n-1}$
* Memory and computational requirements increase linearly as time increases.

Alternative: incremental updates
* $Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]$
* Only requires constant memory and computation per time step
* NewEstimate $\leftarrow$ OldEstimate + StepSize·[Target − OldEstimate]
* [Target - OldEstimate]: error in the action-value estimate

## Simple Bandit Algorithm
Incremental sample-average update + $\epsilon$-greedy action selection

### A simple bandit algorithm
Initialize, for $a=1$ to $k$:
* $Q(a) \leftarrow 0$
* $N(a) \leftarrow 0$

Loop forever:
* $A \leftarrow \arg \max_a Q(a)$ with probability $1-\epsilon$ and a random action with probability $\epsilon$
* $R \leftarrow bandit(A)$
* $N(A) \leftarrow N(A) + 1$
* $Q(A) \leftarrow Q(A) + \frac{1}{N(A)} [R - Q(A)]$


## Optimistic Initial Values
Finite estimates of action-value depend on initial action value $Q_1(a)$

It can be used as a simple way to encourage exploration.
It can set initial values to be “optimistic” (higher than achievable values).


## Upper-Confidence-Bound (UCB) Action Selection
Wouldn’t it be better to select among non-greedy actions according to their potential for actually being optimal?
* $\epsilon$-greedy action selectin ignores this aspect
* Incorporate both how close their estimates are to being optimal and uncertainties in those estimates

Upper-confidence-bound (UCB) action:
$A_t := \arg \max_a [Q_t(a) + c \sqrt{\frac{\ln t}{N_t (a)}}$
* $N_t(a)$: the number of times action $a$ has been selected before $t$
* $c$: controls the degree of exploration


## Gradient Bandit Algorithms
Learn a numerical preference $H_t(a) \in R$ for each action $a$
* Larger $H_t(a) \Rightarrow$ action $a$ is selected more often
* It has no interpretation in terms of reward

Action selection based on soft-max (Boltzmann) distribution:
$P\\{A_t = a \\} := \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} := \pi_t (a)$

* Update rule after selecting action $A_t$ and receiving the reward $R_t$
  * $H_{t+1} (A_t) := H_t (A_t) + \alpha (R_t \bar{R_t}) (1 - \pi_t (A_t))$ and
  * $H_{t+1} (a) := H_t (a) - \alpha (R_t \bar{R_t}) \pi_t (a), \forall a \neq A_t$
  * Where $\bar{R_t}$ is the average of the rewards prior to $t$

## Non-stationary Rewards
What happens when rewards of arms are changing over time?
* For stationary rewards, $\alpha_n$ (typically) decreasing function of $n$: $Q_{n+1}(a) := Q_n (a) + \alpha_n (a) [R_n (a) - Q_n (a)]$
* For non-stationary rewards, use constant step size $\alpha$, $Q_{n+1} (a) := Q_n (a) + \alpha [R_n (a) - Q_n (a)]$
