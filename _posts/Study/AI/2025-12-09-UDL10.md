---
title: Multi-armed Bandits
author: JSH
date: 2025-12-09 9:03:00 +0800
categories: [Study, Artificial Intelligence]
tags: [Study, Artificial Intelligence, UDL]
use_math: true
---

# Multi-armed Bandits

## Multi-armed Bandits
Multiple slot machines to choose from.

Simplification from RL: actions do not have delayed effect nor affect the state transition.

## k-armed Bandit Problem
Each of $k$ actions has an expected reward: value of action.
* $A_t$: action selected at time stpe $t$
* $R_t$: reward at time step $t$

Value of action $a$: $q_* (a) := E[R_t \mid A_t = a]$

## Action-value Methods
Natural way to estimate expected reward $q_* (a)$ is sample-average.

$Q_t(a):=$ (sum of rewards when $a$ taken prior to $t$)/(number of times $a$ taken prior to $t$ = $\frac{\sum_{i=1}^{t-1} R_i \cdot 1_{A_i = a}}{\sum_{i=1}^{t=1} 1_{A_i=a}}$

* $Q_t(a)$: estimated value of action $a$ at time stpe $t$
* May instead define $Q_t(a)$ as some default value

## Exploitation vs. Exploration
* Exploit: maximize reward given what is known.
  * Take $A_t = \arg \max_a Q_t(a)$
* Explore: collect more information for (potentially) higher reward

## Greedy vs. $\epsilon$-Greedy




