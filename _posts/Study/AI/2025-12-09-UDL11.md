---
title: Foundations of Reinforcement Learning
author: JSH
date: 2025-12-09 10:03:00 +0800
categories: [Study, Artificial Intelligence]
tags: [Study, Artificial Intelligence, UDL]
use_math: true
---

# Foundations of Reinforcement Learning

## Reinforcement Learning
Goal: maximize sum of future rewards

## Agent-Environment Interface
For each time $t = 0, 1, 2, \ldots,$
* agent receives state information $S_t \in S$
* agent selects an action $A_t \in A(s_t)$
* (one step later) agent receives reward $R_{t+1}$ and next state $S_{t+1}$ is determined

Interactions produce a sequence (or trajectory)
* $S_0, A_0, R_1, S_1, A_1, R_2, A_2, R_3, \ldots$

## Markov Decision Processes (MDPs)
Classical formalization of sequential decision making.

Model of the environment that the agent interacts with.

Actions influence not just immediate rewards, but also subsequent states.

## Dynamics of MDP
Dynamics function $p: S \times R \times S \times A \rightarrow [0, 1]$
* Takes 4 arguments: next state, reward, current state, action

Given particular state $s$ and action $a$, probability of next state and reward is:
$p(s', r \mid s, a) := P \\{S_t = s', R_t = r \mid S_{t-1} = s, A_{t-1} = a \\}$,
for all $s', s \in S, r \in R, a \in A(s)$


## Markov Property
The future is independent of the past given the present.

A state $s$ has the Markov property if
$P \\{S_t = s', R_t = r \mid S_{t-1} = s, A_{t-1} = a \\} = P \\{S_t = s', R_t = r \mid S_0, A_0, \ldots, S_{t-1} = s, A_{t-1} = a \\}$,
for all possible histories $\\{S_0, A_0, \ldots, S_{t-1}, A_{t-1} \\}$

* State captures all relevant information from history
* Once state is known, entire history is not needed
* State is a sufficient statistic of the past


## State-Transition and Reward Models
Can compute using dynamics $p(s', r \mid s, a)$
* State-transition: $p(s' \mid s, a):=P \\{S_t = s' \mid S_{t-1} = s, A_{t-1} = a \\} = \sum_{r \in R} p(s', r \mid s, a)$
* Expected rewards for state-action pair: $r(s, a) := E \\{R_t \mid S_{t-1} = s, A_{t-1} = a \\} = \sum_{r \in R} r \sum_{s' \in S} p(s', r \mid s, a)$
* Expected rewards for state-action-next-state triple: $r(s, a, s') := E \\{R_t \mid S_{t-1} = s, A_{t-1} = a, S_t = s' \\} = \sum_{r \in R} r \frac{p(s', r \mid s, a)}{p(s' \mid s, a)}$ 


## Boundary between Agent and Environment
Boundary in the MDP framework: not (necessarily) the same as the physical boundary of an agent (e.g., human, robot)
* Muscles, skeleton: part of the environment
* Environment in MDP framework: anything that cannot be changed arbitrarily by agent

## Reward Hypothesis
The goal of the agent is the maximization of the expected value of the cumulative sum of a received scalar (reward) signal.
Rewards need to truly indicate what needs to be accomplished (goal).

## Returns
Goal: maximize cumulative reward the agent receives in long run.

(Formally) maximize the return $G_t$
* (Episodic) $G_t := R_{t+1} + R_{t+2} + R_{t+3} + \ldots + R_T$
* (Continuing) $G_t := R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t_k+1}$, where $\gamma \in [0, 1]$ is the discount rate

It can combine episodic and continuing tasks by considering the absorbing state:
$G_t := \sum_{k = t+1}^T \gamma^{k - t - 1} R_k$ (possibly $T = \infty$ or $\gamma = 1$)


## Policies and Value Functions
Policy: mapping from state to possible action
* $\pi(a \mid x)$ at time $t$: probability that $A_t = a$ given $S_t = s$
* Deterministic policy $\pi (a' \mid s) = 1$ for some $a'$ and $\pi(a \mid s) = 0$ for all $a \neq a'$

Value function: estimates how good state (or state-action pair) is
* State-value function under policy $pi$: $v_{\pi} (s) := E_{\pi} [G_t \mid S_t = s] = E_{\pi} [\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s]$
* Action-value function under policy $\pi$: $q_{\pi} (s, a) := E_{\pi} [G_t \mid S_t = s, A_t = a] = E_{\pi} [\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s, A_t = a]$


## Bellman Equation
Bellman equations shows relationship between the value of a state and the values of its successor states.

$v_{\pi} (s) = E_{\pi} [G_t \mid S_t = s] = E_{\pi} [R_{t+1} + \gamma G_{t+1} \mid S_t = s] = \sum_a \pi(a \mid s) \sum_{s'} \sum_r p(s', r \mid s, a) [r + \gamma E_{\pi} [G_{t+1} \mid S_{t+1} = s']] = \sum_a \pi (a \mid s) \sum_{s'} \sum_r p(s', r \mid s, a)[r + \gamma v_{\pi} (s')]$

## Bellman Backups
$v_{\pi} (s) = \sum_a \pi (a \mid s) \sum_{s'} \sum_r p(s', r \mid s, a)[r + \gamma v_{\pi} (s')]$

Averages over all the possibilities of subsequent states and rewards.
Transfer value information of leaf states back to the root state.

## Optimal Policies and Optimal Value Functions
Optimal policy: better than or equal to all other policies
* A policy $\pi$ is better than $\pi'$ if $v_{\pi} (s) \geq v_{\pi'} (s)$ for all $s \in S$

Optimal state-value function:
$v_* (s) := \max_{\pi} v_{\pi} (s)$ for all $s \in S$

Optimal action-value function:
$q_* (s, a) := \max_{\pi} q_{\pi} (s, a)$ for all $s \in S, a \in A(s)$

We can write $q* $ in terms of $v* $: 
$q* (s, a) = E[R_{t+1} + \gamma v_* (S_{t+1}) \mid S_t = s, A_t = a]$


## Bellman Optimality Equation
Bellman equation for $v*$:
$v_*$ $(s) = \max_{a \in A(s)} q_{\pi *} (s, a) = \max_a E_{\pi *} [G_t \mid S_t = s, A_t = a] = \max_a E[R_{t+1} + \gamma v * (S_{t+1}) \mid S_t = s, A_t = a]$
$= \max_a \sum_{s', r} p(s', r \mid s, a) [r + \gamma v * (s')]$

The value of a state under an optimal policy must equal the expected return for the best action from that state.

Difference from Bellman backup diagram: max operations.

Bellman optimality equation has a unique solution for $v *$:
$v *(s) = \max_a \sum_{s', r} p(s', r \mid s, a) [r + \gamma v * (s')]$

Once one has $v *$, it is easy to determine an optimal policy
* any policy greedy with respect to $v *$ is an optimal policy
* having $q *$ is easier for finding optimal policy


## Policy Iteration

### Algorithm
Policy Iteration (using iterative policy evaluation) for estimating $\pi \approx \pi*$
* Initialization: $V(s) \in R$ and $\pi(s) \in A(s)$ arbitrarily for all $s \in S$
* Policy Evaluation
  * Loop:
    * $\delta \leftarrow 0$
    * Loop for each $s \in S$
      * $v \leftarrow V(s)$
      * $V(s) \leftarrow \sum_{s', r} p(s', r \mid x, \pi(s))[r + \gamma V(s')]$
      * $\delta \leftarrow \max (\delta, \mid v - V(s) \mid )$
  * until $\delta < \theta$ (a small positive number determining the accuracy of estimation)
* Policy Improvement
  * polity-stable $\leftarrow$ true
  * For each $s \in S$:
    * old-action $\leftarrow \pi(s)$
    * $\pi(s) \arg \max_a \sum_{s', r} p(s', r \mid s, a)[r + \delta V(s')]$
    * If old-action $\neq \pi(s)$, then policy-stable $\leftarrow$ false
  * If policy-stable, then stop and returen $V \approx v*$ and $\pi \approx \pi *$; else go to Policy Evaluation


## Value Iteration
Drawback of policy iteration: each iteration involves policy evaluation, requiring multiple sweeps through the state set.

Value iteration: evaluate policy for one sweep.
It does not evaluate $v_{\pi}$ fully for given $\pi$.

### Algorithm
Value Iteration, for estimating $\pi \approx \pi*$

Algorithm parameter: a small threshold $\theta > 0$ determining accuracy of estimaion.

* Initialize $V(s)$, for all $s \in S+$, arbitrarily except that $V(terminal) = 0$
* Loop
  * $\delta \leftarrow 0$
  * Loop for each $s \in S$:
    * $v \leftarrow V(s)$
    * $V(s) \leftarrow \max_a \sum_{s', r} p(s', r \mid s, a)[r + \gamma V(s')]$
    * $\delta \leftarrow \max (\delta, \mid v - V(s) \mid)$
* Until $\delta < \theta$
* Output a deterministic policy, $\pi \approx \pi *$, such that $\pi(s) = \arg \max_a \sum_{s', r} p(s', r \mid s, a)[r + \gamma V(s')]$


## Caveat
The optimal solution we have seen relies on at least the following assumptions:
* States have the Markov property
* The dynamics of the environment are accurately known

In practice, some kind of approximate solution is needed.

How do we find an optimal policy when dynamics are not known?

## Sarsa: On-policy TD Control
Learn an action-value function $q_{\pi} (s, a)$ rather than a state-value function $v_{\pi} (s)$.

Sarsa update for action values:
$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$

* $R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$: temporal difference (TD) error

Update is done after every transition from a nonterminal state.


### Sarsa Pseudocode
Estimate $q_{\pi} for the behavior policy $\pi$, and at the same time change $\pi$ toward greediness with respect to $q_{\pi}$

Sarsa (on-policy TD control) for estimating $Q \approx q*$

Algorithm parameters: step size $\alpha \in (0, 1]$, small $\epsilon > 0$

* Initialize $Q(s, a)$, for all $s \in S+, a \in A(s)$, arbitrarily except that $Q(terminal, \cdot) = 0$
* Loop for each episode:
  * Initialize $S$
  * Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
  * Loop for each step of episode:
    * Take action $A$, observe $R, S'$
    * Choose $A'$ from $S'$ using policy derived from Q (e.g., $\epsilon$-greedy)
    * $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma Q(S', A') - Q(S, A)]$
    * $S \leftarrow S'; A \leftarrow A';$
  * until $S$ is terminal

## Q-learning: Off-policy TD Control
Q-learning update rule:
$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$

c.f., compare with Sarsa update:
$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$

Approximates $q*$, the optimal action-value function, independent of the policy bing followed.

* Requirement: all pairs must continue to be updated. (e.g., use $\epsilon$-greedy policy)

The policy still has an effect in that it determines which state–action pairs are visited and updated.

But as long as all pairs continue to be updated, correct convergence is guranteed.

### Q-learning Pseudocode
Q learning (off-policy TD control) for estimating $\pi \approx \pi *$

Algorithm parameters: step size $\alpha \in (0, 1]$, small $\epsilon > 0$

* Initialize $Q(s, a)$ for all $s \in S+, a \in A(s)$, arbitrarilyi except that $Q(terminal, \cdot) = 0$
* Loop for each episode:
  * Initialize $S$
  * Loop for each step of episode:
    * Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
    * Take action $A$, observe $R, S'$
    * $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \max_a Q(S', a) - Q(S, A)]$
    * $S \leftarrow S'$
  * until $S$ is terminal

### Why is Q-learning Off-policy?
Off-policy: evaluate or improve a target policy from data generated by a different behavior policy

The target policy $\pi$ is greedy w.r.t $Q(S, A): \pi (a \mid S_t) = \arg \max_{a'} Q(S_t, a')$

The behavior policy $b$ is not the same as $\pi$ e.g., $\epsilon$-greedy.

Q-learning target for $A' \sim \pi(\cdot \mid S_t)$:

$R_{t+1} + \gamma Q(S_{t+1}, A') = R_{t+1} + \gamma Q(S_{t+1}, \arg \max_{a'} Q(S_{t+1}, a')) = R_{t+1} + \gamma \max_{a'} Q(R_{t+1}, a')$


### Sarsa vs. Q-learning: Cliff Walking Example
Although Q-learning actually learns the values of the optimal policy, its online performance is worse than that of Sarsa.

What if $\epsilon$ were gradually reduced?


## Large-Scale Reinforcement Learning
Reinforcement learning can be used to solve large problems.
How do we scale up the RL methods for large-scale problems?

### Value Function Approximation
Extend tabular methods to problems with arbitrary large state spaces.

Cannot put every state in a table and just record the associated reward.
* All the state space do not fit in memory.
* Even if it did, we don’t have the time to fill our table.

Use a function approximator that will take the state in input and output the value of the state.

Generalization issue: want our function approximator to generalize the state space.
If we get information about one state, can also be useful for similar states.

### Q-learning with function approximation
Approximate $Q(s, a)$ with some $Q_w (s, a) = f_w (s(s, a)) \approx Q(s, a)$ where $x(s, a) \in R^d$ is a feature vector of $(s, a)$

e.g., linear function with parameter $w \in R^d: Q_w (s, a) = x(s, a)^T w$

Instead of learning the $\mid S \mid \times \mid A \mid$ dimentional Q-table, the Q-learning algorithm will learn the parameter $w$.

Consider the loss function: mean-squared error in Q-values.

$l_w (s, a) = E_{s' \sim P(\cdot \mid s, a)} (Q_w (s, a) - R(s, a, s') - \gamma \max_{a'} Q_w (s', a'))^2 =: E_{s' \sim P(\cdot \mid s, a)} [l_w (s, a, s')]$

where $l_w (s, a, s') = (Q_w (s, a) - target(s'))^2$ and $target(s') = R(s, a, s') + \gamma \max{a'} Q_w (s', a')$

Fundamentally, Q-learning with function approximation uses gradient descent to optimize this loss function given sample (target) observations.

### Algorithm: Q-learning with function approximation
* Take an action $a$ following $\pi (\cdot \mid s)$
* Observe reward $r$, transition to $s' \sim P(\cdot \mid s, a)$
* Update $w_{t+1} \leftarrow w_t - \alpha_t \nabla_{w_t} l_{w_t} (s, a, s')$, where $\nabla_{w_t} l_{w_t} (s, a, s') = -\delta_t \nabla_{w_t} Q_{w_t} (s, a)$ and $\delta_t = r + \gamma \max{a'} Q_{w_t} (s', a') - Q_{w_t} (s, a)$
* $s \leftarrow s'$ and repeat from 1

### Deep Q-Network (DQN)
Use deep neural network to represent Q-function.

$Q_w (s, a) = f_w (x(s, a))$ where $f_w (\cdot)$ is a neural network and w are weights of the network.

## Why $\epsilon$-greedy fails
Optimal policy: go all the way to the rightmost state.

Then, $H$ steps to the optimal solution: $1/e^H$ timesteps are needed.

### Exploitation vs. Exploration
* Exploitation: Maximize rewards given what you already know
* Exploration: Collect more information (hoping for larger reward)
* Exploitation = optimization vs. Exploration = learning

### Upper Confidence Bound (UBC) Approach
Construct optimistic values for each state and action (with h.p.).
Act greedily with respect to the optimistic model.

Overtime, the amount of optimism gets shrunken.
Good because converging to the true value.
Finds the optimal policy with polynomial samples.

