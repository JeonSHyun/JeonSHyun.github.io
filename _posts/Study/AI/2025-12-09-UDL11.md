---
title: Foundations of Reinforcement Learning
author: JSH
date: 2025-12-09 10:03:00 +0800
categories: [Study, Artificial Intelligence]
tags: [Study, Artificial Intelligence, UDL]
use_math: true
---

# Foundations of Reinforcement Learning

## Reinforcement Learning
Goal: maximize sum of future rewards

## Agent-Environment Interface
For each time $t = 0, 1, 2, \ldots,$
* agent receives state information $S_t \in S$
* agent selects an action $A_t \in A(s_t)$
* (one step later) agent receives reward $R_{t+1}$ and next state $S_{t+1}$ is determined

Interactions produce a sequence (or trajectory)
* $S_0, A_0, R_1, S_1, A_1, R_2, A_2, R_3, \ldots$

## Markov Decision Processes (MDPs)
Classical formalization of sequential decision making.

Model of the environment that the agent interacts with.

Actions influence not just immediate rewards, but also subsequent states.

## Dynamics of MDP
Dynamics function $p: S \times R \times S \times A \rightarrow [0, 1]$
* Takes 4 arguments: next state, reward, current state, action

Given particular state $s$ and action $a$, probability of next state and reward is:
$p(s', r \mid s, a) := P \\{S_t = s', R_t = r \mid S_{t-1} = s, A_{t-1} = a \\}$,
for all $s', s \in S, r \in R, a \in A(s)$


## Markov Property
The future is independent of the past given the present.

A state $s$ has the Markov property if
$P \\{S_t = s', R_t = r \mid S_{t-1} = s, A_{t-1} = a \\} = P \\{S_t = s', R_t = r \mid S_0, A_0, \ldots, S_{t-1} = s, A_{t-1} = a \\}$,
for all possible histories $\\{S_0, A_0, \ldots, S_{t-1}, A_{t-1} \\}$

* State captures all relevant information from history
* Once state is known, entire history is not needed
* State is a sufficient statistic of the past


## State-Transition and Reward Models
Can compute using dynamics $p(s', r \mid s, a)$
* State-transition: $p(s' \mid s, a):=P \\{S_t = s' \mid S_{t-1} = s, A_{t-1} = a \\} = \sum_{r \in R} p(s', r \mid s, a)$
* Expected rewards for state-action pair: $r(s, a) := E \\{R_t \mid S_{t-1} = s, A_{t-1} = a \\} = \sum_{r \in R} r \sum_{s' \in S} p(s', r \mid s, a)$
* Expected rewards for state-action-next-state triple: $r(s, a, s') := E \\{R_t \mid S_{t-1} = s, A_{t-1} = a, S_t = s' \} = \sum_{r \in R} r \frac{p(s', r \mid s, a)}{p(s' \mid s, a)}$ 


## Boundary between Agent and Environment
Boundary in the MDP framework: not (necessarily) the same as the physical boundary of an agent (e.g., human, robot)
* Muscles, skeleton: part of the environment
* Environment in MDP framework: anything that cannot be changed arbitrarily by agent

## Reward Hypothesis
The goal of the agent is the maximization of the expected value of the cumulative sum of a received scalar (reward) signal.
Rewards need to truly indicate what needs to be accomplished (goal).

## Returns
Goal: maximize cumulative reward the agent receives in long run.

(Formally) maximize the return $G_t$
* (Episodic) $G_t := R_{t+1} + R_{t+2} + R_{t+3} + \ldots + R_T$
* (Continuing) $G_t := R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t_k+1}$, where $\gamma \in [0, 1]$ is the discount rate

It can combine episodic and continuing tasks by considering the absorbing state:
$G_t := \sum_{k = t+1}^T \gamma^{k - t - 1} R_k$ (possibly $T = \infty$ or $\gamma = 1$)


## Policies and Value Functions
Policy: mapping from state to possible action
* $\pi(a \mid x)$ at time $t$: probability that $A_t = a$ given $S_t = s$
* Deterministic policy $\pi (a' \mid s) = 1$ for some $a'$ and $\pi(a \mid s) = 0$ for all $a \neq a'$

Value function: estimates how good state (or state-action pair) is
* State-value function under policy $pi$: $v_{\pi} (s) := E_{\pi} [G_t \mid S_t = s] = E_{\pi} [\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s]$
* Action-value function under policy $\pi$: $q_{\pi} (s, a) := E_{\pi} [G_t \mid S_t = s, A_t = a] = E_{\pi} [\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s, A_t = a]$


## Bellman Equation
Bellman equations shows relationship between the value of a state and the values of its successor states.

$v_{\pi} (s) = E_{\pi} [G_t \mid S_t = s] = E_{\pi} [R_{t+1} + \gamma G_{t+1} \mid S_t = s] = \sum_a \pi(a \mid s) \sum_{s'} \sum_r p(s', r \mid s, a) [r + \gamma E_{\pi} [G_{t+1} \mid S_{t+1} = s']] = \sum_a \pi (a \mid s) \sum_{s'} \sum_r p(s', r \mid s, a)[r + \gamma v_{\pi} (s')]$

## Bellman Backups
$v_{\pi} (s) = \sum_a \pi (a \mid s) \sum_{s'} \sum_r p(s', r \mid s, a)[r + \gamma v_{\pi} (s')]$

Averages over all the possibilities of subsequent states and rewards.
Transfer value information of leaf states back to the root state.

## Optimal Policies and Optimal Value Functions
Optimal policy: better than or equal to all other policies
* A policy $\pi$ is better than $\pi'$ if $v_{\pi} (s) \geq v_{\pi'} (s)$ for all $s \in S$

Optimal state-value function:
$v_* (s) := \max_{\pi} v_{\pi} (s)$ for all $s \in S$

Optimal action-value function:
$q_* (s, a) := \max_{\pi} q_{\pi} (s, a)$ for all $s \in S, a \in A(s)$

We can write $q* $ in terms of $v* $: 
$q* (s, a) = E[R_{t+1} + \gamma v_* (S_{t+1}) \mid S_t = s, A_t = a]$


## Bellman Optimality Equation
Bellman equation for $v*$:
$v_*$ $(s) = \max_{a \in A(s)} q_{\pi *} (s, a) = \max_a E_{\pi *} [G_t \mid S_t = s, A_t = a] = \max_a E[R_{t+1} + \gamma v * (S_{t+1}) \mid S_t = s, A_t = a]$
$= \max_a \sum_{s', r} p(s', r \mid s, a) [r + \gamma v * (s')]$

The value of a state under an optimal policy must equal the expected return for the best action from that state.

Difference from Bellman backup diagram: max operations.

Bellman optimality equation has a unique solution for $v *$:
$v *(s) = \max_a \sum_{s', r} p(s', r \mid s, a) [r + \gamma v * (s')]$

Once one has $v *$, it is easy to determine an optimal policy
* any policy greedy with respect to $v *$ is an optimal policy
* having $q *$ is easier for finding optimal policy
