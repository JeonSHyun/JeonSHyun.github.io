---
title: Deep Neural Network
author: JSH
date: 2025-12-05 9:03:00 +0800
categories: [Study, AI]
tags: [Study, AI, UDL]
use_math: true
---

# Deep Neural Network

## Composing two networks
Network 1: $y = \phi_0 + \phi_1 h_1 + \phi_2 h_2 + \phi_3 h_3$
* $h_1 = a[\theta_{10} + \theta_{11} x]$
* $h_2 = a[\theta_{20} + \theta_{21} x]$
* $h_3 = a[\theta_{30} + \theta_{31} x]$

Network 2: $y' = \phi_0' + \phi_1' h_1' + \phi_2' h_2' + \phi_3' h_3'$
* $h_1' = a[\theta_{10}' + \theta_{11}' y]$
* $h_2' = a[\theta_{20}' + \theta_{21}' y]$
* $h_3' = a[\theta_{30}' + \theta_{31}' y]$

### Comparing to shallow with six hidden units
Deep neural network: 20 parameters with at least 9 regions.

Shallow network: 19 parameters with max 7 regions.


## Combining the two networks into one
Network 1: $y = \phi_0 + \phi_1 h_1 + \phi_2 h_2 + \phi_3 h_3$
* $h_1 = a[\theta_{10} + \theta_{11} x]$
* $h_2 = a[\theta_{20} + \theta_{21} x]$
* $h_3 = a[\theta_{30} + \theta_{31} x]$

Network 2: $y' = \phi_0' + \phi_1' h_1' + \phi_2' h_2' + \phi_3' h_3'$
* $h_1' = a[\theta_{10}' + \theta_{11}' y]$
* $h_2' = a[\theta_{20}' + \theta_{21}' y]$
* $h_3' = a[\theta_{30}' + \theta_{31}' y]$

Hidden units of second network in therms of first:
* $h_1' = a[\theta_{10}' + \theta_{11}' y] = a[\theta_{10}' + \theta_{11}'\phi_0 + \theta_{11}'\phi_1 h_1 + \theta_{11}' \phi_2 h_2 + \theta_{11}' \phi_3 h_3]$
* $h_2' = a[\theta_{20}' + \theta_{21}' y] = a[\theta_{20}' + \theta_{21}'\phi_0 + \theta_{21}'\phi_1 h_1 + \theta_{21}' \phi_2 h_2 + \theta_{21}' \phi_3 h_3]$
* $h_3' = a[\theta_{30}' + \theta_{31}' y] = a[\theta_{30}' + \theta_{31}'\phi_0 + \theta_{31}'\phi_1 h_1 + \theta_{31}' \phi_2 h_2 + \theta_{31}' \phi_3 h_3]$

### Create new variables
* $h_1' = a[\psi_{10} + \psi_{11} h_1 + \psi_{12} h_2 + \psi_{13} h_3]$
* $h_2' = a[\psi_{20} + \psi_{21} h_1 + \psi_{22} h_2 + \psi_{23} h_3]$
* $h_3' = a[\psi_{30} + \psi_{31} h_1 + \psi_{32} h_2 + \psi_{33} h_3]$

### Two-layer network
$y' = \phi_0' + \phi_1' h_1' + \phi_2' h_2' + \phi_3' h_3'$

* $h_1 = a[\theta_{10} + \theta_{11} x]$
* $h_2 = a[\theta_{20} + \theta_{21} x]$
* $h_3 = a[\theta_{30} + \theta_{31} x]$
* $h_1' = a[\psi_{10} + \psi_{11} h_1 + \psi_{12} h_2 + \psi_{13} h_3]$
* $h_2' = a[\psi_{20} + \psi_{21} h_1 + \psi_{22} h_2 + \psi_{23} h_3]$
* $h_3' = a[\psi_{30} + \psi_{31} h_1 + \psi_{32} h_2 + \psi_{33} h_3]$

When considering the pre-activations at the second hidden units ($\psi_{10} + \psi_{11} h_1 + \psi_{12} h_2 + \psi_{13} h_3$), it is a one-layer network with three outputs. <!-- h1 -> h1', h2', h3' -->

Therefore, $y' = \phi_0' + \phi_1' a[\psi_{10} + \psi_{11} a[\theta_{10} + \theta_{11} x] + \psi_{12} a[\theta_{20} + \theta_{21} x] + \psi_{13} a[\theta_{30} + \theta_{31} x]] + \phi_2' a[\psi_{20} + \psi_{21} a[\theta_{10} + \theta_{11} x] + \psi_{22} a[\theta_{20} + \theta_{21} x] + \psi_{23} a[\theta_{30} + \theta_{31} x]] + \phi_3' a[\psi_{30} + \psi_{11} a[\theta_{10} + \theta_{11} x] + \psi_{32} a[\theta_{20} + \theta_{21} x] + \psi_{33} a[\theta_{30} + \theta_{31} x]]$ 

## Hyperparameters
Hyperparameters are chosen before training the network.

We can try retraining with different hyperparameters - hyperparameter optimization or hyperparameter search.

* K layers = depth of network
* $D_k$ hidden units per layer = width of network


## Notation change and general case
$h_K = a[\beta_K + \Omega_K h_{K-1}]$, where $\beta_K$ is bias vector and $\Omega_K$ is weight matrix.

* $h_1 = a[\beta_0 + \Omega_0 x]$
* $h_2 = a[\beta_1 + \Omega_1 h_1]$
* $\ldots$
* $h_K = a[\beta_{K-1} + \Omega_{K-1} h_{K-1}]$
* $y = \beta_K + \Omega_K h_K$

Therefore, $y = \beta_K + \Omega_K a[\beta_{K_1} + \Omega_{K-1} a[ \ldots \beta_2 + \Omega_2 a[\beta_1 + \Omega_1 a[\beta_0 + \Omega_0 x]] \ldots ]]$


## Shallow vs. deep networks
1. Ability to approximate different functions
* Both obey the universal approximation theorem
* Argument: One layer is enough, and for deep networks could arrange for the other layers to compute the identity function
2. Number of linear regions per parameters
* Deep networks create many more regions per parameters
* But there are dependencies between them - folding example
3. Depth efficiency
* Depth efficienty of deep networks: There are some functions that require a shallow network with exponentially more hidden units than a deep network to achieve an equivalent approximation
4. Large structured networks
* e.g., convolutional networks for image dataset
5. Fitting and generalization
* Fitting of deep models seems to be easier up to about 20 layers
* Then needs various tricks to train deeper networks, so (in vanilla form), fitting becomes harder
* Generalization is good in deep networks
