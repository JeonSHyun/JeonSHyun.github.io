---
title: Transformers
author: JSH
date: 2025-12-06 13:03:00 +0800
categories: [Study, Artificial Intelligence]
tags: [Study, Artificial Intelligence, UDL]
use_math: true
---

# Transformers

## Motivation
Design neural network to encode and process text.
* There must be connections between the words
* The strength of these connections will depend on the words themselves

### Standard fully-connected layer
Problem:
* A very large number of parameters
* Can’t cope with text of different lengths

Conclusion:
* We need a model where parameters don’t increase with input length

## Dot-product self-attention
Takes $N$ inputs of size $D \times 1$ and returns $N$ inputs of size $D \times 1$.

Computes $N$ values (no ReLU): $v_n = \beta_v + \Omega_v x_n$

$N$ outputs are weighted sums of these values: $sa[x_n] = \sum_{m=1}^N a[x_n, x_m] v_m$

Weights depend on the inputs themselves.

### Attention weights
Compute $N$ "queries" and $N$ "keys" from input
* $q_n = \beta_q + \Omega_q x_n$
* $k_n = \beta_k + \Omega_k x_n$

Calculate similarity and pass through softmax:
* $a[x_n, x_m] = softmax_m [sim[k_m q_n]] = \frac{\exp [sim[k_m q_n]]}{\sum_{m'=1}^N \exp [sim[k_m' q_n]]}$

Take dot products and pass through softmax:
* $a[x_n, x_m] = softmax_m [k_m^T q_n] = \frac{\exp [k_m^T q_n]}{\sum_{m'=1}^N \exp [k_m^T q_n]}$

### Dot product (measure of similarity)



