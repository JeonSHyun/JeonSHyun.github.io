---
title: Transformers
author: JSH
date: 2025-12-06 13:03:00 +0800
categories: [Study, Artificial Intelligence]
tags: [Study, Artificial Intelligence, UDL]
use_math: true
---

# Transformers

## Motivation
Design neural network to encode and process text.
* There must be connections between the words
* The strength of these connections will depend on the words themselves

### Standard fully-connected layer
Problem:
* A very large number of parameters
* Can’t cope with text of different lengths

Conclusion:
* We need a model where parameters don’t increase with input length

## Dot-product self-attention
Takes $N$ inputs of size $D \times 1$ and returns $N$ inputs of size $D \times 1$.

Computes $N$ values (no ReLU): $v_n = \beta_v + \Omega_v x_n$

$N$ outputs are weighted sums of these values: $sa[x_n] = \sum_{m=1}^N a[x_n, x_m] v_m$

Weights depend on the inputs themselves.

### Attention weights
Compute $N$ "queries" and $N$ "keys" from input
* $q_n = \beta_q + \Omega_q x_n$
* $k_n = \beta_k + \Omega_k x_n$

Calculate similarity and pass through softmax:
* $a[x_n, x_m] = softmax_m [sim[k_m q_n]] = \frac{\exp [sim[k_m q_n]]}{\sum_{m'=1}^N \exp [sim[k_m' q_n]]}$

Take dot products and pass through softmax:
* $a[x_n, x_m] = softmax_m [k_m^T q_n] = \frac{\exp [k_m^T q_n]}{\sum_{m'=1}^N \exp [k_m^T q_n]}$

### Dot product (measure of similarity)
$x^T y = \mid x \mid \cdot \mid y \mid \cdot \theta$
* Similar vectors: angle $\theta$ close to 0, $\cos(\theta)$ close to 1
* Orthogonal vectors: angle $\theta$ close to 90, $\cos(\theta)$ close to 0
* Opposite vectors: angle $\theta$ close to 180, $\cos(\theta)$ close to -1

### Self-attention summary
No activation function but mechanism is nonlinear due to the dot-product and a softmax operation.

Shared set of parameters: parameters don't increase with input length.

There must be connections between the words.
The strength of these connections will depend on the words themselves.

### Self-attention as matrix form
Store $N$ input vectors in matrix $X$.

Compute values, queries and keys:
* $V[X] = \beta_v 1^T + \Omega_v X$
* $Q[X] = \beta_q 1^T + \Omega_q X$
* $K[X] = \beta_k 1^T + \Omega_k X$

Combine self-attentions: $Sa[X] = V[X] \cdot Softmax[K[X]^T Q[X]]$


## Extensions to dot-product self-attention

### Positional encoding
Self-attention is equivariant to permuting word order.
But word order is important in language.
* The man ate the fish
* The fish ate the man

Consider a matrix $\pi$ is added to the input $X$ that encodes positional information.
Each column of $\pi$ is unique and hence contains information about the position in the input sequence.

$Sa[X] = (V + \pi) \cdot Softmax[(K + \pi)^T (Q + \pi)]$

Sometimes, only applied to queries and keys but not to the values.

### Scaled dot-product self-attention
Dot products in the attention computation can have large magnitudes and move the arguments to the softmax function into a region where the largest value completely dominates.

Small changes to the inputs to the softmax function now have little effect on the output (i.e., the gradients are very small)

To prevent this, the dot products are scaled by the square root of the dimension $D_q$:
$Sa[X] = V \cdot Softmax [\frac{K^T Q}{\sqrt{D_q}}]$

### Multiple head self-attention
$H$ different sets of values, keys, and queries are computed.
* $V_h = \beta_{vh} 1^T + \Omega_{vh} X$
* $Q_h = \beta_{qh} 1^T + \Omega_{qh} X$
* $K_h = \beta_{kh} 1^T + \Omega_{kh} X$

The $h^{th}$ self-attention mechanism or head can be written as:
$Sa_h [X] = V_h \cdot Softmax [\frac{K^T_h Q_h}{\sqrt{D_q}}]$

$MhSa[X] = \Omega_c [Sa_1 [X]^T, Sa_2 [X]^T, \ldots, Sa_H [X]^T]^T$

If the dimension of the inputs $X_m$ is $D$ and there are $H$ heads, the values, queries, and keys will all be of size $D/H$, as this allows for an efficient implementation.

Multiple heads appear to make self-attention network more robust to bad initializations.

### Transformer layers
Self-attention is just one part of a larger transformer layer.

Consists of a multi-head self-attention unit, followed by a fully connected network $mlp[X]$.

* $X \leftarrow X + MhSa[X]$
* $X \leftarrow LayerNorm[X]$
* $x_n \leftarrow x_n + mlp[x_n], \forall n \in \\{1, \ldots, N \\}$
* $X \leftarrow LayerNorm[X]$
  * LayerNorm: Similar to BatchNorm but uses statistics across tokens within a single input sequence

## Transformers for natural language processing

### Natural language processing (NLP)
Translation, question answering, summarizing, generating new text, correcting spelling and grammar, finding entities, classifying bodies of text, changing style etc.

### Transformers for NLP
* Typical NLP pipeline starts with a tokenizer
* A tokenizer splits the text into words or word fragments
* Each of these tokens is mapped to a learned embedding
* Embeddings are passed through a series of transformer layers

### Tokenizer
Goal: Tokenizer splits text into smaller units (tokens) from vocabulary of possible tokens

* Inevitably, some words (e.g., names) will not be in the vocabulary
* Not clear how to handle punctuation
* The vocabulary would need different tokens for versions of the same word with different suffixes (e.g., walk, walks, walked, walking) and there is no way to clarify that these variations are related

Solution: Sub-word tokenization
* Initially, the tokens are just characters and whitespace. Their frequencies are displayed in the table
* At each iteration, the sub-word tokenizer looks for the most commonly occurring adjacent pair of tokens and merges them
* After some iterations, the tokens consist of a mix of letters, word fragments, and commonly occurring words

### Embedding
Each token in the vocabulary $v$ is mapped to a unique word embedding.
Eembeddings for the whole vocabulary are stored in a matrix: $\Omega_e \in R^{D \times \mid V \mid}$

### Transformer Model
Embedding matrix $X$ representing the text is passed through a series of $K$ transformer layers, called a transformer model.

Three types of transformer models:
* Encoder: transforms text embeddings into representation
* Decoder: predicts next token to continue the input text
* Encoder-decoder: used in sequence-to-sequence tasks, where one text string is converted into another


## Encoder model example: BERT
BERT exploits transfer learning: pre-training and fine tuning.

### BERT: Pre-training
Network is trained using self-supervision: without manual labels.
It predict missing words from sentences from a large internet corpus.
It trained for a million steps.

### BERT: Fine-tuning
Model parameters are adjusted to specialize the network to particular tasks.

Task examples
* Text classification (e.g., sentiment analysis)
* Word classification (e.g., named entity recognition)


## Decoder model example: GPT3

### Decoder model
Basic architecture is extremely similar to encoder model and comprises a series of transformer layers.
However, the goal is different.
The decoder has one purpose: to generate the next token in a sequence.

Builds autoregressive probability model:
$Pr(t_1, t_2, \ldots, t_N) = Pr(t_1) \prod_{n=2}^N Pr(t_n \mid t_1 \ldots t_{n-1})$

E.g., "It takes great courage to let ourself appear weak":
Pr(It takes great courage to let ourself appear weak) = Pr(It) Pr(takes|It) Pr(great|It takes) Pr(courage|It takes great) Pr(to|It takes great courage) Pr(let|It takes great courage to) Pr(ourself|It takes great courage to let) Pr(appear|It takes great courage to let ourself) Pr(weak|It takes great courage to let ourself appear).

* Predicting all next words simultaneously
* Masked self-attention

### Decoder example: GPT3
Few shot learning: Can learn to do novel tasks based on just a few examples

### ChatGPT
GPT3.5 (InstructGPT) fine-tuned with human annotations.

Trained to predict the next word + be "helpful, honest, harmless"

### InstructGPT
Reinforcement learning from human feedback.

### Transparency of GPT
GPT3 is functionally transparent; its architecture is described in Brown et al. (2020).
* No structural transparency: no access to the source code
* No run transparency: no access to the learned parameters, hardware, or training data.

Subsequent version GPT4 (or GPT4o) is not transparent at all


## Encoder-decoder model example: machine translation

### Encoder-decoder model: machine translation
Translation: sequence-to-sequence task
* Encoder: compute a good representation of the source sentence
* Decoder: generate the sentence in the target language
* Encoder-decoder model

During training, the decoder receives the ground truth translation in target language
* Passes it through a series of transformer layers with masked self-attention
* Predict the following word at each position

### Flow
The first sentence is passed through a standard encoder.

The second sentence is passed through a decoder that uses masked self-attention but also attends to the output embeddings of the encoder using cross-attention.

Loss function: Same as for decoder model; maximize the probability of the next word in the output sequence.

### Cross-attention
Encoder-decoder attention.

Flow of computation is the same as in standard self-attention.
However, the queries are calculated from the decoder embeddings, and the keys and values from the encoder embeddings.

## Transformers for long sequences
Each token in a transformer encoder model interacts with every other token
* computational complexity scales quadratically with sequence length.
* Even for a decoder model (that only interacts with previous tokens), complexity still scales quadratically
* Limits the length of sequences that can be used

Approaches to cope with these challenges
* convolutional structure so that each token only interacts with a few neighboring tokens. Across multiple layers, tokens still interact at larger distances
* allow select tokens (perhaps at the start of every sentence) to attend to all other tokens (encoder model) or all previous tokens (decoder model)


## Transformers for images

### Challenges
Many more pixels in an image than words in a sentence.
Quadratic complexity of self-attention poses a practical bottleneck.

Need to learn a good inductive bias.
* Note that CNNs have a good inductive bias: Takes into account the 2D structure of the image.
* However, this must be learned in a transformer network

Despite these challenges, transformer networks outperform CNNs.
Partly because of the enormous scale at which they can be constructed and the large amounts of data that can be used to pre-train networks.

### Transformers for images
ImageGPT: transformer decder
* Autoregressive model of image pixels that ingests a partial image and predicts the subsequent pixel value.
* simply learns a different positional encoding at each pixel; learn that each pixel has a close relationship with its preceding neighbors and also with nearby pixels in the row above

Vision Transformer (ViT)
* tackles the image resolution issue by dividing images into 16×16 patches
* Each patch is mapped to a lower dimension via a learned linear transformation, and then are fed into the transformer network
* Unlike BERT, it uses supervised pre-training
