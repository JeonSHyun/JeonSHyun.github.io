---
title: Transformers
author: JSH
date: 2025-12-06 13:03:00 +0800
categories: [Study, Artificial Intelligence]
tags: [Study, Artificial Intelligence, UDL]
use_math: true
---

# Transformers

## Motivation
Design neural network to encode and process text.
* There must be connections between the words
* The strength of these connections will depend on the words themselves

### Standard fully-connected layer
Problem:
* A very large number of parameters
* Can’t cope with text of different lengths

Conclusion:
* We need a model where parameters don’t increase with input length

## Dot-product self-attention
Takes $N$ inputs of size $D \times 1$ and returns $N$ inputs of size $D \times 1$.

Computes $N$ values (no ReLU): $v_n = \beta_v + \Omega_v x_n$

$N$ outputs are weighted sums of these values: $sa[x_n] = \sum_{m=1}^N a[x_n, x_m] v_m$

Weights depend on the inputs themselves.

### Attention weights
Compute $N$ "queries" and $N$ "keys" from input
* $q_n = \beta_q + \Omega_q x_n$
* $k_n = \beta_k + \Omega_k x_n$

Calculate similarity and pass through softmax:
* $a[x_n, x_m] = softmax_m [sim[k_m q_n]] = \frac{\exp [sim[k_m q_n]]}{\sum_{m'=1}^N \exp [sim[k_m' q_n]]}$

Take dot products and pass through softmax:
* $a[x_n, x_m] = softmax_m [k_m^T q_n] = \frac{\exp [k_m^T q_n]}{\sum_{m'=1}^N \exp [k_m^T q_n]}$

### Dot product (measure of similarity)
$x^T y = \mid x \mid \cdot \mid y \mid \cdot \theta$
* Similar vectors: angle $\theta$ close to 0, $\cos(\theta)$ close to 1
* Orthogonal vectors: angle $\theta$ close to 90, $\cos(\theta)$ close to 0
* Opposite vectors: angle $\theta$ close to 180, $\cos(\theta)$ close to -1

### Self-attention summary
No activation function but mechanism is nonlinear due to the dot-product and a softmax operation.

Shared set of parameters: parameters don't increase with input length.

There must be connections between the words.
The strength of these connections will depend on the words themselves.

### Self-attention as matrix form
Store $N$ input vectors in matrix $X$.

Compute values, queries and keys:
* $V[X] = \beta_v 1^T + \Omega_v X$
* $Q[X] = \beta_q 1^T + \Omega_q X$
* $K[X] = \beta_k 1^T + \Omega_k X$

Combine self-attentions: $Sa[X] = V[X] \cdot Softmax[K[X]^T Q[X]]$


## Extensions to dot-product self-attention

### Positional encoding
Self-attention is equivariant to permuting word order.
But word order is important in language.
* The man ate the fish
* The fish ate the man

Consider a matrix $\pi$ is added to the input $X$ that encodes positional information.
Each column of $\pi$ is unique and hence contains information about the position in the input sequence.

$Sa[X] = (V + \pi) \cdot Softmax[(K + \pi)^T (Q + \pi)]$

Sometimes, only applied to queries and keys but not to the values.

### Scaled dot-product self-attention
Dot products in the attention computation can have large magnitudes and move the arguments to the softmax function into a region where the largest value completely dominates.

Small changes to the inputs to the softmax function now have little effect on the output (i.e., the gradients are very small)

To prevent this, the dot products are scaled by the square root of the dimension $D_q$:
$Sa[X] = V \cdot Softmax [\frac{K^T Q}{\sqrt{D_q}}]$

### Multiple head self-attention
$H$ different sets of values, keys, and queries are computed.
* $V_h = \beta_{vh} 1^T + \Omega_{vh} X$
* $Q_h = \beta_{qh} 1^T + \Omega_{qh} X$
* $K_h = \beta_{kh} 1^T + \Omega_{kh} X$

The $h^{th}$ self-attention mechanism or head can be written as:
$Sa_h [X] = V_h \cdot Softmax [\frac{K^T_h Q_h}{\sqrt{D_q}}]$

$MhSa[X] = \Omega_c [Sa_1 [X]^T, Sa_2 [X]^T, \ldots, Sa_H [X]^T]^T$

If the dimension of the inputs $X_m$ is $D$ and there are $H$ heads, the values, queries, and keys will all be of size $D/H$, as this allows for an efficient implementation.

Multiple heads appear to make self-attention network more robust to bad initializations.

### Transformer layers
Self-attention is just one part of a larger transformer layer.

Consists of a multi-head self-attention unit, followed by a fully connected network $mlp[X]$.

* $X \leftarrow X + MhSa[X]$
* $X \leftarrow LayerNorm[X]$
* $x_n \leftarrow x_n + mlp[x_n], \forall n \in \\{1, \ldots, N \\}$
* $X \leftarrow LayerNorm[X]$
  * LayerNorm: Similar to BatchNorm but uses statistics across tokens within a single input sequence

