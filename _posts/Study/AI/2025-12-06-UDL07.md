---
title: Residual Networks
author: JSH
date: 2025-12-06 12:30:00 +0800
categories: [Study, Artificial Intelligence]
tags: [Study, Artificial Intelligence, UDL]
use_math: true
---

# Residual Networks

Regular network:
* $h_1 = f_1 [x, \phi_1]$
* $h_2 = f_2 [h_1, \phi_2]$
* $h_3 = f_3 [h_2, \phi_3]$
* $y = f_4 [h_3, \phi_4]$

Residual network:
* $h_1 = f_1 [x, \phi_1]$
* $h_2 = h_1 + f_2[h_1, \phi_2]$
* $h_3 = h_2 + f_3[h_2, \phi_3]$
* $y = h_3 + f_4[h_3, \phi_4]$

## Residual connections
Each function $f_k$ learns an additive change to current representation.
Their outputs must be the same size as their inputs.

Adding residual connections roughly doubles the depth of a network that can be practically trained before performance degrades.

## Batch normalization
Shifts and rescales each activation $h$ so that its mean and variance across the batch become values learned during training.

$m_h = \frac{1}{\mid B \mid} \sum_{i \in B} h_i$

$s_h = \sqrt{\frac{1}{\mid B \mid} \sum_{i \in B} (h_i - m_h)^2}$

$\Rightarrow h_i \leftarrow \frac{h_i - m_h}{s_h + \epsilon}$

$\Rightarrow h_i \leftarrow \gamma h_i + \delta, \forall i \in B$

## Resnet
Resnet 200 (2016)
